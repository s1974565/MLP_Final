{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "# Helpers\n",
    "from testing import *\n",
    "# Baseline model 1\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, pipeline\n",
    "# Sentence similarity model\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Check if GPU acceleration is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    device_num = torch.cuda.current_device()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    device_num = -1\n",
    "\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning by \"Cosine Similarity\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Run this if you removed pickle files\n",
    "# saving_pickles()\n",
    "\n",
    "train_df, valid_df, test_df = loading_pickles()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Constants ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# For training (not validation)\n",
    "batch_size = 50\n",
    "\n",
    "# Whether to shuffle the training set or not\n",
    "shuffle = False\n",
    "\n",
    "# Using only a portion of the train dataset for performance\n",
    "# Set it to 1 to use all train dataset; increasing this may give better result.\n",
    "size_proportion_train = 0.01\n",
    "\n",
    "# Hyperparameters ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "total_epoch_number = 2\n",
    "# Initial learning rate for the optimizer\n",
    "learning_rate = 0.00005\n",
    "weight_decay_coefficient = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Do not change these values unless necessary\n",
    "mask_prob = 0.5\n",
    "window_size = 100\n",
    "rng_seed = 42\n",
    "size_proportion_valid = 0.5\n",
    "tokenizer_cs = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model_cs = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/4121 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "648f6e9a45914089bfd3f4a1c8167d96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Window split:   0%|          | 0/4121 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee5864175b5a45c490cb9d5a6f1a9f94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/11553 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39224dfa4d1c4da9b8b0403844d3ad5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/22176 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3babedd9a9374d738a92850f50aea61b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def masking_df(code_df):\n",
    "    masked_code_df = mask_variable_df(code_df, mask_prob=mask_prob, rng_seed=rng_seed)\n",
    "    merged_code_df = pd.concat([code_df, masked_code_df], axis=\"columns\")\n",
    "    return merged_code_df\n",
    "\n",
    "def window_df(code_df):\n",
    "    merged_code_df = masking_df(code_df)\n",
    "    return split_into_windows(merged_code_df, window_size=window_size, mask_token=\"<mask>\")\n",
    "\n",
    "train_df_size = int(len(train_df) * size_proportion_train)\n",
    "valid_df_size = int(len(valid_df) * size_proportion_valid)\n",
    "\n",
    "if shuffle:\n",
    "    train_df = train_df.sample(frac=1, random_state=rng_seed).reset_index(drop=True)\n",
    "\n",
    "window_train_df = window_df(train_df[:train_df_size])\n",
    "\n",
    "merged_valid_df = masking_df(valid_df[:valid_df_size])\n",
    "merged_test_df = masking_df(test_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "three ways to fine tune the model:\n",
    "1. Fine-tune the last head of the model <- Current approach\n",
    "2. Fine-tune the whole model\n",
    "3. Add extra layers to the model and fine tune the additional layer\n",
    "\n",
    "We use the first baseline model, as it showed the better performance in the baseline testing.\n",
    "See the baseline_testing notebook for more details.\n",
    "\n",
    "There are three main hyperparameters: total_epoch_number, learning_rate, and weight_decay_coefficient\n",
    "Try different combinations to fine the best performing model. I commented thoroughly so there should be no problem in understanding the codes.\n",
    "If you want to change the optimizer, then you need to directly edit the optimizer definition in the training loop cell.\n",
    "There are other changeable constants, and varying them may also give a better performance. For example, if you increase size_limit_proportion_train,\n",
    "the more training set will be used for the fine-tuning. But the fine-tuning time will be increased too.\n",
    "\n",
    "After a model is fine-tuned, its performance will then be evaluated based on the average cosine similarity with the validation set.\n",
    "You can check if the fine-tuned model performs better or worse compared to the baseline model. If your model performs well, then\n",
    "consider saving the model so that we can use it later. For performance, it is set to only use the half of the validation set.\n",
    "\n",
    "If any error occurs, or if you have a suggestion, let me know.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model investigation\n",
    "\n",
    "You need to uncomment some cells to see the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# All Huggingface models are standard torch.nn.Module, so they can easily be used in any training loop.\n",
    "\n",
    "# Model architecture information:\n",
    "# https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/roberta#transformers.RobertaForMaskedLM\n",
    "model_b1 = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base-mlm')\n",
    "tokenizer_b1 = RobertaTokenizer.from_pretrained('microsoft/codebert-base-mlm')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Model structure (Uncomment to see)\n",
    "# print(model_b1)\n",
    "\n",
    "# Embedding size = (50265, 768)\n",
    "# Dropout probability = 0.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Model parameters (Uncomment to see)\n",
    "\n",
    "# for name, param in model_b1.named_parameters():\n",
    "#     print(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight False\n",
      "roberta.embeddings.position_embeddings.weight False\n",
      "roberta.embeddings.token_type_embeddings.weight False\n",
      "roberta.embeddings.LayerNorm.weight False\n",
      "roberta.embeddings.LayerNorm.bias False\n",
      "roberta.encoder.layer.0.attention.self.query.weight False\n",
      "roberta.encoder.layer.0.attention.self.query.bias False\n",
      "roberta.encoder.layer.0.attention.self.key.weight False\n",
      "roberta.encoder.layer.0.attention.self.key.bias False\n",
      "roberta.encoder.layer.0.attention.self.value.weight False\n",
      "roberta.encoder.layer.0.attention.self.value.bias False\n",
      "roberta.encoder.layer.0.attention.output.dense.weight False\n",
      "roberta.encoder.layer.0.attention.output.dense.bias False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.0.intermediate.dense.weight False\n",
      "roberta.encoder.layer.0.intermediate.dense.bias False\n",
      "roberta.encoder.layer.0.output.dense.weight False\n",
      "roberta.encoder.layer.0.output.dense.bias False\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.1.attention.self.query.weight False\n",
      "roberta.encoder.layer.1.attention.self.query.bias False\n",
      "roberta.encoder.layer.1.attention.self.key.weight False\n",
      "roberta.encoder.layer.1.attention.self.key.bias False\n",
      "roberta.encoder.layer.1.attention.self.value.weight False\n",
      "roberta.encoder.layer.1.attention.self.value.bias False\n",
      "roberta.encoder.layer.1.attention.output.dense.weight False\n",
      "roberta.encoder.layer.1.attention.output.dense.bias False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.1.intermediate.dense.weight False\n",
      "roberta.encoder.layer.1.intermediate.dense.bias False\n",
      "roberta.encoder.layer.1.output.dense.weight False\n",
      "roberta.encoder.layer.1.output.dense.bias False\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.2.attention.self.query.weight False\n",
      "roberta.encoder.layer.2.attention.self.query.bias False\n",
      "roberta.encoder.layer.2.attention.self.key.weight False\n",
      "roberta.encoder.layer.2.attention.self.key.bias False\n",
      "roberta.encoder.layer.2.attention.self.value.weight False\n",
      "roberta.encoder.layer.2.attention.self.value.bias False\n",
      "roberta.encoder.layer.2.attention.output.dense.weight False\n",
      "roberta.encoder.layer.2.attention.output.dense.bias False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.2.intermediate.dense.weight False\n",
      "roberta.encoder.layer.2.intermediate.dense.bias False\n",
      "roberta.encoder.layer.2.output.dense.weight False\n",
      "roberta.encoder.layer.2.output.dense.bias False\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.3.attention.self.query.weight False\n",
      "roberta.encoder.layer.3.attention.self.query.bias False\n",
      "roberta.encoder.layer.3.attention.self.key.weight False\n",
      "roberta.encoder.layer.3.attention.self.key.bias False\n",
      "roberta.encoder.layer.3.attention.self.value.weight False\n",
      "roberta.encoder.layer.3.attention.self.value.bias False\n",
      "roberta.encoder.layer.3.attention.output.dense.weight False\n",
      "roberta.encoder.layer.3.attention.output.dense.bias False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.3.intermediate.dense.weight False\n",
      "roberta.encoder.layer.3.intermediate.dense.bias False\n",
      "roberta.encoder.layer.3.output.dense.weight False\n",
      "roberta.encoder.layer.3.output.dense.bias False\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.4.attention.self.query.weight False\n",
      "roberta.encoder.layer.4.attention.self.query.bias False\n",
      "roberta.encoder.layer.4.attention.self.key.weight False\n",
      "roberta.encoder.layer.4.attention.self.key.bias False\n",
      "roberta.encoder.layer.4.attention.self.value.weight False\n",
      "roberta.encoder.layer.4.attention.self.value.bias False\n",
      "roberta.encoder.layer.4.attention.output.dense.weight False\n",
      "roberta.encoder.layer.4.attention.output.dense.bias False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.4.intermediate.dense.weight False\n",
      "roberta.encoder.layer.4.intermediate.dense.bias False\n",
      "roberta.encoder.layer.4.output.dense.weight False\n",
      "roberta.encoder.layer.4.output.dense.bias False\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.5.attention.self.query.weight False\n",
      "roberta.encoder.layer.5.attention.self.query.bias False\n",
      "roberta.encoder.layer.5.attention.self.key.weight False\n",
      "roberta.encoder.layer.5.attention.self.key.bias False\n",
      "roberta.encoder.layer.5.attention.self.value.weight False\n",
      "roberta.encoder.layer.5.attention.self.value.bias False\n",
      "roberta.encoder.layer.5.attention.output.dense.weight False\n",
      "roberta.encoder.layer.5.attention.output.dense.bias False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.5.intermediate.dense.weight False\n",
      "roberta.encoder.layer.5.intermediate.dense.bias False\n",
      "roberta.encoder.layer.5.output.dense.weight False\n",
      "roberta.encoder.layer.5.output.dense.bias False\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.6.attention.self.query.weight False\n",
      "roberta.encoder.layer.6.attention.self.query.bias False\n",
      "roberta.encoder.layer.6.attention.self.key.weight False\n",
      "roberta.encoder.layer.6.attention.self.key.bias False\n",
      "roberta.encoder.layer.6.attention.self.value.weight False\n",
      "roberta.encoder.layer.6.attention.self.value.bias False\n",
      "roberta.encoder.layer.6.attention.output.dense.weight False\n",
      "roberta.encoder.layer.6.attention.output.dense.bias False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.6.intermediate.dense.weight False\n",
      "roberta.encoder.layer.6.intermediate.dense.bias False\n",
      "roberta.encoder.layer.6.output.dense.weight False\n",
      "roberta.encoder.layer.6.output.dense.bias False\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.7.attention.self.query.weight False\n",
      "roberta.encoder.layer.7.attention.self.query.bias False\n",
      "roberta.encoder.layer.7.attention.self.key.weight False\n",
      "roberta.encoder.layer.7.attention.self.key.bias False\n",
      "roberta.encoder.layer.7.attention.self.value.weight False\n",
      "roberta.encoder.layer.7.attention.self.value.bias False\n",
      "roberta.encoder.layer.7.attention.output.dense.weight False\n",
      "roberta.encoder.layer.7.attention.output.dense.bias False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.7.intermediate.dense.weight False\n",
      "roberta.encoder.layer.7.intermediate.dense.bias False\n",
      "roberta.encoder.layer.7.output.dense.weight False\n",
      "roberta.encoder.layer.7.output.dense.bias False\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.8.attention.self.query.weight False\n",
      "roberta.encoder.layer.8.attention.self.query.bias False\n",
      "roberta.encoder.layer.8.attention.self.key.weight False\n",
      "roberta.encoder.layer.8.attention.self.key.bias False\n",
      "roberta.encoder.layer.8.attention.self.value.weight False\n",
      "roberta.encoder.layer.8.attention.self.value.bias False\n",
      "roberta.encoder.layer.8.attention.output.dense.weight False\n",
      "roberta.encoder.layer.8.attention.output.dense.bias False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.8.intermediate.dense.weight False\n",
      "roberta.encoder.layer.8.intermediate.dense.bias False\n",
      "roberta.encoder.layer.8.output.dense.weight False\n",
      "roberta.encoder.layer.8.output.dense.bias False\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.9.attention.self.query.weight False\n",
      "roberta.encoder.layer.9.attention.self.query.bias False\n",
      "roberta.encoder.layer.9.attention.self.key.weight False\n",
      "roberta.encoder.layer.9.attention.self.key.bias False\n",
      "roberta.encoder.layer.9.attention.self.value.weight False\n",
      "roberta.encoder.layer.9.attention.self.value.bias False\n",
      "roberta.encoder.layer.9.attention.output.dense.weight False\n",
      "roberta.encoder.layer.9.attention.output.dense.bias False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.9.intermediate.dense.weight False\n",
      "roberta.encoder.layer.9.intermediate.dense.bias False\n",
      "roberta.encoder.layer.9.output.dense.weight False\n",
      "roberta.encoder.layer.9.output.dense.bias False\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.10.attention.self.query.weight False\n",
      "roberta.encoder.layer.10.attention.self.query.bias False\n",
      "roberta.encoder.layer.10.attention.self.key.weight False\n",
      "roberta.encoder.layer.10.attention.self.key.bias False\n",
      "roberta.encoder.layer.10.attention.self.value.weight False\n",
      "roberta.encoder.layer.10.attention.self.value.bias False\n",
      "roberta.encoder.layer.10.attention.output.dense.weight False\n",
      "roberta.encoder.layer.10.attention.output.dense.bias False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.10.intermediate.dense.weight False\n",
      "roberta.encoder.layer.10.intermediate.dense.bias False\n",
      "roberta.encoder.layer.10.output.dense.weight False\n",
      "roberta.encoder.layer.10.output.dense.bias False\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.11.attention.self.query.weight False\n",
      "roberta.encoder.layer.11.attention.self.query.bias False\n",
      "roberta.encoder.layer.11.attention.self.key.weight False\n",
      "roberta.encoder.layer.11.attention.self.key.bias False\n",
      "roberta.encoder.layer.11.attention.self.value.weight False\n",
      "roberta.encoder.layer.11.attention.self.value.bias False\n",
      "roberta.encoder.layer.11.attention.output.dense.weight False\n",
      "roberta.encoder.layer.11.attention.output.dense.bias False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.11.intermediate.dense.weight False\n",
      "roberta.encoder.layer.11.intermediate.dense.bias False\n",
      "roberta.encoder.layer.11.output.dense.weight False\n",
      "roberta.encoder.layer.11.output.dense.bias False\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias False\n",
      "lm_head.bias True\n",
      "lm_head.dense.weight True\n",
      "lm_head.dense.bias True\n",
      "lm_head.layer_norm.weight True\n",
      "lm_head.layer_norm.bias True\n"
     ]
    }
   ],
   "source": [
    "# Freeze parameters except the last head\n",
    "\n",
    "for name, param in model_b1.named_parameters():\n",
    "    if \"lm_head\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    print(name, param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class CodeNetDataset(Dataset):\n",
    "    def __init__(self, window_df, tokenizer):\n",
    "        self.window_df = window_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "        # Not sure how to apply tqdm (progress bar) for this; I plan to update soon\n",
    "        self.tokenized = tokenizer(list(window_df[\"window\"]), padding=True)\n",
    "        self.input_ids = self.tokenized[\"input_ids\"]\n",
    "        self.attention_mask = self.tokenized[\"attention_mask\"]\n",
    "        self.label = self.window_df[\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mask_token_index = self.input_ids[index].index(self.mask_token_id)\n",
    "        # String cannot be stored in a tensor. They need to be converted to numeric values first.\n",
    "        label_token_id = self.tokenizer.convert_tokens_to_ids(self.label[index])\n",
    "\n",
    "        return torch.tensor(self.input_ids[index]), torch.tensor(self.attention_mask[index]), torch.tensor(mask_token_index), torch.tensor(label_token_id)\n",
    "\n",
    "train_dataset = CodeNetDataset(window_train_df, tokenizer_b1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b### Fine-tuning (Cosine Similarity)\n",
    "\n",
    "We only use the top_1 prediction as it showed the best performance in the baseline testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Cosine similarity loss is calculated for each batch, before backpropagation.\n",
    "def cosine_similarity_loss(prediction_logits, mask_token_indices, label_token_ids):\n",
    "\n",
    "    cosine_similarity_list = list()\n",
    "    for row, mask_token_index, label_token_id in zip(range(prediction_logits.shape[0]), mask_token_indices, label_token_ids):\n",
    "        prediction_word_id = torch.argmax(prediction_logits[row, mask_token_index])\n",
    "        # Words will be stripped to remove unnecessary whitespaces.\n",
    "        prediction_word = tokenizer_b1.decode(prediction_word_id).strip()\n",
    "        label_word = tokenizer_b1.decode(label_token_id).strip()\n",
    "\n",
    "        # Computing cosine similarity\n",
    "        encoded_words = tokenizer_cs([label_word, prediction_word], padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_cs_output = model_cs(**encoded_words)\n",
    "        word_embeddings = mean_pooling(model_cs_output, encoded_words[\"attention_mask\"])\n",
    "        word_embeddings = F.normalize(word_embeddings, p=2, dim=1)\n",
    "        dot_product = torch.dot(word_embeddings[0, :], word_embeddings[1, :])\n",
    "        # dot_product is a zero-dimensional tensor. We need to add a dummy dimension at 1 for concatenation.\n",
    "        cosine_similarity_list.append(torch.unsqueeze(dot_product, dim=0))\n",
    "\n",
    "    # The mean cosine similarity for a batch\n",
    "    # -1 is multiplied because we are minimizing the loss\n",
    "    return torch.mean(torch.concat(cosine_similarity_list)).requires_grad_() * -1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/257 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "661800c1ae1c42818f06dd7a828c5209"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Cosine Similarity Loss: -0.4523374296811768\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2:   0%|          | 0/257 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3db02bbbeb29464aa1a8cf6ea8ce529e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Cosine Similarity Loss: -0.4536707161233583\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3:   0%|          | 0/257 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5cf307dfd7848689e21a88344caaf5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Cosine Similarity Loss: -0.45291782109653905\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 4:   0%|          | 0/257 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7133f49f864c4a888569fc1ed9761bf8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running this cell multiple times in a single notebook can fully saturate the GPU memory, which leads the OutOfMemoryError.\n",
    "# If it happens, re-start the notebook kernel to remove all model instances from the GPU memory.\n",
    "# It seems like the memory error sometimes happens when the dataset size is too big, regardless of the batch size.\n",
    "# I am not sure why it is the case; Let me know if you encounter this issue.\n",
    "\n",
    "# Setting the model to the train mode\n",
    "model_b1.train()\n",
    "model_b1.to(device)\n",
    "\n",
    "print(f\"Total train set size: {len(train_dataset)}, batch_size: {batch_size}, batch_number: {math.ceil(len(train_dataset) / batch_size)}\")\n",
    "\n",
    "# Using the AdamW optimizer: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "# Feel free to try others\n",
    "optimizer = torch.optim.AdamW(model_b1.parameters(), lr=learning_rate, weight_decay=weight_decay_coefficient)\n",
    "\n",
    "for epoch_num in range(1, total_epoch_number+1):\n",
    "    loss_list = list()\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch_num}\", total=math.ceil(len(train_dataset) / batch_size)):\n",
    "        # Sending to GPU\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_mask_token_index = batch[2].to(device)\n",
    "        batch_label_token_id = batch[3].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        prediction_logits = model_b1(input_ids=batch_input_ids, attention_mask=batch_attention_mask).logits\n",
    "        # Computing prediction error\n",
    "        loss = cosine_similarity_loss(prediction_logits, batch_mask_token_index, batch_label_token_id)\n",
    "        # Removing gradients from the past iteration\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Removing to free up memory\n",
    "        del batch_input_ids\n",
    "        del batch_attention_mask\n",
    "        del batch_mask_token_index\n",
    "        del batch_label_token_id\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch_num}, Cosine Similarity Loss: {np.mean(loss_list)}\")\n",
    "\n",
    "# Setting the model back to the evaluation mode\n",
    "model_b1.eval()\n",
    "# Not to print the model structure\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation (Cosine similarity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Window split:   0%|          | 0/23107 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34919356a5f54ce49ba37597e5d10d9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Prediction:   0%|          | 0/67804 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d570a2913c64abb99211546476942cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Similarity:   0%|          | 0/67804 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1345900de62c488cbf4fa97917c39858"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b1_finetuned = pipeline('fill-mask', model=model_b1, tokenizer=tokenizer_b1, device=device_num)\n",
    "b1_finetuned_result = model_test(merged_code_df=merged_valid_df, unmasker=b1_finetuned, top_k=1, window_size=window_size, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity: 0.49379345774650574\n",
      "The fine-tuned model performance is better than the baseline, by 0.01018860936164856\n"
     ]
    }
   ],
   "source": [
    "average_similarity = np.mean(b1_finetuned_result['similarity'])\n",
    "# Baseline performance (I plan to send you the result by this evening)\n",
    "baseline_similarity = 0.4816761314868927\n",
    "print(f\"Average cosine similarity: {average_similarity}\")\n",
    "if average_similarity < baseline_similarity:\n",
    "    print(f\"The fine-tuned model performance is worse than the baseline, by {baseline_similarity-average_similarity}\")\n",
    "else:\n",
    "    print(f\"The fine-tuned model performance is better than the baseline, by {average_similarity-baseline_similarity}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the model\n",
    "\n",
    "Check the following for loading the model: https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Saving the hyperparameters and the fine-tuned model\n",
    "# The model size is around 500 MB. You may want to save the dictionary only\n",
    "training_constants = [batch_size, shuffle, size_proportion_train, total_epoch_number, learning_rate, weight_decay_coefficient, mask_prob, window_size, rng_seed, size_proportion_valid]\n",
    "result_dict = {\"training_constants\": training_constants, \"validation_similarity\": average_similarity, \"loss_metric\": \"cosine_similarity\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Generating non-existing filenames\n",
    "file_index = 0\n",
    "while os.path.exists(f\"./saved_models/model_{file_index}\"):\n",
    "    file_index += 1\n",
    "\n",
    "model_filepath = f\"./saved_models/model_{file_index}\"\n",
    "hparameter_filepath = f\"./saved_models/hparameter_{file_index}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model_b1, model_filepath)\n",
    "# Saving the hyperparameters\n",
    "with open(hparameter_filepath, \"wb\") as fw:\n",
    "    pickle.dump(result_dict, fw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
