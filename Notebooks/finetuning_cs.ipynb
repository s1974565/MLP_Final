{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helpers\n",
    "from testing import *\n",
    "# Baseline model 1\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, pipeline\n",
    "# Sentence similarity model\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Check if GPU acceleration is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    device_num = torch.cuda.current_device()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    device_num = -1\n",
    "\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning by \"Cosine Similarity\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Run this if you removed pickle files\n",
    "# saving_pickles()\n",
    "\n",
    "train_df, valid_df, test_df = loading_pickles()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Constants ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# For training (not validation)\n",
    "batch_size = 10\n",
    "\n",
    "# Using only a portion of the train dataset for performance\n",
    "# Set it to 1 to use all train dataset; increasing this may give better result.\n",
    "size_proportion_train = 0.01\n",
    "\n",
    "# Hyperparameters ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "total_epoch_number = 5\n",
    "# Initial learning rate for the optimizer\n",
    "learning_rate = 5.00E-02\n",
    "weight_decay_coefficient = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Do not change these values unless necessary\n",
    "mask_prob = 0.5\n",
    "window_size = 100\n",
    "rng_seed = 42\n",
    "# Train and Valid\n",
    "shuffle = True\n",
    "size_proportion_valid = size_proportion_train\n",
    "tokenizer_cs = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model_cs = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/4121 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da01f848d15547a0985864f31932f25d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Window split:   0%|          | 0/4121 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af719efd8e344e22876c24aea124b23c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/231 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "398539ffad4b4c3eb8b3c04922f4627e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Window split:   0%|          | 0/231 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3aac02dab76b4f81a912d50f6de5e296"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/231 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b90ed24052a48748c66cd645dedf540"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/22176 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a814523f80b4ac595d49f9ec2ed378a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def masking_df(code_df):\n",
    "    masked_code_df = mask_variable_df(code_df, mask_prob=mask_prob, rng_seed=rng_seed)\n",
    "    merged_code_df = pd.concat([code_df, masked_code_df], axis=\"columns\")\n",
    "    return merged_code_df\n",
    "\n",
    "def window_df(code_df):\n",
    "    merged_code_df = masking_df(code_df)\n",
    "    return split_into_windows(merged_code_df, window_size=window_size, mask_token=\"<mask>\")\n",
    "\n",
    "train_df_size = int(len(train_df) * size_proportion_train)\n",
    "valid_df_size = int(len(valid_df) * size_proportion_valid)\n",
    "\n",
    "if shuffle:\n",
    "    train_df = train_df.sample(frac=1, random_state=rng_seed).reset_index(drop=True)\n",
    "    valid_df = valid_df.sample(frac=1, random_state=rng_seed).reset_index(drop=True)\n",
    "\n",
    "window_train_df = window_df(train_df[:train_df_size])\n",
    "window_valid_df = window_df(valid_df[:valid_df_size])\n",
    "\n",
    "merged_valid_df = masking_df(valid_df[:valid_df_size])\n",
    "merged_test_df = masking_df(test_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "three ways to fine tune the model:\n",
    "\n",
    "1. Fine-tune the last head of the model <- Current approach\n",
    "2. Fine-tune the whole model\n",
    "3. Add extra layers to the model and fine tune the additional layer\n",
    "\n",
    "We use the first baseline model, as it showed the better performance in the baseline testing.\n",
    "See the baseline_testing notebook for more details.\n",
    "\n",
    "There are three main hyperparameters: total_epoch_number, learning_rate, and weight_decay_coefficient\n",
    "Try different combinations to fine the best performing model. I commented thoroughly so there should be no problem in understanding the codes.\n",
    "If you want to change the optimizer, then you need to directly edit the optimizer definition in the training loop cell.\n",
    "There are other changeable constants, and varying them may also give a better performance. For example, if you increase size_limit_proportion_train,\n",
    "the more training set will be used for the fine-tuning. But the fine-tuning time will be increased too.\n",
    "\n",
    "After a model is fine-tuned, its performance will then be evaluated based on the average cosine similarity with the validation set.\n",
    "You can check if the fine-tuned model performs better or worse compared to the baseline model. If your model performs well, then\n",
    "consider saving the model so that we can use it later. For performance, it is set to only use the half of the validation set.\n",
    "\n",
    "If any error occurs, or if you have a suggestion, let me know.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model investigation\n",
    "\n",
    "You need to uncomment some cells to see the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# All Huggingface models are standard torch.nn.Module, so they can easily be used in any training loop.\n",
    "\n",
    "# Model architecture information:\n",
    "# https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/roberta#transformers.RobertaForMaskedLM\n",
    "model_b1 = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base-mlm')\n",
    "tokenizer_b1 = RobertaTokenizer.from_pretrained('microsoft/codebert-base-mlm')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Model structure (Uncomment to see)\n",
    "# print(model_b1)\n",
    "\n",
    "# Embedding size = (50265, 768)\n",
    "# Dropout probability = 0.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Model parameters (Uncomment to see)\n",
    "\n",
    "# for name, param in model_b1.named_parameters():\n",
    "#     print(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight False\n",
      "roberta.embeddings.position_embeddings.weight False\n",
      "roberta.embeddings.token_type_embeddings.weight False\n",
      "roberta.embeddings.LayerNorm.weight False\n",
      "roberta.embeddings.LayerNorm.bias False\n",
      "roberta.encoder.layer.0.attention.self.query.weight False\n",
      "roberta.encoder.layer.0.attention.self.query.bias False\n",
      "roberta.encoder.layer.0.attention.self.key.weight False\n",
      "roberta.encoder.layer.0.attention.self.key.bias False\n",
      "roberta.encoder.layer.0.attention.self.value.weight False\n",
      "roberta.encoder.layer.0.attention.self.value.bias False\n",
      "roberta.encoder.layer.0.attention.output.dense.weight False\n",
      "roberta.encoder.layer.0.attention.output.dense.bias False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.0.intermediate.dense.weight False\n",
      "roberta.encoder.layer.0.intermediate.dense.bias False\n",
      "roberta.encoder.layer.0.output.dense.weight False\n",
      "roberta.encoder.layer.0.output.dense.bias False\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.1.attention.self.query.weight False\n",
      "roberta.encoder.layer.1.attention.self.query.bias False\n",
      "roberta.encoder.layer.1.attention.self.key.weight False\n",
      "roberta.encoder.layer.1.attention.self.key.bias False\n",
      "roberta.encoder.layer.1.attention.self.value.weight False\n",
      "roberta.encoder.layer.1.attention.self.value.bias False\n",
      "roberta.encoder.layer.1.attention.output.dense.weight False\n",
      "roberta.encoder.layer.1.attention.output.dense.bias False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.1.intermediate.dense.weight False\n",
      "roberta.encoder.layer.1.intermediate.dense.bias False\n",
      "roberta.encoder.layer.1.output.dense.weight False\n",
      "roberta.encoder.layer.1.output.dense.bias False\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.2.attention.self.query.weight False\n",
      "roberta.encoder.layer.2.attention.self.query.bias False\n",
      "roberta.encoder.layer.2.attention.self.key.weight False\n",
      "roberta.encoder.layer.2.attention.self.key.bias False\n",
      "roberta.encoder.layer.2.attention.self.value.weight False\n",
      "roberta.encoder.layer.2.attention.self.value.bias False\n",
      "roberta.encoder.layer.2.attention.output.dense.weight False\n",
      "roberta.encoder.layer.2.attention.output.dense.bias False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.2.intermediate.dense.weight False\n",
      "roberta.encoder.layer.2.intermediate.dense.bias False\n",
      "roberta.encoder.layer.2.output.dense.weight False\n",
      "roberta.encoder.layer.2.output.dense.bias False\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.3.attention.self.query.weight False\n",
      "roberta.encoder.layer.3.attention.self.query.bias False\n",
      "roberta.encoder.layer.3.attention.self.key.weight False\n",
      "roberta.encoder.layer.3.attention.self.key.bias False\n",
      "roberta.encoder.layer.3.attention.self.value.weight False\n",
      "roberta.encoder.layer.3.attention.self.value.bias False\n",
      "roberta.encoder.layer.3.attention.output.dense.weight False\n",
      "roberta.encoder.layer.3.attention.output.dense.bias False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.3.intermediate.dense.weight False\n",
      "roberta.encoder.layer.3.intermediate.dense.bias False\n",
      "roberta.encoder.layer.3.output.dense.weight False\n",
      "roberta.encoder.layer.3.output.dense.bias False\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.4.attention.self.query.weight False\n",
      "roberta.encoder.layer.4.attention.self.query.bias False\n",
      "roberta.encoder.layer.4.attention.self.key.weight False\n",
      "roberta.encoder.layer.4.attention.self.key.bias False\n",
      "roberta.encoder.layer.4.attention.self.value.weight False\n",
      "roberta.encoder.layer.4.attention.self.value.bias False\n",
      "roberta.encoder.layer.4.attention.output.dense.weight False\n",
      "roberta.encoder.layer.4.attention.output.dense.bias False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.4.intermediate.dense.weight False\n",
      "roberta.encoder.layer.4.intermediate.dense.bias False\n",
      "roberta.encoder.layer.4.output.dense.weight False\n",
      "roberta.encoder.layer.4.output.dense.bias False\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.5.attention.self.query.weight False\n",
      "roberta.encoder.layer.5.attention.self.query.bias False\n",
      "roberta.encoder.layer.5.attention.self.key.weight False\n",
      "roberta.encoder.layer.5.attention.self.key.bias False\n",
      "roberta.encoder.layer.5.attention.self.value.weight False\n",
      "roberta.encoder.layer.5.attention.self.value.bias False\n",
      "roberta.encoder.layer.5.attention.output.dense.weight False\n",
      "roberta.encoder.layer.5.attention.output.dense.bias False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.5.intermediate.dense.weight False\n",
      "roberta.encoder.layer.5.intermediate.dense.bias False\n",
      "roberta.encoder.layer.5.output.dense.weight False\n",
      "roberta.encoder.layer.5.output.dense.bias False\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.6.attention.self.query.weight False\n",
      "roberta.encoder.layer.6.attention.self.query.bias False\n",
      "roberta.encoder.layer.6.attention.self.key.weight False\n",
      "roberta.encoder.layer.6.attention.self.key.bias False\n",
      "roberta.encoder.layer.6.attention.self.value.weight False\n",
      "roberta.encoder.layer.6.attention.self.value.bias False\n",
      "roberta.encoder.layer.6.attention.output.dense.weight False\n",
      "roberta.encoder.layer.6.attention.output.dense.bias False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.6.intermediate.dense.weight False\n",
      "roberta.encoder.layer.6.intermediate.dense.bias False\n",
      "roberta.encoder.layer.6.output.dense.weight False\n",
      "roberta.encoder.layer.6.output.dense.bias False\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.7.attention.self.query.weight False\n",
      "roberta.encoder.layer.7.attention.self.query.bias False\n",
      "roberta.encoder.layer.7.attention.self.key.weight False\n",
      "roberta.encoder.layer.7.attention.self.key.bias False\n",
      "roberta.encoder.layer.7.attention.self.value.weight False\n",
      "roberta.encoder.layer.7.attention.self.value.bias False\n",
      "roberta.encoder.layer.7.attention.output.dense.weight False\n",
      "roberta.encoder.layer.7.attention.output.dense.bias False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.7.intermediate.dense.weight False\n",
      "roberta.encoder.layer.7.intermediate.dense.bias False\n",
      "roberta.encoder.layer.7.output.dense.weight False\n",
      "roberta.encoder.layer.7.output.dense.bias False\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.8.attention.self.query.weight False\n",
      "roberta.encoder.layer.8.attention.self.query.bias False\n",
      "roberta.encoder.layer.8.attention.self.key.weight False\n",
      "roberta.encoder.layer.8.attention.self.key.bias False\n",
      "roberta.encoder.layer.8.attention.self.value.weight False\n",
      "roberta.encoder.layer.8.attention.self.value.bias False\n",
      "roberta.encoder.layer.8.attention.output.dense.weight False\n",
      "roberta.encoder.layer.8.attention.output.dense.bias False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.8.intermediate.dense.weight False\n",
      "roberta.encoder.layer.8.intermediate.dense.bias False\n",
      "roberta.encoder.layer.8.output.dense.weight False\n",
      "roberta.encoder.layer.8.output.dense.bias False\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.9.attention.self.query.weight False\n",
      "roberta.encoder.layer.9.attention.self.query.bias False\n",
      "roberta.encoder.layer.9.attention.self.key.weight False\n",
      "roberta.encoder.layer.9.attention.self.key.bias False\n",
      "roberta.encoder.layer.9.attention.self.value.weight False\n",
      "roberta.encoder.layer.9.attention.self.value.bias False\n",
      "roberta.encoder.layer.9.attention.output.dense.weight False\n",
      "roberta.encoder.layer.9.attention.output.dense.bias False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.9.intermediate.dense.weight False\n",
      "roberta.encoder.layer.9.intermediate.dense.bias False\n",
      "roberta.encoder.layer.9.output.dense.weight False\n",
      "roberta.encoder.layer.9.output.dense.bias False\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.10.attention.self.query.weight False\n",
      "roberta.encoder.layer.10.attention.self.query.bias False\n",
      "roberta.encoder.layer.10.attention.self.key.weight False\n",
      "roberta.encoder.layer.10.attention.self.key.bias False\n",
      "roberta.encoder.layer.10.attention.self.value.weight False\n",
      "roberta.encoder.layer.10.attention.self.value.bias False\n",
      "roberta.encoder.layer.10.attention.output.dense.weight False\n",
      "roberta.encoder.layer.10.attention.output.dense.bias False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.10.intermediate.dense.weight False\n",
      "roberta.encoder.layer.10.intermediate.dense.bias False\n",
      "roberta.encoder.layer.10.output.dense.weight False\n",
      "roberta.encoder.layer.10.output.dense.bias False\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.11.attention.self.query.weight False\n",
      "roberta.encoder.layer.11.attention.self.query.bias False\n",
      "roberta.encoder.layer.11.attention.self.key.weight False\n",
      "roberta.encoder.layer.11.attention.self.key.bias False\n",
      "roberta.encoder.layer.11.attention.self.value.weight False\n",
      "roberta.encoder.layer.11.attention.self.value.bias False\n",
      "roberta.encoder.layer.11.attention.output.dense.weight False\n",
      "roberta.encoder.layer.11.attention.output.dense.bias False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.11.intermediate.dense.weight False\n",
      "roberta.encoder.layer.11.intermediate.dense.bias False\n",
      "roberta.encoder.layer.11.output.dense.weight False\n",
      "roberta.encoder.layer.11.output.dense.bias False\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias False\n",
      "lm_head.bias True\n",
      "lm_head.dense.weight True\n",
      "lm_head.dense.bias True\n",
      "lm_head.layer_norm.weight True\n",
      "lm_head.layer_norm.bias True\n"
     ]
    }
   ],
   "source": [
    "# Freeze parameters except the last head\n",
    "\n",
    "for name, param in model_b1.named_parameters():\n",
    "    if \"lm_head\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    print(name, param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class CodeNetDataset(Dataset):\n",
    "    def __init__(self, window_df, tokenizer):\n",
    "        self.window_df = window_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "        # Not sure how to apply tqdm (progress bar) for this; I plan to update soon\n",
    "        self.tokenized = tokenizer(list(window_df[\"window\"]), padding=True)\n",
    "        self.input_ids = self.tokenized[\"input_ids\"]\n",
    "        self.attention_mask = self.tokenized[\"attention_mask\"]\n",
    "        self.label = self.window_df[\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mask_token_index = self.input_ids[index].index(self.mask_token_id)\n",
    "        # String cannot be stored in a tensor. They need to be converted to numeric values first.\n",
    "        label_token_id = self.tokenizer.convert_tokens_to_ids(self.label[index])\n",
    "\n",
    "        return torch.tensor(self.input_ids[index]), torch.tensor(self.attention_mask[index]), torch.tensor(mask_token_index), torch.tensor(label_token_id)\n",
    "\n",
    "train_dataset = CodeNetDataset(window_train_df, tokenizer_b1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "valid_dataset = CodeNetDataset(window_valid_df, tokenizer_b1)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-tuning (Cosine Similarity)\n",
    "\n",
    "We only use the top_1 prediction as it showed the best performance in the baseline testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Cosine similarity loss is calculated for each batch, before backpropagation.\n",
    "def cosine_similarity_loss(prediction_logits, mask_token_indices, label_token_ids):\n",
    "\n",
    "    cosine_similarity_list = []\n",
    "    for row, mask_token_index, label_token_id in zip(range(prediction_logits.shape[0]), mask_token_indices, label_token_ids):\n",
    "        prediction_word_id = torch.argmax(prediction_logits[row, mask_token_index])\n",
    "        # Words will be stripped to remove unnecessary whitespaces.\n",
    "        prediction_word = tokenizer_b1.decode(prediction_word_id).strip()\n",
    "        label_word = tokenizer_b1.decode(label_token_id).strip()\n",
    "\n",
    "        # Computing cosine similarity\n",
    "        encoded_words = tokenizer_cs([label_word, prediction_word], padding=True, truncation=True, return_tensors='pt')\n",
    "        model_cs_output = model_cs(**encoded_words)\n",
    "        word_embeddings = mean_pooling(model_cs_output, encoded_words[\"attention_mask\"])\n",
    "        word_embeddings = F.normalize(word_embeddings, p=2, dim=1)\n",
    "        dot_product = torch.dot(word_embeddings[0, :], word_embeddings[1, :])\n",
    "        # dot_product is a zero-dimensional tensor. We need to add a dummy dimension at 1 for concatenation.\n",
    "        cosine_similarity_list.append(torch.unsqueeze(dot_product, dim=0) * -1)\n",
    "\n",
    "    # The mean cosine similarity for a batch\n",
    "    # -1 is multiplied because we are minimizing the loss\n",
    "    return torch.mean(torch.concat(cosine_similarity_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train set size: 11684, batch_size: 10, batch_number: 1169\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/1169 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40f60aea6cc64060b0d40060ad5fb896"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Evaluation:   0%|          | 0/64 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47be20d0e13d4f719dac78ce880ca1f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train cosine similarity 0.43422829970985116, Valid cosine similarity 0.4254048028960824\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2:   0%|          | 0/1169 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14530f38f676444aa3cf0b60f73ffbfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Evaluation:   0%|          | 0/64 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ec5a7ddf525448a9fa573971b5e08c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train cosine similarity 0.4352732971884227, Valid cosine similarity 0.4308343541342765\n"
     ]
    },
    {
     "data": {
      "text/plain": "RobertaForMaskedLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n  )\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running this cell multiple times in a single notebook can fully saturate the GPU memory, which leads the OutOfMemoryError.\n",
    "# If it happens, re-start the notebook kernel to remove all model instances from the GPU memory.\n",
    "# It seems like the memory error sometimes happens when the dataset size is too big, regardless of the batch size.\n",
    "# I am not sure why it is the case; Let me know if you encounter this issue.\n",
    "\n",
    "# Setting the model to the train mode\n",
    "model_b1.train()\n",
    "model_b1.to(device)\n",
    "\n",
    "print(f\"Total train set size: {len(train_dataset)}, batch_size: {batch_size}, batch_number: {math.ceil(len(train_dataset) / batch_size)}\")\n",
    "train_similarity = list()\n",
    "valid_similarity = list()\n",
    "\n",
    "# Using the AdamW optimizer: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "# Feel free to try others\n",
    "optimizer = torch.optim.AdamW(model_b1.parameters(), lr=learning_rate, weight_decay=weight_decay_coefficient)\n",
    "\n",
    "for epoch_num in range(1, total_epoch_number+1):\n",
    "    loss_list = list()\n",
    "    # model_b1.train()\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch_num}\", total=math.ceil(len(train_dataset) / batch_size)):\n",
    "        # Sending to GPU\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_mask_token_index = batch[2].to(device)\n",
    "        batch_label_token_id = batch[3].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        prediction_logits = model_b1(input_ids=batch_input_ids, attention_mask=batch_attention_mask).logits\n",
    "        # Computing prediction error\n",
    "        loss = cosine_similarity_loss(prediction_logits, batch_mask_token_index, batch_label_token_id)\n",
    "        # Removing gradients from the past iteration\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Removing to free up memory\n",
    "        del batch_input_ids\n",
    "        del batch_attention_mask\n",
    "        del batch_mask_token_index\n",
    "        del batch_label_token_id\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Computing the validation similarity for each epoch\n",
    "    # model_b1.eval()\n",
    "    eval_similarity = list()\n",
    "\n",
    "    for eval_batch in tqdm(valid_dataloader, desc=\"Evaluation\", total=math.ceil(len(valid_dataset) / batch_size)):\n",
    "        eval_input_ids = eval_batch[0].to(device)\n",
    "        eval_attention_mask = eval_batch[1].to(device)\n",
    "        eval_mask_token_index = eval_batch[2].to(device)\n",
    "        eval_label_token_id = eval_batch[3].to(device)\n",
    "\n",
    "        eval_logits = model_b1(input_ids=eval_input_ids, attention_mask=eval_attention_mask).logits\n",
    "        eval_similarity.append(cosine_similarity_loss(eval_logits, eval_mask_token_index, eval_label_token_id).item())\n",
    "\n",
    "        del eval_input_ids\n",
    "        del eval_attention_mask\n",
    "        del eval_mask_token_index\n",
    "        del eval_label_token_id\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch_num}, Train cosine similarity {-1 * np.mean(loss_list)}, Valid cosine similarity {-1 * np.mean(eval_similarity)}\")\n",
    "    train_similarity.append(-1 * np.mean(loss_list))\n",
    "    valid_similarity.append(-1 * np.mean(eval_similarity))\n",
    "\n",
    "model_b1.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation (Cosine similarity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x2f2d078a380>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment to draw loss plot\n",
    "plt.plot(range(1, len(train_similarity) + 1), train_similarity, label=\"Train similarity\")\n",
    "plt.plot(range(1, len(valid_similarity) + 1), valid_similarity, label=\"Valid similarity\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43422401505560915, 0.43412723115160445, 0.43484928970001946]\n"
     ]
    }
   ],
   "source": [
    "print(train_similarity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5064330228737423, 0.4944506457873753, 0.5197252290589469]\n"
     ]
    }
   ],
   "source": [
    "print(valid_similarity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Window split:   0%|          | 0/231 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "498249a6dcf2444e8e566e0f1def34e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Prediction:   0%|          | 0/633 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ea01bfacb2f48819c599fccb3c0919b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Similarity:   0%|          | 0/633 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00745580a4d44d6aa9792a29a80aa0fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b1_finetuned = pipeline('fill-mask', model=model_b1, tokenizer=tokenizer_b1, device=device_num)\n",
    "b1_finetuned_result = model_test(merged_code_df=merged_valid_df, unmasker=b1_finetuned, top_k=1, window_size=window_size, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity: 0.4904963970184326\n",
      "The fine-tuned model performance is better than the baseline, by 0.008820265531539917\n"
     ]
    }
   ],
   "source": [
    "baverage_similarity = np.mean(b1_finetuned_result['similarity'])\n",
    "# Baseline performance (I plan to send you the result by this evening)\n",
    "baseline_similarity = 0.4816761314868927\n",
    "print(f\"Average cosine similarity: {average_similarity}\")\n",
    "if average_similarity < baseline_similarity:\n",
    "    print(f\"The fine-tuned model performance is worse than the baseline, by {baseline_similarity-average_similarity}\")\n",
    "else:\n",
    "    print(f\"The fine-tuned model performance is better than the baseline, by {average_similarity-baseline_similarity}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the model\n",
    "\n",
    "Check the following for loading the model: https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Saving the hyperparameters and the fine-tuned model\n",
    "# The model size is around 500 MB. You may want to save the dictionary only\n",
    "training_constants = [batch_size, shuffle, size_proportion_train, total_epoch_number, learning_rate, weight_decay_coefficient, mask_prob, window_size, rng_seed, size_proportion_valid]\n",
    "result_dict = {\"training_constants\": training_constants,\n",
    "               \"train_similarity\": train_similarity,\n",
    "               \"valid_similarity\": valid_similarity,\n",
    "               \"loss_metric\": \"cosine_similarity\",\n",
    "               \"valid_final_similarity\": average_similarity}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Generating non-existing filenames\n",
    "file_index = 0\n",
    "while os.path.exists(f\"./saved_models/model_{file_index}\"):\n",
    "    file_index += 1\n",
    "\n",
    "model_filepath = f\"./saved_models/model_{file_index}\"\n",
    "hparameter_filepath = f\"./saved_models/hparameter_{file_index}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving the hyperparameters and results\n",
    "with open(hparameter_filepath, \"wb\") as fw:\n",
    "    pickle.dump(result_dict, fw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model_b1, model_filepath)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
