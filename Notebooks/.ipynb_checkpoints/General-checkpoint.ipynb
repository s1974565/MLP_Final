{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7fa15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Sentence similarity model\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Baseline model 1\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e477e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU acceleration is available\n",
    "if torch.cuda.is_available():\n",
    "    device_num = torch.cuda.current_device()\n",
    "else:\n",
    "    # CPU\n",
    "    device_num = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700cacb",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a61540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f, orient='records', compression='gzip', lines=True)[columns] for f in file_list], \n",
    "                     sort=False)\n",
    "def get_dfs(path):\n",
    "    \"\"\"Grabs the different data splits and converts them into dataframes\"\"\"\n",
    "    dfs = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted(glob.glob(path+\"/\"+split+\"**/*.gz\"))\n",
    "        df = jsonl_list_to_dataframe(files, [\"func_name\", \"code\", \"code_tokens\", \"repo\"])\n",
    "        dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ea4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving the original files into pickle files.\n",
    "# df_train, df_valid, df_test = get_dfs(\"data/codenet/python/final/jsonl\")\n",
    "\n",
    "# df_train.to_pickle(\"train.pickle\")\n",
    "# df_valid.to_pickle(\"valid.pickle\")\n",
    "# df_test.to_pickle(\"test.pickle\")\n",
    "\n",
    "df_train = pd.read_pickle(\"train.pickle\").reset_index(drop=True)\n",
    "df_valid = pd.read_pickle(\"valid.pickle\").reset_index(drop=True)\n",
    "df_test = pd.read_pickle(\"test.pickle\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9216b1",
   "metadata": {},
   "source": [
    "### Helper methods for baseline models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3825c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_print(input_sequence, unmasker, true_labels=None, top_k=2, mask_token=\"<mask>\"):\n",
    "    mask_num = input_sequence.count(mask_token)\n",
    "    output = unmasker(input_sequence, top_k=top_k)\n",
    "    if mask_num == 1:\n",
    "        print(\"-\" * 50)\n",
    "        if true_labels:\n",
    "            print(f\"True label: {true_labels[0]}\")\n",
    "            print(\"\")\n",
    "        for candidate in output:\n",
    "            print(f\"Predicted_word: {candidate['token_str']}\")\n",
    "            print(f\"Probability: {round(candidate['score'], 3)}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\")\n",
    "        \n",
    "    else:\n",
    "        for index, word_prediction in enumerate(output):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Mask number: {index}\")\n",
    "            if true_labels:\n",
    "                print(f\"True label: {true_labels[index]}\")\n",
    "                print(\"\")\n",
    "            for candidate in word_prediction:\n",
    "                print(f\"Predicted_word: {candidate['token_str']}\")\n",
    "                print(f\"Probability: {round(candidate['score'], 3)}\")\n",
    "            print(\"-\" * 50)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6a783b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def learn(env,\\n          network,\\n          seed=None,\\n          lr=5e-4,\\n          total_timesteps=100000,\\n          buffer_size=50000,\\n          exploration_fraction=0.1,\\n          exploration_final_eps=0.02,\\n          train_freq=1,\\n          batch_size=32,\\n          print_freq=100,\\n          checkpoint_freq=10000,\\n          checkpoint_path=None,\\n          learning_starts=1000,\\n          gamma=1.0,\\n          target_network_update_freq=500,\\n          prioritized_replay=False,\\n          prioritized_replay_alpha=0.6,\\n          prioritized_replay_beta0=0.4,\\n          prioritized_replay_beta_iters=None,\\n          prioritized_replay_eps=1e-6,\\n          param_noise=False,\\n          callback=None,\\n          load_path=None,\\n          **network_kwargs\\n            )',\n",
       " 'return a latent variable tensor, which\\n',\n",
       " 'returns true training stops.\\n',\n",
       " '# Create all the functions necessary to train the model\\n',\n",
       " 'sess = get_session()\\n',\n",
       " 'q_func = build_q_func(network, **network_kwargs)\\n',\n",
       " '# capture the shape outside the closure so that the env object is not serialized\\n',\n",
       " '# by cloudpickle when serializing make_obs_ph\\n',\n",
       " 'observation_space = env.observation_space\\n',\n",
       " 'def make_obs_ph(name)',\n",
       " 'return ObservationInput(observation_space, name=name)\\n',\n",
       " 'act, train, update_target, debug = deepq.build_train(\\n',\n",
       " 'make_obs_ph=make_obs_ph,\\n',\n",
       " 'q_func=q_func,\\n',\n",
       " 'num_actions=env.action_space.n,\\n',\n",
       " 'optimizer=tf.train.AdamOptimizer(learning_rate=lr),\\n',\n",
       " 'gamma=gamma,\\n',\n",
       " 'grad_norm_clipping=10,\\n',\n",
       " 'param_noise=param_noise\\n',\n",
       " 'act_params = {\\n',\n",
       " 'act = ActWrapper(act, act_params)\\n',\n",
       " '# Create the replay buffer\\n',\n",
       " 'replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\\n',\n",
       " 'prioritized_replay_beta_iters = total_timesteps\\n',\n",
       " 'beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\\n',\n",
       " 'initial_p=prioritized_replay_beta0,\\n',\n",
       " 'final_p=1.0)\\n',\n",
       " 'replay_buffer = ReplayBuffer(buffer_size)\\n',\n",
       " 'beta_schedule = None\\n',\n",
       " '# Create the schedule for exploration starting from 1.\\n',\n",
       " 'exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\\n',\n",
       " 'initial_p=1.0,\\n',\n",
       " 'final_p=exploration_final_eps)\\n',\n",
       " '# Initialize the parameters and copy them to the target network.\\n',\n",
       " 'episode_rewards = [0.0]\\n',\n",
       " 'saved_mean_reward = None\\n',\n",
       " 'obs = env.reset()\\n',\n",
       " 'reset = True\\n',\n",
       " 'td = checkpoint_path or td\\n',\n",
       " 'model_file = os.path.join(td, \"model\")\\n',\n",
       " 'model_saved = False\\n',\n",
       " 'model_saved = True\\n',\n",
       " '# Take action and update exploration to the newest value\\n',\n",
       " 'kwargs = {}\\n',\n",
       " 'update_eps = exploration.value(t)\\n',\n",
       " 'update_param_noise_threshold = 0.\\n',\n",
       " 'update_eps = 0.\\n',\n",
       " '# Compute the threshold such that the KL divergence between perturbed and non-perturbed\\n',\n",
       " '# policy is comparable to eps-greedy exploration with eps = exploration.value(t).\\n',\n",
       " '# See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\\n',\n",
       " '# for detailed explanation.\\n',\n",
       " 'update_param_noise_threshold = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\\n',\n",
       " 'True\\n            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\\n',\n",
       " 'env_action = action\\n',\n",
       " 'reset = False\\n',\n",
       " 'new_obs, rew, done, _ = env.step(env_action)\\n',\n",
       " '# Store transition in the replay buffer.\\n',\n",
       " 'obs = new_obs\\n',\n",
       " 'obs = env.reset()\\n',\n",
       " 'reset = True\\n',\n",
       " 'train_freq == 0:\\n',\n",
       " \"# Minimize the error in Bellman's equation on a batch sampled from replay buffer.\\n\",\n",
       " 'experience = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\\n',\n",
       " 'obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\\n',\n",
       " 'weights, batch_idxes = np.ones_like(rewards), None\\n',\n",
       " 'td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\\n',\n",
       " 'new_priorities = np.abs(td_errors) + prioritized_replay_eps\\n',\n",
       " 'target_network_update_freq == 0:\\n',\n",
       " '# Update target network periodically.\\n',\n",
       " 'mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\\n',\n",
       " 'num_episodes = len(episode_rewards)\\n',\n",
       " 'print_freq == 0:\\n',\n",
       " 'checkpoint_freq == 0):\\n',\n",
       " 'model_saved = True\\n',\n",
       " 'saved_mean_reward = mean_100ep_reward\\n',\n",
       " 'return act\\n']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable masker testing\n",
    "test = \"\"\"\n",
    "def learn(env,\n",
    "          network,\n",
    "          seed=None,\n",
    "          callback=None,\n",
    "          load_path=None,\n",
    "          **network_kwargs\n",
    "            ):\n",
    "    act: ActWrapper\n",
    "        Wrapper over act function. Adds ability to save it and load it.\n",
    "        See header of baselines/deepq/categorical.py for details on the act function.\n",
    "    # Create all the functions necessary to train the model\n",
    "\n",
    "    sess = get_session()\n",
    "    set_global_seeds(seed)\n",
    "\n",
    "    q_func = build_q_func(network, **network_kwargs)\n",
    "\n",
    "    # capture the shape outside the closure so that the env object is not serialized\n",
    "    # by cloudpickle when serializing make_obs_ph\n",
    "\n",
    "    observation_space = env.observation_space\n",
    "    def make_obs_ph(name):\n",
    "        return ObservationInput(observation_space, name=name)\n",
    "\n",
    "    act, train, update_target, debug = deepq.build_train(\n",
    "        make_obs_ph=make_obs_ph,\n",
    "        q_func=q_func,\n",
    "        num_actions=env.action_space.n,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
    "        gamma=gamma,\n",
    "        grad_norm_clipping=10,\n",
    "        param_noise=param_noise\n",
    "    )\n",
    "\n",
    "    act_params = {\n",
    "        'make_obs_ph': make_obs_ph,\n",
    "        'q_func': q_func,\n",
    "        'num_actions': env.action_space.n,\n",
    "    }\n",
    "\n",
    "    act = ActWrapper(act, act_params)\n",
    "\n",
    "    # Create the replay buffer\n",
    "    if prioritized_replay:\n",
    "        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "        if prioritized_replay_beta_iters is None:\n",
    "            prioritized_replay_beta_iters = total_timesteps\n",
    "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
    "                                       initial_p=prioritized_replay_beta0,\n",
    "                                       final_p=1.0)\n",
    "    else:\n",
    "        replay_buffer = ReplayBuffer(buffer_size)\n",
    "        beta_schedule = None\n",
    "    # Create the schedule for exploration starting from 1.\n",
    "    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
    "                                 initial_p=1.0,\n",
    "                                 final_p=exploration_final_eps)\n",
    "\"\"\"\n",
    "\n",
    "pattern = r\"(\\bdef\\s\\w*\\(.*?\\)):|(#\\s*.*?\\n)|(return\\s*.*?\\n)|(\\b[\\w,\\s]*=\\s*.*?\\n)\"\n",
    "matches = [str().join(x) for x in re.findall(pattern, test, flags=re.DOTALL)]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f6364509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic assumption: The same line of code never occurs twice.\n",
    "def mask_variable_names(code, mask_prob):\n",
    "    \"\"\"\n",
    "    Mask the values of variables in a code with a certain probability.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to match variable assignments\n",
    "    # Function signature (to be filtered out later) | common variable definitions\n",
    "    pattern = r\"(\\bdef\\s\\w*\\(.*?\\)):|(#\\s*.*?\\n)|(return\\s*.*?\\n)|(\\b[\\w,\\s]*=\\s*.*?\\n)\"\n",
    "    matches = [str().join(x) for x in re.findall(pattern, code, flags=re.DOTALL)]\n",
    "    var_indices = list()\n",
    "    var_labels = list()\n",
    "    # characters that should not exist in the first sub part of a found match.\n",
    "    invalid_list = [\"(\", \")\", \"def\", \"#\", \"return\"]\n",
    "    \n",
    "    # If there is a variable found\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            # Split the match into sub-parts by the equal sign, and check if the first sub-part contain any parenthesis\n",
    "            # or \"def\" (implies function signature).\n",
    "            # If not, then the first sub-part is variable(s).\n",
    "            first_sub_part = match.split(\"=\")[0]\n",
    "            if not any([invalid_character in first_sub_part for invalid_character in invalid_list]):\n",
    "                variables = set(re.split(\",|=\", first_sub_part))\n",
    "                \n",
    "                # Masking variables based on the mask_prob\n",
    "                masked_match = str(match)\n",
    "                match_begin_index = code.find(masked_match)\n",
    "                for var in variables:\n",
    "                    # If beginning of the function call, then process no further.\n",
    "                    if \"(\" in var:\n",
    "                        break\n",
    "                    if np.random.uniform() < mask_prob:\n",
    "                        var_begin_index = masked_match.find(var.strip())\n",
    "                        var_index = (var_begin_index + match_begin_index, var_begin_index + match_begin_index + len(var.strip()))\n",
    "                        var_indices.append(var_index)\n",
    "                        var_labels.append(var.strip())\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return var_indices, var_labels\n",
    "    \n",
    "    # If no variable is found\n",
    "    else:\n",
    "        return code, list()\n",
    "        \n",
    "def mask_variable_df(df, code_column_name=\"code\", mask_prob=0.5, return_df=True):\n",
    "    variable_indices_list = list()\n",
    "    variable_labels_list = list()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        variable_indices, variable_labels = mask_variable_names(row[\"code\"], mask_prob)\n",
    "        variable_indices_list.append(variable_indices)\n",
    "        variable_labels_list.append(variable_labels)\n",
    "        \n",
    "    if return_df:\n",
    "        return pd.DataFrame({\"variable_indices\" : variable_indices_list, \"variable_labels\" : variable_labels_list})\n",
    "    else:\n",
    "        return variable_indices_list, variable_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bffe94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_se = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "def cosine_similarity(sentences, model=model_se):\n",
    "    embeddings = model.encode(sentences)\n",
    "    return np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e344f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_docstring(code):\n",
    "    pattern = r'(\"\"\".*?\"\"\")|(\\'\\'\\'.*?\\'\\'\\')'\n",
    "    return re.sub(pattern, '', code, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b02f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_substring_indices(text, substring):\n",
    "    pattern = re.compile(f'{substring}')\n",
    "    indices = [(match.start(), match.end()-1) for match in pattern.finditer(text)]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3c7e3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_windows(row, window_size, mask_token):\n",
    "    windows = list()\n",
    "    \n",
    "    for variable_indices, variable_labels in zip(row[\"variable_indices\"], row[\"variable_labels\"]):\n",
    "        # Window indices\n",
    "        begin_index = variable_indices[0] - window_size if variable_indices[0] - window_size > 0 else 0\n",
    "        end_index = variable_indices[1] + window_size if variable_indices[1] + window_size < len(row[\"code\"]) else len(row[\"code\"])\n",
    "        \n",
    "        current_window = row[\"code\"][begin_index : variable_indices[0]] + mask_token + row[\"code\"][variable_indices[1] : end_index]\n",
    "        windows.append(current_window)\n",
    "        \n",
    "#         print(current_window)\n",
    "#         print(\"--------\")\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12111036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the prediction to the given masked code. If top_k is bigger than 1, then the top_k predictions\n",
    "# will be concatenated by the given top_k_connection. Each prediction(s) will be stripped to remove unnecessary whitespaces.\n",
    "def mask_prediction(row, top_k, unmasker, top_k_connection, mask_token, window_size):\n",
    "    mask_num = len(row[\"variable_indices\"])\n",
    "    predictions = list()\n",
    "    \n",
    "    if mask_num == 0:\n",
    "        return predictions\n",
    "    \n",
    "#     elif mask_num == 1:\n",
    "#         window = split_into_windows(masked_code, window_size, mask_token, true_labels)\n",
    "#         output = unmasker(window, top_k=top_k)\n",
    "#         candidate_concat = top_k_connection.join([candidate['token_str'].strip() for candidate in output])\n",
    "#         predictions.append(candidate_concat)\n",
    "#         return predictions\n",
    "    \n",
    "    else:\n",
    "        windows = split_into_windows(row, window_size, mask_token)\n",
    "        for window in windows:\n",
    "            output = unmasker(window, top_k=top_k)\n",
    "            candidate_concat = top_k_connection.join([candidate['token_str'].strip() for candidate in output])\n",
    "            predictions.append(candidate_concat)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "355c551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the given code dataframe, it automatically masks the codes and fill the masks by the supplied unmasker.\n",
    "# The predicted results are then compared with the true labels, with cosine similarity.\n",
    "# If top_k is set bigger than 1, then top_k number of predictions will be concatenated to form a single predictions\n",
    "# by the top_k_connection (default to the underscore).\n",
    "# For example, if the predictions are: \"A\", \"B\", and \"C\", then top_k = 2, the final prediction will be \"A_B\".\n",
    "\n",
    "# Currently runtime errors will be ignored. Runtime errors happen when the given code is longer than the maximum\n",
    "# size of the unmasker model (512 tokens)\n",
    "\n",
    "# Pre-trained transformers typically can take up to 512 tokens. Thus, if the given code is larger than this,\n",
    "# then a RuntimeError will be raised. To avoid this, the window_size variable is added. It regulates the amount of\n",
    "# context which will be give to the unmasker. If it is set to 100, total 200 characters will be given to the unmasker:\n",
    "# 100 characters before the mask token, and 100 characters after the mask token.\n",
    "# For example, 100 characters <mask> 100 characters\n",
    "def baseline_test(code_df, unmasker, mask_token=\"<mask>\", mask_prob=0.5, top_k=1, top_k_connection=\"_\", \n",
    "                  code_column_name=\"code\", window_size=100):\n",
    "    \n",
    "    masked_code_df = mask_variable_df(code_df, mask_prob=mask_prob, code_column_name=code_column_name)\n",
    "    merged_code_df = pd.concat([code_df, masked_code_df], axis=\"columns\")\n",
    "    \n",
    "    similarity_scores_list = list()\n",
    "    predictions_list = list()\n",
    "    true_labels_list = list()\n",
    "    \n",
    "    total_size = len(code_df)\n",
    "    for index, row in merged_code_df.iterrows():\n",
    "        if index % 1000 == 0:\n",
    "            print(f\"Progress: {round(index / total_size, 3) * 100}%\")\n",
    "        \n",
    "        true_labels = row[\"variable_labels\"]\n",
    "        true_labels_list.append(true_labels)\n",
    "        \n",
    "        # If the current code snippet is longer than the maximum input size of the given unmasker \n",
    "        # then the runtime error will be raised. Try to reduce window_size.\n",
    "        try:\n",
    "            predictions = mask_prediction(row, top_k, unmasker, top_k_connection, mask_token, window_size)\n",
    "            predictions_list.append(predictions)\n",
    "        except RuntimeError:\n",
    "            raise RuntimeError(\"The given input size is bigger than the maximum model input. Reduce the window size.\")\n",
    "        \n",
    "        similarity_scores = list()\n",
    "        for prediction, true_label in zip(predictions, true_labels):\n",
    "            similarity_scores.append(cosine_similarity([prediction, true_label]))\n",
    "        similarity_scores_list.append(similarity_scores)\n",
    "        \n",
    "    return predictions_list, true_labels_list, similarity_scores_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc2c9f",
   "metadata": {},
   "source": [
    "### Baseline score 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9532f1a",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/microsoft/codebert-base-mlm <br>\n",
    "As stated in https://github.com/microsoft/CodeBERT, the basic CobeBERT is not suitable for filling-mask task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5af3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b1 = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base-mlm')\n",
    "tokenizer_b1 = RobertaTokenizer.from_pretrained('microsoft/codebert-base-mlm')\n",
    "fill_mask_b1 = pipeline('fill-mask', model=model_b1, tokenizer=tokenizer_b1, device=device_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55f2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted_word:  and\n",
      "Probability: 0.724\n",
      "Predicted_word:  &\n",
      "Probability: 0.106\n",
      "Predicted_word: and\n",
      "Probability: 0.022\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_example = \"if (x is not None) <mask> (x>1)\"\n",
    "output_print(code_example, fill_mask_b1, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d5b2a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40455922"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "model_ss = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "cosine_similarity(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec4b749f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinyoung\\anaconda3\\envs\\mlp\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# It takes a lot of time, even though GPU acceleration is applied. It is because of many variables in each code snippet\n",
    "# and using the dataframe, not the huggingface api dataset. It should be refactored soon.\n",
    "# I do recommend to load the pickle files I saved below, instead of running this again.\n",
    "\n",
    "# You can just implement another metric and use it instead (implementation of perplexity should be straightforward).\n",
    "\n",
    "# top_k = 1\n",
    "b1_result_k1 = baseline_test(df_valid, fill_mask_b1, top_k=1)\n",
    "print(\"Finished\")\n",
    "\n",
    "# top_k = 2\n",
    "b1_result_k2 = baseline_test(df_valid, fill_mask_b1, top_k=2, mask_prob=1)\n",
    "print(\"Finished\")\n",
    "\n",
    "# top_k = 3\n",
    "b1_result_k3 = baseline_test(df_valid, fill_mask_b1, top_k=3)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ef515",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"b1_result_k1.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(b1_result_k1, fw)\n",
    "    \n",
    "with open(\"b1_result_k2\",\"wb\") as fw:\n",
    "    pickle.dump(b1_result_k2, fw)\n",
    "    \n",
    "with open(\"b1_result_k3\",\"wb\") as fw:\n",
    "    pickle.dump(b1_result_k3, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0ce23",
   "metadata": {},
   "source": [
    "### For debug (Printing out all windows)\n",
    "\n",
    "To use, uncomment the following two lines in the function \"split_into_windows\" <br>\n",
    "print(current_window) <br>\n",
    "print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2aa7f3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0%\n",
      "tails on the act function.\n",
      "    \"\"\"\n",
      "    # Create all the functions necessary to train the model\n",
      "\n",
      "    <mask> = get_session()\n",
      "    set_global_seeds(seed)\n",
      "\n",
      "    q_func = build_q_func(network, **network_kwargs)\n",
      "\n",
      " \n",
      "--------\n",
      "he functions necessary to train the model\n",
      "\n",
      "    sess = get_session()\n",
      "    set_global_seeds(seed)\n",
      "\n",
      "    <mask> = build_q_func(network, **network_kwargs)\n",
      "\n",
      "    # capture the shape outside the closure so that the \n",
      "--------\n",
      "ure so that the env object is not serialized\n",
      "    # by cloudpickle when serializing make_obs_ph\n",
      "\n",
      "    <mask> = env.observation_space\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_spac\n",
      "--------\n",
      "space\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    <mask>, train, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q\n",
      "--------\n",
      "ame):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, train, update_target, <mask> = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=en\n",
      "--------\n",
      "\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, <mask>, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "\n",
      "--------\n",
      "f make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, train, <mask>, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_act\n",
      "--------\n",
      "ut(observation_space, name=name)\n",
      "\n",
      "    act, train, update_target, debug = deepq.build_train(\n",
      "        <mask>=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        optimizer=tf.tr\n",
      "--------\n",
      "\n",
      "    act, train, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        <mask>=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_r\n",
      "--------\n",
      "_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        <mask>=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma\n",
      "--------\n",
      "    make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        <mask>=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "     \n",
      "--------\n",
      "num_actions=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        <mask>=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "   \n",
      "--------\n",
      "n_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        <mask>=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      " \n",
      "--------\n",
      ".train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        <mask>=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "--------\n",
      "lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    <mask> = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "        'num_actions': env.action\n",
      "--------\n",
      "s_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "        'num_actions': env.action_space.n,\n",
      "    }\n",
      "\n",
      "    <mask> = ActWrapper(act, act_params)\n",
      "\n",
      "    # Create the replay buffer\n",
      "    if prioritized_replay:\n",
      "        re\n",
      "--------\n",
      "ct = ActWrapper(act, act_params)\n",
      "\n",
      "    # Create the replay buffer\n",
      "    if prioritized_replay:\n",
      "        <mask> = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
      "        if prioritized_repla\n",
      "--------\n",
      "size, alpha=prioritized_replay_alpha)\n",
      "        if prioritized_replay_beta_iters is None:\n",
      "            <mask> = total_timesteps\n",
      "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
      "           \n",
      "--------\n",
      "ized_replay_beta_iters is None:\n",
      "            prioritized_replay_beta_iters = total_timesteps\n",
      "        <mask> = LinearSchedule(prioritized_replay_beta_iters,\n",
      "                                       initial_p=pr\n",
      "--------\n",
      "eta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
      "                                       <mask>=prioritized_replay_beta0,\n",
      "                                       final_p=1.0)\n",
      "    else:\n",
      "        rep\n",
      "--------\n",
      "                         initial_p=prioritized_replay_beta0,\n",
      "                                       <mask>=1.0)\n",
      "    else:\n",
      "        replay_buffer = ReplayBuffer(buffer_size)\n",
      "        beta_schedule = None\n",
      "    #\n",
      "--------\n",
      "l_p=prioritized_replay_beta0,\n",
      "                                       final_p=1.0)\n",
      "    else:\n",
      "        <mask> = ReplayBuffer(buffer_size)\n",
      "        beta_schedule = None\n",
      "    # Create the schedule for exploration \n",
      "--------\n",
      "                   final_p=1.0)\n",
      "    else:\n",
      "        replay_buffer = ReplayBuffer(buffer_size)\n",
      "        <mask> = None\n",
      "    # Create the schedule for exploration starting from 1.\n",
      "    exploration = LinearSchedule(\n",
      "--------\n",
      "r_size)\n",
      "        beta_schedule = None\n",
      "    # Create the schedule for exploration starting from 1.\n",
      "    <mask> = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
      "                  \n",
      "--------\n",
      "le(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
      "                                 <mask>=1.0,\n",
      "                                 final_p=exploration_final_eps)\n",
      "\n",
      "    # Initialize the paramete\n",
      "--------\n",
      " total_timesteps),\n",
      "                                 initial_p=1.0,\n",
      "                                 <mask>=exploration_final_eps)\n",
      "\n",
      "    # Initialize the parameters and copy them to the target network.\n",
      "    U.\n",
      "--------\n",
      "ize the parameters and copy them to the target network.\n",
      "    U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    <mask> = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.Temp\n",
      "--------\n",
      "them to the target network.\n",
      "    U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    <mask> = None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "      \n",
      "--------\n",
      "   U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    <mask> = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoi\n",
      "--------\n",
      "update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    <mask> = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        m\n",
      "--------\n",
      " None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        <mask> = checkpoint_path or td\n",
      "\n",
      "        model_file = os.path.join(td, \"model\")\n",
      "        model_saved = False\n",
      "--------\n",
      "t = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        <mask> = os.path.join(td, \"model\")\n",
      "        model_saved = False\n",
      "\n",
      "        if tf.train.latest_checkpoint(td) \n",
      "--------\n",
      ") as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        model_file = os.path.join(td, \"model\")\n",
      "        <mask> = False\n",
      "\n",
      "        if tf.train.latest_checkpoint(td) is not None:\n",
      "            load_variables(model_fi\n",
      "--------\n",
      "variables(model_file)\n",
      "            logger.log('Loaded model from {}'.format(model_file))\n",
      "            <mask> = True\n",
      "        elif load_path is not None:\n",
      "            load_variables(load_path)\n",
      "            logger\n",
      "--------\n",
      "             break\n",
      "            # Take action and update exploration to the newest value\n",
      "            <mask> = {}\n",
      "            if not param_noise:\n",
      "                update_eps = exploration.value(t)\n",
      "            \n",
      "--------\n",
      "oration to the newest value\n",
      "            kwargs = {}\n",
      "            if not param_noise:\n",
      "                <mask> = exploration.value(t)\n",
      "                update_param_noise_threshold = 0.\n",
      "            else:\n",
      "        \n",
      "--------\n",
      "}\n",
      "            if not param_noise:\n",
      "                update_eps = exploration.value(t)\n",
      "                <mask> = 0.\n",
      "            else:\n",
      "                update_eps = 0.\n",
      "                # Compute the threshold such\n",
      "--------\n",
      "ration.value(t)\n",
      "                update_param_noise_threshold = 0.\n",
      "            else:\n",
      "                <mask> = 0.\n",
      "                # Compute the threshold such that the KL divergence between perturbed and non-\n",
      "--------\n",
      " for Exploration, Plappert et al., 2017\n",
      "                # for detailed explanation.\n",
      "                <mask> = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\n",
      "           \n",
      "--------\n",
      "ise_threshold'] = update_param_noise_threshold\n",
      "                kwargs['update_param_noise_scale'] = <mask> = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "    \n",
      "--------\n",
      " True\n",
      "            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            <mask> = action\n",
      "            reset = False\n",
      "            new_obs, rew, done, _ = env.step(env_action)\n",
      "       \n",
      "--------\n",
      "p.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            <mask> = False\n",
      "            new_obs, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in \n",
      "--------\n",
      "s=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new<mask>obs, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "      \n",
      "--------\n",
      "_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            <mask>, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "         \n",
      "--------\n",
      "s, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new_obs, rew, <mask>, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "            replay_b\n",
      "--------\n",
      "te_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new_obs, <mask>, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "            re\n",
      "--------\n",
      "he replay buffer.\n",
      "            replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
      "            <mask> = new_obs\n",
      "\n",
      "            episode_rewards[-1] += rew\n",
      "            if done:\n",
      "                obs = env.re\n",
      "--------\n",
      "   U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    <mask> = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoi\n",
      "--------\n",
      "update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    <mask> = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        m\n",
      "--------\n",
      "pisode_rewards.append(0.0)\n",
      "                reset = True\n",
      "\n",
      "            if t > learning_starts and t % <mask> == 0:\n",
      "                # Minimize the error in Bellman's equation on a batch sampled from replay buf\n",
      "--------\n",
      "n on a batch sampled from replay buffer.\n",
      "                if prioritized_replay:\n",
      "                    <mask> = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n",
      "                    (obses_t, actio\n",
      "--------\n",
      "rds, obses_tp1, dones, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    <mask>, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights,\n",
      "--------\n",
      "nes, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, <mask>, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.\n",
      "--------\n",
      "hts, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, rewards, <mask>, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(r\n",
      "--------\n",
      "idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, rewards, obses_tp1, <mask> = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(rewards)\n",
      "--------\n",
      "s_tp1, dones, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, <mask>, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_id\n",
      "--------\n",
      " obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    <mask>, batch_idxes = np.ones_like(rewards), None\n",
      "                td_errors = train(obses_t, actions, rewa\n",
      "--------\n",
      " actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, <mask> = np.ones_like(rewards), None\n",
      "                td_errors = train(obses_t, actions, rewards, obses_tp\n",
      "--------\n",
      "(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(rewards), None\n",
      "                <mask> = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
      "                if prioritized_replay\n",
      "--------\n",
      "ons, rewards, obses_tp1, dones, weights)\n",
      "                if prioritized_replay:\n",
      "                    <mask> = np.abs(td_errors) + prioritized_replay_eps\n",
      "                    replay_buffer.update_priorities(ba\n",
      "--------\n",
      "y_buffer.update_priorities(batch_idxes, new_priorities)\n",
      "\n",
      "            if t > learning_starts and t % <mask> == 0:\n",
      "                # Update target network periodically.\n",
      "                update_target()\n",
      "\n",
      "      \n",
      "--------\n",
      "\n",
      "                # Update target network periodically.\n",
      "                update_target()\n",
      "\n",
      "            <mask> = round(np.mean(episode_rewards[-101:-1]), 1)\n",
      "            num_episodes = len(episode_rewards)\n",
      "     \n",
      "--------\n",
      "e_target()\n",
      "\n",
      "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
      "            <mask> = len(episode_rewards)\n",
      "            if done and print_freq is not None and len(episode_rewards) % pr\n",
      "--------\n",
      "es = len(episode_rewards)\n",
      "            if done and print_freq is not None and len(episode_rewards) % <mask> == 0:\n",
      "                logger.record_tabular(\"steps\", t)\n",
      "                logger.record_tabular(\"epis\n",
      "--------\n",
      "ckpoint_freq is not None and t > learning_starts and\n",
      "                    num_episodes > 100 and t % <mask> == 0):\n",
      "                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n",
      "     \n",
      "--------\n",
      "variables(model_file)\n",
      "            logger.log('Loaded model from {}'.format(model_file))\n",
      "            <mask> = True\n",
      "        elif load_path is not None:\n",
      "            load_variables(load_path)\n",
      "            logger\n",
      "--------\n",
      "              save_variables(model_file)\n",
      "                    model_saved = True\n",
      "                    <mask> = mean_100ep_reward\n",
      "        if model_saved:\n",
      "            if print_freq is not None:\n",
      "                \n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinyoung\\anaconda3\\envs\\mlp\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['#',\n",
       "   'q',\n",
       "   '*',\n",
       "   'obs',\n",
       "   '_',\n",
       "   'target',\n",
       "   'log',\n",
       "   'fn',\n",
       "   'q',\n",
       "   'n',\n",
       "   'model',\n",
       "   'gamma',\n",
       "   'beta',\n",
       "   'noise',\n",
       "   'env',\n",
       "   'act',\n",
       "   'buff',\n",
       "   'beta',\n",
       "   'schedule',\n",
       "   'beta',\n",
       "   'beta',\n",
       "   'b',\n",
       "   'p',\n",
       "   'schedule',\n",
       "   'p',\n",
       "   'steps',\n",
       "   '/',\n",
       "   'obs',\n",
       "   '#',\n",
       "   'obs',\n",
       "   'td',\n",
       "   '+',\n",
       "   'train',\n",
       "   'result',\n",
       "   't',\n",
       "   't',\n",
       "   '/',\n",
       "   'threshold',\n",
       "   'p',\n",
       "   'action',\n",
       "   'reset',\n",
       "   'done',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '/',\n",
       "   '#',\n",
       "   'obs',\n",
       "   '2',\n",
       "   't',\n",
       "   'experience',\n",
       "   'rewards',\n",
       "   'losses',\n",
       "   'weights',\n",
       "   'weights',\n",
       "   '#',\n",
       "   '_',\n",
       "   'acc',\n",
       "   'ba',\n",
       "   '2',\n",
       "   '/',\n",
       "   'pr',\n",
       "   '2',\n",
       "   '2',\n",
       "   'result',\n",
       "   '*']],\n",
       " [['sess',\n",
       "   'q_func',\n",
       "   'observation_space',\n",
       "   'act',\n",
       "   'debug',\n",
       "   'train',\n",
       "   'update_target',\n",
       "   'make_obs_ph',\n",
       "   'q_func',\n",
       "   'num_actions',\n",
       "   'optimizer',\n",
       "   'gamma',\n",
       "   'grad_norm_clipping',\n",
       "   'param_noise',\n",
       "   'act_params',\n",
       "   'act',\n",
       "   'replay_buffer',\n",
       "   'prioritized_replay_beta_iters',\n",
       "   'beta_schedule',\n",
       "   'initial_p',\n",
       "   'final_p',\n",
       "   'replay_buffer',\n",
       "   'beta_schedule',\n",
       "   'exploration',\n",
       "   'initial_p',\n",
       "   'final_p',\n",
       "   'episode_rewards',\n",
       "   'saved_mean_reward',\n",
       "   'obs',\n",
       "   'reset',\n",
       "   'td',\n",
       "   'model_file',\n",
       "   'model_saved',\n",
       "   'model_saved',\n",
       "   'kwargs',\n",
       "   'update_eps',\n",
       "   'update_param_noise_threshold',\n",
       "   'update_eps',\n",
       "   'update_param_noise_threshold',\n",
       "   'True\\n            action',\n",
       "   'env_action',\n",
       "   'reset',\n",
       "   '_',\n",
       "   'new_obs',\n",
       "   'done',\n",
       "   'rew',\n",
       "   'obs',\n",
       "   'obs',\n",
       "   'reset',\n",
       "   'train_freq',\n",
       "   'experience',\n",
       "   'obses_t',\n",
       "   'rewards',\n",
       "   'obses_tp1',\n",
       "   'dones',\n",
       "   'actions',\n",
       "   'weights',\n",
       "   'batch_idxes',\n",
       "   'td_errors',\n",
       "   'new_priorities',\n",
       "   'target_network_update_freq',\n",
       "   'mean_100ep_reward',\n",
       "   'num_episodes',\n",
       "   'print_freq',\n",
       "   'checkpoint_freq',\n",
       "   'model_saved',\n",
       "   'saved_mean_reward']],\n",
       " [[0.2947819,\n",
       "   0.41529685,\n",
       "   0.1636069,\n",
       "   0.21532297,\n",
       "   0.13182333,\n",
       "   0.36016828,\n",
       "   0.07122346,\n",
       "   0.07105673,\n",
       "   0.41529685,\n",
       "   0.25388032,\n",
       "   0.3253739,\n",
       "   1.0000001,\n",
       "   0.06346449,\n",
       "   0.60662854,\n",
       "   0.20330966,\n",
       "   1.0000001,\n",
       "   0.30218685,\n",
       "   0.46546584,\n",
       "   0.5948588,\n",
       "   0.16273127,\n",
       "   0.27964023,\n",
       "   0.101940885,\n",
       "   0.09043784,\n",
       "   0.19328178,\n",
       "   0.34524935,\n",
       "   0.26112336,\n",
       "   0.15396966,\n",
       "   0.102608435,\n",
       "   0.19715221,\n",
       "   0.18008484,\n",
       "   1.0000001,\n",
       "   0.04936334,\n",
       "   0.1320903,\n",
       "   0.11304621,\n",
       "   0.24170542,\n",
       "   0.078141645,\n",
       "   0.015610911,\n",
       "   -0.022503989,\n",
       "   0.08729151,\n",
       "   0.7902819,\n",
       "   0.11731528,\n",
       "   0.3038026,\n",
       "   1.0,\n",
       "   0.25797737,\n",
       "   0.25104502,\n",
       "   0.29512385,\n",
       "   0.23818575,\n",
       "   0.19715221,\n",
       "   0.18008484,\n",
       "   0.1965228,\n",
       "   0.2609732,\n",
       "   0.28792956,\n",
       "   1.0,\n",
       "   0.11206854,\n",
       "   0.13446906,\n",
       "   0.25120395,\n",
       "   0.17106567,\n",
       "   0.03399688,\n",
       "   0.054185577,\n",
       "   0.14459443,\n",
       "   0.06412354,\n",
       "   0.1709067,\n",
       "   0.13339606,\n",
       "   0.056961883,\n",
       "   0.12620357,\n",
       "   0.11304621,\n",
       "   0.20381415]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline_test(df_valid.head(1), fill_mask_b1, top_k=1, mask_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "70f32e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def learn(env,\n",
      "          network,\n",
      "          seed=None,\n",
      "          lr=5e-4,\n",
      "          total_timesteps=100000,\n",
      "          buffer_size=50000,\n",
      "          exploration_fraction=0.1,\n",
      "          exploration_final_eps=0.02,\n",
      "          train_freq=1,\n",
      "          batch_size=32,\n",
      "          print_freq=100,\n",
      "          checkpoint_freq=10000,\n",
      "          checkpoint_path=None,\n",
      "          learning_starts=1000,\n",
      "          gamma=1.0,\n",
      "          target_network_update_freq=500,\n",
      "          prioritized_replay=False,\n",
      "          prioritized_replay_alpha=0.6,\n",
      "          prioritized_replay_beta0=0.4,\n",
      "          prioritized_replay_beta_iters=None,\n",
      "          prioritized_replay_eps=1e-6,\n",
      "          param_noise=False,\n",
      "          callback=None,\n",
      "          load_path=None,\n",
      "          **network_kwargs\n",
      "            ):\n",
      "    \"\"\"Train a deepq model.\n",
      "\n",
      "    Parameters\n",
      "    -------\n",
      "    env: gym.Env\n",
      "        environment to train on\n",
      "    network: string or a function\n",
      "        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n",
      "        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n",
      "        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n",
      "    seed: int or None\n",
      "        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n",
      "    lr: float\n",
      "        learning rate for adam optimizer\n",
      "    total_timesteps: int\n",
      "        number of env steps to optimizer for\n",
      "    buffer_size: int\n",
      "        size of the replay buffer\n",
      "    exploration_fraction: float\n",
      "        fraction of entire training period over which the exploration rate is annealed\n",
      "    exploration_final_eps: float\n",
      "        final value of random action probability\n",
      "    train_freq: int\n",
      "        update the model every `train_freq` steps.\n",
      "        set to None to disable printing\n",
      "    batch_size: int\n",
      "        size of a batched sampled from replay buffer for training\n",
      "    print_freq: int\n",
      "        how often to print out training progress\n",
      "        set to None to disable printing\n",
      "    checkpoint_freq: int\n",
      "        how often to save the model. This is so that the best version is restored\n",
      "        at the end of the training. If you do not wish to restore the best version at\n",
      "        the end of the training set this variable to None.\n",
      "    learning_starts: int\n",
      "        how many steps of the model to collect transitions for before learning starts\n",
      "    gamma: float\n",
      "        discount factor\n",
      "    target_network_update_freq: int\n",
      "        update the target network every `target_network_update_freq` steps.\n",
      "    prioritized_replay: True\n",
      "        if True prioritized replay buffer will be used.\n",
      "    prioritized_replay_alpha: float\n",
      "        alpha parameter for prioritized replay buffer\n",
      "    prioritized_replay_beta0: float\n",
      "        initial value of beta for prioritized replay buffer\n",
      "    prioritized_replay_beta_iters: int\n",
      "        number of iterations over which beta will be annealed from initial value\n",
      "        to 1.0. If set to None equals to total_timesteps.\n",
      "    prioritized_replay_eps: float\n",
      "        epsilon to add to the TD errors when updating priorities.\n",
      "    param_noise: bool\n",
      "        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n",
      "    callback: (locals, globals) -> None\n",
      "        function called at every steps with state of the algorithm.\n",
      "        If callback returns true training stops.\n",
      "    load_path: str\n",
      "        path to load the model from. (default: None)\n",
      "    **network_kwargs\n",
      "        additional keyword arguments to pass to the network builder.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    act: ActWrapper\n",
      "        Wrapper over act function. Adds ability to save it and load it.\n",
      "        See header of baselines/deepq/categorical.py for details on the act function.\n",
      "    \"\"\"\n",
      "    # Create all the functions necessary to train the model\n",
      "\n",
      "    sess = get_session()\n",
      "    set_global_seeds(seed)\n",
      "\n",
      "    q_func = build_q_func(network, **network_kwargs)\n",
      "\n",
      "    # capture the shape outside the closure so that the env object is not serialized\n",
      "    # by cloudpickle when serializing make_obs_ph\n",
      "\n",
      "    observation_space = env.observation_space\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, train, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "        'num_actions': env.action_space.n,\n",
      "    }\n",
      "\n",
      "    act = ActWrapper(act, act_params)\n",
      "\n",
      "    # Create the replay buffer\n",
      "    if prioritized_replay:\n",
      "        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
      "        if prioritized_replay_beta_iters is None:\n",
      "            prioritized_replay_beta_iters = total_timesteps\n",
      "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
      "                                       initial_p=prioritized_replay_beta0,\n",
      "                                       final_p=1.0)\n",
      "    else:\n",
      "        replay_buffer = ReplayBuffer(buffer_size)\n",
      "        beta_schedule = None\n",
      "    # Create the schedule for exploration starting from 1.\n",
      "    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
      "                                 initial_p=1.0,\n",
      "                                 final_p=exploration_final_eps)\n",
      "\n",
      "    # Initialize the parameters and copy them to the target network.\n",
      "    U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        model_file = os.path.join(td, \"model\")\n",
      "        model_saved = False\n",
      "\n",
      "        if tf.train.latest_checkpoint(td) is not None:\n",
      "            load_variables(model_file)\n",
      "            logger.log('Loaded model from {}'.format(model_file))\n",
      "            model_saved = True\n",
      "        elif load_path is not None:\n",
      "            load_variables(load_path)\n",
      "            logger.log('Loaded model from {}'.format(load_path))\n",
      "\n",
      "\n",
      "        for t in range(total_timesteps):\n",
      "            if callback is not None:\n",
      "                if callback(locals(), globals()):\n",
      "                    break\n",
      "            # Take action and update exploration to the newest value\n",
      "            kwargs = {}\n",
      "            if not param_noise:\n",
      "                update_eps = exploration.value(t)\n",
      "                update_param_noise_threshold = 0.\n",
      "            else:\n",
      "                update_eps = 0.\n",
      "                # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
      "                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
      "                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
      "                # for detailed explanation.\n",
      "                update_param_noise_threshold = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\n",
      "                kwargs['reset'] = reset\n",
      "                kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
      "                kwargs['update_param_noise_scale'] = True\n",
      "            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new_obs, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "            replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
      "            obs = new_obs\n",
      "\n",
      "            episode_rewards[-1] += rew\n",
      "            if done:\n",
      "                obs = env.reset()\n",
      "                episode_rewards.append(0.0)\n",
      "                reset = True\n",
      "\n",
      "            if t > learning_starts and t % train_freq == 0:\n",
      "                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
      "                if prioritized_replay:\n",
      "                    experience = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n",
      "                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(rewards), None\n",
      "                td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
      "                if prioritized_replay:\n",
      "                    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n",
      "                    replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
      "\n",
      "            if t > learning_starts and t % target_network_update_freq == 0:\n",
      "                # Update target network periodically.\n",
      "                update_target()\n",
      "\n",
      "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
      "            num_episodes = len(episode_rewards)\n",
      "            if done and print_freq is not None and len(episode_rewards) % print_freq == 0:\n",
      "                logger.record_tabular(\"steps\", t)\n",
      "                logger.record_tabular(\"episodes\", num_episodes)\n",
      "                logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
      "                logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n",
      "                logger.dump_tabular()\n",
      "\n",
      "            if (checkpoint_freq is not None and t > learning_starts and\n",
      "                    num_episodes > 100 and t % checkpoint_freq == 0):\n",
      "                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n",
      "                    if print_freq is not None:\n",
      "                        logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
      "                                   saved_mean_reward, mean_100ep_reward))\n",
      "                    save_variables(model_file)\n",
      "                    model_saved = True\n",
      "                    saved_mean_reward = mean_100ep_reward\n",
      "        if model_saved:\n",
      "            if print_freq is not None:\n",
      "                logger.log(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n",
      "            load_variables(model_file)\n",
      "\n",
      "    return act\n"
     ]
    }
   ],
   "source": [
    "# print(df_valid.loc[0, 'code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e0eae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final testing (in progress)\n",
    "\n",
    "# print(\"Total mean value of all cosine similarities\")\n",
    "# for index, result in enumerate([b1_result_k1, b1_result_k2, b1_result_k3]):\n",
    "#     similarity_scores_list = result[2]\n",
    "#     total_values = 0\n",
    "#     total_mask_num = 0\n",
    "#     for similarity_scores in similarity_scores_list:\n",
    "#         if similarity_scores:\n",
    "#             total_values += np.sum(similarity_scores)\n",
    "#             total_mask_num += len(similarity_scores)\n",
    "    \n",
    "#     total_average = total_values / total_mask_num\n",
    "#     print(f\"Top_k={index+1}\\n Total mask number: {total_mask_num}, Total average cosine similarity: {round(total_average, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461527e3",
   "metadata": {},
   "source": [
    "### Baseline score 2: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
