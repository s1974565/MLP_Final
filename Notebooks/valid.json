[
    {
        "code": "def learn(env,\n          network,\n          seed=None,\n          lr=5e-4,\n          total_timesteps=100000,\n          buffer_size=50000,\n          exploration_fraction=0.1,\n          exploration_final_eps=0.02,\n          train_freq=1,\n          batch_size=32,\n          print_freq=100,\n          checkpoint_freq=10000,\n          checkpoint_path=None,\n          learning_starts=1000,\n          gamma=1.0,\n          target_network_update_freq=500,\n          prioritized_replay=False,\n          prioritized_replay_alpha=0.6,\n          prioritized_replay_beta0=0.4,\n          prioritized_replay_beta_iters=None,\n          prioritized_replay_eps=1e-6,\n          param_noise=False,\n          callback=None,\n          load_path=None,\n          **network_kwargs\n            ):\n    \"\"\"Train a deepq model.\n\n    Parameters\n    -------\n    env: gym.Env\n        environment to train on\n    network: string or a function\n        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n    seed: int or None\n        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n    lr: float\n        learning rate for adam optimizer\n    total_timesteps: int\n        number of env steps to optimizer for\n    buffer_size: int\n        size of the replay buffer\n    exploration_fraction: float\n        fraction of entire training period over which the exploration rate is annealed\n    exploration_final_eps: float\n        final value of random action probability\n    train_freq: int\n        update the model every `train_freq` steps.\n        set to None to disable printing\n    batch_size: int\n        size of a batched sampled from replay buffer for training\n    print_freq: int\n        how often to print out training progress\n        set to None to disable printing\n    checkpoint_freq: int\n        how often to save the model. This is so that the best version is restored\n        at the end of the training. If you do not wish to restore the best version at\n        the end of the training set this variable to None.\n    learning_starts: int\n        how many steps of the model to collect transitions for before learning starts\n    gamma: float\n        discount factor\n    target_network_update_freq: int\n        update the target network every `target_network_update_freq` steps.\n    prioritized_replay: True\n        if True prioritized replay buffer will be used.\n    prioritized_replay_alpha: float\n        alpha parameter for prioritized replay buffer\n    prioritized_replay_beta0: float\n        initial value of beta for prioritized replay buffer\n    prioritized_replay_beta_iters: int\n        number of iterations over which beta will be annealed from initial value\n        to 1.0. If set to None equals to total_timesteps.\n    prioritized_replay_eps: float\n        epsilon to add to the TD errors when updating priorities.\n    param_noise: bool\n        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n    callback: (locals, globals) -> None\n        function called at every steps with state of the algorithm.\n        If callback returns true training stops.\n    load_path: str\n        path to load the model from. (default: None)\n    **network_kwargs\n        additional keyword arguments to pass to the network builder.\n\n    Returns\n    -------\n    act: ActWrapper\n        Wrapper over act function. Adds ability to save it and load it.\n        See header of baselines/deepq/categorical.py for details on the act function.\n    \"\"\"\n    # Create all the functions necessary to train the model\n\n    <mask> = get_session()\n    set_global_seeds(seed)\n\n    <mask> = build_q_func(network, **network_kwargs)\n\n    # capture the shape outside the closure so that the env object is not serialized\n    # by cloudpickle when serializing make_obs_ph\n\n    <mask> = env.observation_space\n    def make_obs_ph(name):\n        return ObservationInput(observation_space, name=name)\n\n    <mask>, <mask>, <mask>, <mask> = deepq.build_train(\n        make_obs_ph=make_obs_ph,\n        q_func=q_func,\n        num_actions=env.action_space.n,\n        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n        gamma=gamma,\n        grad_norm_clipping=10,\n        param_noise=param_noise\n    )\n\n    <mask> = {\n        'make_obs_ph': make_obs_ph,\n        'q_func': q_func,\n        'num_actions': env.action_space.n,\n    }\n\n    <mask> = ActWrapper(act, act_params)\n\n    # Create the replay buffer\n    if prioritized_replay:\n        <mask> = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n        if prioritized_replay_beta_iters is None:\n            <mask> = total_timesteps\n        <mask> = LinearSchedule(prioritized_replay_beta_iters,\n                                       initial_p=prioritized_replay_beta0,\n                                       final_p=1.0)\n    else:\n        <mask> = ReplayBuffer(buffer_size)\n        <mask> = None\n    # Create the schedule for exploration starting from 1.\n    <mask> = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n                                 initial_p=1.0,\n                                 final_p=exploration_final_eps)\n\n    # Initialize the parameters and copy them to the target network.\n    U.initialize()\n    update_target()\n\n    <mask> = [0.0]\n    <mask> = None\n    <mask> = env.reset()\n    <mask> = True\n\n    with tempfile.TemporaryDirectory() as td:\n        <mask> = checkpoint_path or td\n\n        <mask> = os.path.join(td, \"model\")\n        <mask> = False\n\n        if tf.train.latest_checkpoint(td) is not None:\n            load_variables(model_file)\n            logger.log('Loaded model from {}'.format(model_file))\n            <mask> = True\n        elif load_path is not None:\n            load_variables(load_path)\n            logger.log('Loaded model from {}'.format(load_path))\n\n\n        for t in range(total_timesteps):\n            if callback is not None:\n                if callback(locals(), globals()):\n                    break\n            # Take action and update exploration to the newest value\n            <mask> = {}\n            if not param_noise:\n                <mask> = exploration.value(t)\n                <mask> = 0.\n            else:\n                <mask> = 0.\n                # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n                # for detailed explanation.\n                <mask> = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\n                <mask> = reset\n                <mask> = update_param_noise_threshold\n                <mask> = True\n            <mask> = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n            <mask> = action\n            <mask> = False\n            <mask>, <mask>, <mask>, <mask> = env.step(env_action)\n            # Store transition in the replay buffer.\n            replay_buffer.add(obs, action, rew, new_obs, float(done))\n            <mask> = new_obs\n\n            episode_rewards[-1] += rew\n            if done:\n                <mask> = env.reset()\n                episode_rewards.append(0.0)\n                <mask> = True\n\n            if t > learning_starts and t % train_freq == 0:\n                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n                if prioritized_replay:\n                    <mask> = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n                else:\n                    <mask>, <mask>, <mask>, <mask>, <mask> = replay_buffer.sample(batch_size)\n                    <mask>, <mask> = np.ones_like(rewards), None\n                <mask> = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n                if prioritized_replay:\n                    <mask> = np.abs(td_errors) + prioritized_replay_eps\n                    replay_buffer.update_priorities(batch_idxes, new_priorities)\n\n            if t > learning_starts and t % target_network_update_freq == 0:\n                # Update target network periodically.\n                update_target()\n\n            <mask> = round(np.mean(episode_rewards[-101:-1]), 1)\n            <mask> = len(episode_rewards)\n            if done and print_freq is not None and len(episode_rewards) % print_freq == 0:\n                logger.record_tabular(\"steps\", t)\n                logger.record_tabular(\"episodes\", num_episodes)\n                logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n                logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n                logger.dump_tabular()\n\n            if (checkpoint_freq is not None and t > learning_starts and\n                    num_episodes > 100 and t % checkpoint_freq == 0):\n                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n                    if print_freq is not None:\n                        logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n                                   saved_mean_reward, mean_100ep_reward))\n                    save_variables(model_file)\n                    <mask> = True\n                    <mask> = mean_100ep_reward\n        if model_saved:\n            if print_freq is not None:\n                logger.log(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n            load_variables(model_file)\n\n    return act"
    },
    {
        "code": "def save_act(self, path=None):\n        \"\"\"Save model to a pickle located at `path`\"\"\"\n        if path is None:\n            <mask> = os.path.join(logger.get_dir(), \"model.pkl\")\n\n        with tempfile.TemporaryDirectory() as td:\n            save_variables(os.path.join(td, \"model\"))\n            <mask> = os.path.join(td, \"packed.zip\")\n            with zipfile.ZipFile(arc_name, 'w') as zipf:\n                for root, dirs, files in os.walk(td):\n                    for fname in files:\n                        <mask> = os.path.join(root, fname)\n                        if file_path != arc_name:\n                            zipf.write(file_path, os.path.relpath(file_path, td))\n            with open(arc_name, \"rb\") as f:\n                <mask> = f.read()\n        with open(path, \"wb\") as f:\n            cloudpickle.dump((model_data, self._act_params), f)"
    },
    {
        "code": "def nature_cnn(unscaled_images, **conv_kwargs):\n    \"\"\"\n    CNN from Nature paper.\n    \"\"\"\n    <mask> = tf.cast(unscaled_images, tf.float32) / 255.\n    <mask> = tf.nn.relu\n    <mask> = activ(conv(scaled_images, 'c1', nf=32, rf=8, stride=4, init_scale=np.sqrt(2),\n                   **conv_kwargs))\n    <mask> = activ(conv(h, 'c2', nf=64, rf=4, stride=2, init_scale=np.sqrt(2), **conv_kwargs))\n    <mask> = activ(conv(h2, 'c3', nf=64, rf=3, stride=1, init_scale=np.sqrt(2), **conv_kwargs))\n    <mask> = conv_to_fc(h3)\n    return activ(fc(h3, 'fc1', nh=512, init_scale=np.sqrt(2)))"
    },
    {
        "code": "def mlp(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n    \"\"\"\n    Stack of fully-connected layers to be used in a policy / q-function approximator\n\n    Parameters:\n    ----------\n\n    num_layers: int                 number of fully-connected layers (default: 2)\n\n    num_hidden: int                 size of fully-connected layers (default: 64)\n\n    activation:                     activation function (default: tf.tanh)\n\n    Returns:\n    -------\n\n    function that builds fully connected network with a given input tensor / placeholder\n    \"\"\"\n    def network_fn(X):\n        <mask> = tf.layers.flatten(X)\n        for i in range(num_layers):\n            <mask> = fc(h, 'mlp_fc{}'.format(i), nh=num_hidden, init_scale=np.sqrt(2))\n            if layer_norm:\n                <mask> = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n            <mask> = activation(h)\n\n        return h\n\n    return network_fn"
    },
    {
        "code": "def lstm(nlstm=128, layer_norm=False):\n    \"\"\"\n    Builds LSTM (Long-Short Term Memory) network to be used in a policy.\n    Note that the resulting function returns not only the output of the LSTM\n    (i.e. hidden state of lstm for each step in the sequence), but also a dictionary\n    with auxiliary tensors to be set as policy attributes.\n\n    Specifically,\n        S is a placeholder to feed current state (LSTM state has to be managed outside policy)\n        M is a placeholder for the mask (used to mask out observations after the end of the episode, but can be used for other purposes too)\n        initial_state is a numpy array containing initial lstm state (usually zeros)\n        state is the output LSTM state (to be fed into S at the next call)\n\n\n    An example of usage of lstm-based policy can be found here: common/tests/test_doc_examples.py/test_lstm_example\n\n    Parameters:\n    ----------\n\n    nlstm: int          LSTM hidden state size\n\n    layer_norm: bool    if True, layer-normalized version of LSTM is used\n\n    Returns:\n    -------\n\n    function that builds LSTM with a given input tensor / placeholder\n    \"\"\"\n\n    def network_fn(X, nenv=1):\n        <mask> = X.shape[0]\n        <mask> = nbatch // nenv\n\n        <mask> = tf.layers.flatten(X)\n\n        <mask> = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        <mask> = tf.placeholder(tf.float32, [nenv, 2*nlstm]) #states\n\n        <mask> = batch_to_seq(h, nenv, nsteps)\n        <mask> = batch_to_seq(M, nenv, nsteps)\n\n        if layer_norm:\n            <mask>, <mask> = utils.lnlstm(xs, ms, S, scope='lnlstm', nh=nlstm)\n        else:\n            <mask>, <mask> = utils.lstm(xs, ms, S, scope='lstm', nh=nlstm)\n\n        <mask> = seq_to_batch(h5)\n        <mask> = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {'S':S, 'M':M, 'state':snew, 'initial_state':initial_state}\n\n    return network_fn"
    },
    {
        "code": "def conv_only(convs=[(32, 8, 4), (64, 4, 2), (64, 3, 1)], **conv_kwargs):\n    '''\n    convolutions-only net\n\n    Parameters:\n    ----------\n\n    conv:       list of triples (filter_number, filter_size, stride) specifying parameters for each layer.\n\n    Returns:\n\n    function that takes tensorflow tensor as input and returns the output of the last convolutional layer\n\n    '''\n\n    def network_fn(X):\n        <mask> = tf.cast(X, tf.float32) / 255.\n        with tf.variable_scope(\"convnet\"):\n            for num_outputs, kernel_size, stride in convs:\n                <mask> = layers.convolution2d(out,\n                                           num_outputs=num_outputs,\n                                           kernel_size=kernel_size,\n                                           stride=stride,\n                                           activation_fn=tf.nn.relu,\n                                           **conv_kwargs)\n\n        return out\n    return network_fn"
    },
    {
        "code": "def get_network_builder(name):\n    \"\"\"\n    If you want to register your own network outside models.py, you just need:\n\n    Usage Example:\n    -------------\n    from baselines.common.models import register\n    @register(\"your_network_name\")\n    def your_network_define(**net_kwargs):\n        ...\n        return network_fn\n\n    \"\"\"\n    if callable(name):\n        return name\n    elif name in mapping:\n        return mapping[name]\n    else:\n        raise ValueError('Unknown network type: {}'.format(name))"
    },
    {
        "code": "def mlp(hiddens=[], layer_norm=False):\n    \"\"\"This model takes as input an observation and returns values of all actions.\n\n    Parameters\n    ----------\n    hiddens: [int]\n        list of sizes of hidden layers\n    layer_norm: bool\n        if true applies layer normalization for every layer\n        as described in https://arxiv.org/abs/1607.06450\n\n    Returns\n    -------\n    q_func: function\n        q_function for DQN algorithm.\n    \"\"\"\n    return lambda *args, **kwargs: _mlp(hiddens, layer_norm=layer_norm, *args, **kwargs)"
    },
    {
        "code": "def cnn_to_mlp(convs, hiddens, dueling=False, layer_norm=False):\n    \"\"\"This model takes as input an observation and returns values of all actions.\n\n    Parameters\n    ----------\n    convs: [(int, int, int)]\n        list of convolutional layers in form of\n        (num_outputs, kernel_size, stride)\n    hiddens: [int]\n        list of sizes of hidden layers\n    dueling: bool\n        if true double the output MLP to compute a baseline\n        for action scores\n    layer_norm: bool\n        if true applies layer normalization for every layer\n        as described in https://arxiv.org/abs/1607.06450\n\n    Returns\n    -------\n    q_func: function\n        q_function for DQN algorithm.\n    \"\"\"\n\n    return lambda *args, **kwargs: _cnn_to_mlp(convs, hiddens, dueling, layer_norm=layer_norm, *args, **kwargs)"
    },
    {
        "code": "def make_vec_env(env_id, env_type, num_env, seed,\n                 wrapper_kwargs=None,\n                 start_index=0,\n                 reward_scale=1.0,\n                 flatten_dict_observations=True,\n                 gamestate=None):\n    \"\"\"\n    Create a wrapped, monitored SubprocVecEnv for Atari and MuJoCo.\n    \"\"\"\n    <mask> = wrapper_kwargs or {}\n    <mask> = MPI.COMM_WORLD.Get_rank() if MPI else 0\n    <mask> = seed + 10000 * mpi_rank if seed is not None else None\n    <mask> = logger.get_dir()\n    def make_thunk(rank):\n        return lambda: make_env(\n            env_id=env_id,\n            env_type=env_type,\n            mpi_rank=mpi_rank,\n            subrank=rank,\n            seed=seed,\n            reward_scale=reward_scale,\n            gamestate=gamestate,\n            flatten_dict_observations=flatten_dict_observations,\n            wrapper_kwargs=wrapper_kwargs,\n            logger_dir=logger_dir\n        )\n\n    set_global_seeds(seed)\n    if num_env > 1:\n        return SubprocVecEnv([make_thunk(i + start_index) for i in range(num_env)])\n    else:\n        return DummyVecEnv([make_thunk(start_index)])"
    },
    {
        "code": "def make_mujoco_env(env_id, seed, reward_scale=1.0):\n    \"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"\n    <mask> = MPI.COMM_WORLD.Get_rank()\n    <mask> = seed  + 1000 * rank if seed is not None else None\n    set_global_seeds(myseed)\n    <mask> = gym.make(env_id)\n    <mask> = None if logger.get_dir() is None else os.path.join(logger.get_dir(), str(rank))\n    <mask> = Monitor(env, logger_path, allow_early_resets=True)\n    env.seed(seed)\n    if reward_scale != 1.0:\n        from baselines.common.retro_wrappers import RewardScaler\n        <mask> = RewardScaler(env, reward_scale)\n    return env"
    },
    {
        "code": "def make_robotics_env(env_id, seed, rank=0):\n    \"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"\n    set_global_seeds(seed)\n    <mask> = gym.make(env_id)\n    <mask> = FlattenDictWrapper(env, ['observation', 'desired_goal'])\n    <mask> = Monitor(\n        env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)),\n        info_keywords=('is_success',))\n    env.seed(seed)\n    return env"
    },
    {
        "code": "def common_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"\n    parser = arg_parser()\n    parser.add_argument('--env', help='environment ID', type=str, default='Reacher-v2')\n    parser.add_argument('--env_type', help='type of environment, used when the environment type cannot be automatically determined', type=str)\n    parser.add_argument('--seed', help='RNG seed', type=int, default=None)\n    parser.add_argument('--alg', help='Algorithm', type=str, default='ppo2')\n    parser.add_argument('--num_timesteps', type=float, default=1e6),\n    parser.add_argument('--network', help='network type (mlp, cnn, lstm, cnn_lstm, conv_only)', default=None)\n    parser.add_argument('--gamestate', help='game state to load (so far only used in retro games)', default=None)\n    parser.add_argument('--num_env', help='Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco', default=None, type=int)\n    parser.add_argument('--reward_scale', help='Reward scale factor. Default: 1.0', default=1.0, type=float)\n    parser.add_argument('--save_path', help='Path to save trained model to', default=None, type=str)\n    parser.add_argument('--save_video_interval', help='Save video every x steps (0 = disabled)', default=0, type=int)\n    parser.add_argument('--save_video_length', help='Length of recorded video. Default: 200', default=200, type=int)\n    parser.add_argument('--play', default=False, action='store_true')\n    return parser"
    },
    {
        "code": "def robotics_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"\n    parser = arg_parser()\n    parser.add_argument('--env', help='environment ID', type=str, default='FetchReach-v0')\n    parser.add_argument('--seed', help='RNG seed', type=int, default=None)\n    parser.add_argument('--num-timesteps', type=int, default=int(1e6))\n    return parser"
    },
    {
        "code": "def parse_unknown_args(args):\n    \"\"\"\n    Parse arguments not consumed by arg parser into a dicitonary\n    \"\"\"\n    <mask> = {}\n    <mask> = False\n    for arg in args:\n        if arg.startswith('--'):\n            if '=' in arg:\n                <mask> = arg.split('=')[0][2:]\n                <mask> = arg.split('=')[1]\n                <mask> = value\n            else:\n                <mask> = arg[2:]\n                <mask> = True\n        elif preceded_by_key:\n            <mask> = arg\n            <mask> = False\n\n    return retval"
    },
    {
        "code": "def clear_mpi_env_vars():\n    \"\"\"\n    from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n    Processes.\n    \"\"\"\n    <mask> = {}\n    for k, v in list(os.environ.items()):\n        for prefix in ['OMPI_', 'PMI_']:\n            if k.startswith(prefix):\n                <mask> = v\n                del os.environ[k]\n    try:\n        yield\n    finally:\n        os.environ.update(removed_environment)"
    },
    {
        "code": "def learn(*, network, env, total_timesteps, eval_env = None, seed=None, nsteps=2048, ent_coef=0.0, lr=3e-4,\n            vf_coef=0.5,  max_grad_norm=0.5, gamma=0.99, lam=0.95,\n            log_interval=10, nminibatches=4, noptepochs=4, cliprange=0.2,\n            save_interval=0, load_path=None, model_fn=None, **network_kwargs):\n    '''\n    Learn policy using PPO algorithm (https://arxiv.org/abs/1707.06347)\n\n    Parameters:\n    ----------\n\n    network:                          policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                                      specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                                      tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                                      neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                                      See common/models.py/lstm for more details on using recurrent nets in policies\n\n    env: baselines.common.vec_env.VecEnv     environment. Needs to be vectorized for parallel environment simulation.\n                                      The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.\n\n\n    nsteps: int                       number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                                      nenv is number of environment copies simulated in parallel)\n\n    total_timesteps: int              number of timesteps (i.e. number of actions taken in the environment)\n\n    ent_coef: float                   policy entropy coefficient in the optimization objective\n\n    lr: float or function             learning rate, constant or a schedule function [0,1] -> R+ where 1 is beginning of the\n                                      training and 0 is the end of the training.\n\n    vf_coef: float                    value function loss coefficient in the optimization objective\n\n    max_grad_norm: float or None      gradient norm clipping coefficient\n\n    gamma: float                      discounting factor\n\n    lam: float                        advantage estimation discounting factor (lambda in the paper)\n\n    log_interval: int                 number of timesteps between logging events\n\n    nminibatches: int                 number of training minibatches per update. For recurrent policies,\n                                      should be smaller or equal than number of environments run in parallel.\n\n    noptepochs: int                   number of training epochs per update\n\n    cliprange: float or function      clipping range, constant or schedule function [0,1] -> R+ where 1 is beginning of the training\n                                      and 0 is the end of the training\n\n    save_interval: int                number of timesteps between saving events\n\n    load_path: str                    path to load the model from\n\n    **network_kwargs:                 keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                                      For instance, 'mlp' network architecture has arguments num_hidden and num_layers.\n\n\n\n    '''\n\n    set_global_seeds(seed)\n\n    if isinstance(lr, float): <mask> = constfn(lr)\n    else: assert callable(lr)\n    if isinstance(cliprange, float): <mask> = constfn(cliprange)\n    else: assert callable(cliprange)\n    <mask> = int(total_timesteps)\n\n    <mask> = build_policy(env, network, **network_kwargs)\n\n    # Get the nb of env\n    <mask> = env.num_envs\n\n    # Get state_space and action_space\n    <mask> = env.observation_space\n    <mask> = env.action_space\n\n    # Calculate the batch_size\n    <mask> = nenvs * nsteps\n    <mask> = nbatch // nminibatches\n\n    # Instantiate the model object (that creates act_model and train_model)\n    if model_fn is None:\n        from baselines.ppo2.model import Model\n        <mask> = Model\n\n    <mask> = model_fn(policy=policy, ob_space=ob_space, ac_space=ac_space, nbatch_act=nenvs, nbatch_train=nbatch_train,\n                    nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\n                    max_grad_norm=max_grad_norm)\n\n    if load_path is not None:\n        model.load(load_path)\n    # Instantiate the runner object\n    <mask> = Runner(env=env, model=model, nsteps=nsteps, gamma=gamma, lam=lam)\n    if eval_env is not None:\n        <mask> = Runner(env = eval_env, model = model, nsteps = nsteps, gamma = gamma, lam= lam)\n\n    epinfobuf = deque(maxlen=100)\n    if eval_env is not None:\n        <mask> = deque(maxlen=100)\n\n    # Start total timer\n    <mask> = time.perf_counter()\n\n    nupdates = total_timesteps//nbatch\n    for update in range(1, nupdates+1):\n        assert nbatch % nminibatches == 0\n        # Start timer\n        <mask> = time.perf_counter()\n        <mask> = 1.0 - (update - 1.0) / nupdates\n        # Calculate the learning rate\n        <mask> = lr(frac)\n        # Calculate the cliprange\n        <mask> = cliprange(frac)\n        # Get minibatch\n        <mask>, <mask>, <mask>, <mask>, <mask>, <mask>, <mask>, <mask> = runner.run() #pylint: disable=E0632\n        if eval_env is not None:\n            <mask>, <mask>, <mask>, <mask>, <mask>, <mask>, <mask>, <mask> = eval_runner.run() #pylint: disable=E0632\n\n        epinfobuf.extend(epinfos)\n        if eval_env is not None:\n            eval_epinfobuf.extend(eval_epinfos)\n\n        # Here what we're going to do is for each minibatch calculate the loss and append it.\n        <mask> = []\n        if states is None: # nonrecurrent version\n            # Index of each element of batch_size\n            # Create the indices array\n            <mask> = np.arange(nbatch)\n            for _ in range(noptepochs):\n                # Randomize the indexes\n                np.random.shuffle(inds)\n                # 0 to batch_size with batch_train_size step\n                for start in range(0, nbatch, nbatch_train):\n                    <mask> = start + nbatch_train\n                    <mask> = inds[start:end]\n                    <mask> = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n                    mblossvals.append(model.train(lrnow, cliprangenow, *slices))\n        else: # recurrent version\n            assert nenvs % nminibatches == 0\n            <mask> = nenvs // nminibatches\n            <mask> = np.arange(nenvs)\n            <mask> = np.arange(nenvs * nsteps).reshape(nenvs, nsteps)\n            for _ in range(noptepochs):\n                np.random.shuffle(envinds)\n                for start in range(0, nenvs, envsperbatch):\n                    <mask> = start + envsperbatch\n                    <mask> = envinds[start:end]\n                    <mask> = flatinds[mbenvinds].ravel()\n                    <mask> = (arr[mbflatinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n                    <mask> = states[mbenvinds]\n                    mblossvals.append(model.train(lrnow, cliprangenow, *slices, mbstates))\n\n        # Feedforward --> get losses --> update\n        <mask> = np.mean(mblossvals, axis=0)\n        # End timer\n        <mask> = time.perf_counter()\n        # Calculate the fps (frame per second)\n        <mask> = int(nbatch / (tnow - tstart))\n        if update % log_interval == 0 or update == 1:\n            # Calculates if value function is a good predicator of the returns (ev > 1)\n            # or if it's just worse than predicting nothing (ev =< 0)\n            <mask> = explained_variance(values, returns)\n            logger.logkv(\"serial_timesteps\", update*nsteps)\n            logger.logkv(\"nupdates\", update)\n            logger.logkv(\"total_timesteps\", update*nbatch)\n            logger.logkv(\"fps\", fps)\n            logger.logkv(\"explained_variance\", float(ev))\n            logger.logkv('eprewmean', safemean([epinfo['r'] for epinfo in epinfobuf]))\n            logger.logkv('eplenmean', safemean([epinfo['l'] for epinfo in epinfobuf]))\n            if eval_env is not None:\n                logger.logkv('eval_eprewmean', safemean([epinfo['r'] for epinfo in eval_epinfobuf]) )\n                logger.logkv('eval_eplenmean', safemean([epinfo['l'] for epinfo in eval_epinfobuf]) )\n            logger.logkv('time_elapsed', tnow - tfirststart)\n            for (lossval, lossname) in zip(lossvals, model.loss_names):\n                logger.logkv(lossname, lossval)\n            if MPI is None or MPI.COMM_WORLD.Get_rank() == 0:\n                logger.dumpkvs()\n        if save_interval and (update % save_interval == 0 or update == 1) and logger.get_dir() and (MPI is None or MPI.COMM_WORLD.Get_rank() == 0):\n            <mask> = osp.join(logger.get_dir(), 'checkpoints')\n            os.makedirs(checkdir, exist_ok=True)\n            <mask> = osp.join(checkdir, '%.5i'%update)\n            print('Saving to', savepath)\n            model.save(savepath)\n    return model"
    },
    {
        "code": "def cg(f_Ax, b, cg_iters=10, callback=None, verbose=False, residual_tol=1e-10):\n    \"\"\"\n    Demmel p 312\n    \"\"\"\n    <mask> = b.copy()\n    <mask> = b.copy()\n    <mask> = np.zeros_like(b)\n    <mask> = r.dot(r)\n\n    <mask> =  \"%10i %10.3g %10.3g\"\n    <mask> =  \"%10s %10s %10s\"\n    if verbose: print(titlestr % (\"iter\", \"residual norm\", \"soln norm\"))\n\n    for i in range(cg_iters):\n        if callback is not None:\n            callback(x)\n        if verbose: print(fmtstr % (i, rdotr, np.linalg.norm(x)))\n        <mask> = f_Ax(p)\n        <mask> = rdotr / p.dot(z)\n        <mask> += v*p\n        <mask> -= v*z\n        <mask> = r.dot(r)\n        <mask> = newrdotr/rdotr\n        <mask> = r + mu*p\n\n        <mask> = newrdotr\n        if rdotr < residual_tol:\n            break\n\n    if callback is not None:\n        callback(x)\n    if verbose: print(fmtstr % (i+1, rdotr, np.linalg.norm(x)))  # pylint: disable=W0631\n    return x"
    },
    {
        "code": "def observation_placeholder(ob_space, batch_size=None, name='Ob'):\n    '''\n    Create placeholder to feed observations into of the size appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space     observation space\n\n    batch_size: int         size of the batch to be fed into input. Can be left None in most cases.\n\n    name: str               name of the placeholder\n\n    Returns:\n    -------\n\n    tensorflow placeholder tensor\n    '''\n\n    assert isinstance(ob_space, Discrete) or isinstance(ob_space, Box) or isinstance(ob_space, MultiDiscrete), \\\n        'Can only deal with Discrete and Box observation spaces for now'\n\n    <mask> = ob_space.dtype\n    if dtype == np.int8:\n        <mask> = np.uint8\n\n    return tf.placeholder(shape=(batch_size,) + ob_space.shape, dtype=dtype, name=name)"
    },
    {
        "code": "def observation_input(ob_space, batch_size=None, name='Ob'):\n    '''\n    Create placeholder to feed observations into of the size appropriate to the observation space, and add input\n    encoder of the appropriate type.\n    '''\n\n    <mask> = observation_placeholder(ob_space, batch_size, name)\n    return placeholder, encode_observation(ob_space, placeholder)"
    }
]