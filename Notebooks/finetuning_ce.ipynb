{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "# Helpers\n",
    "from testing import *\n",
    "# Baseline model 1\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, pipeline\n",
    "# Sentence similarity model\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Check if GPU acceleration is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    device_num = torch.cuda.current_device()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    device_num = -1\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning by \"Cross Entropy\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Run this if you removed pickle files\n",
    "# saving_pickles()\n",
    "\n",
    "train_df, valid_df, test_df = loading_pickles()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Constants ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# For training (not validation)\n",
    "batch_size = 50\n",
    "\n",
    "# Whether to shuffle the training set or not\n",
    "shuffle = False\n",
    "\n",
    "# Using only a portion of the train dataset for performance\n",
    "# Set it to 1 to use all train dataset; increasing this may give better result.\n",
    "size_proportion_train = 0.01\n",
    "\n",
    "# Hyperparameters ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "total_epoch_number = 2\n",
    "# Initial learning rate for the optimizer\n",
    "learning_rate = 0.00005\n",
    "weight_decay_coefficient = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Do not change these values unless necessary\n",
    "mask_prob = 0.5\n",
    "window_size = 100\n",
    "rng_seed = 42\n",
    "size_proportion_valid = 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/4121 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b0f0b83a5fa45f8945b4f1c32c22b51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Window split:   0%|          | 0/4121 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "348aae0455d746d6a3c5fbb996a40870"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/11553 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "887e9dee4d1244a19f964011ffb21cb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Masking:   0%|          | 0/22176 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a6a0fce5ab548d58298c8e9ac724a88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def masking_df(code_df):\n",
    "    masked_code_df = mask_variable_df(code_df, mask_prob=mask_prob, rng_seed=rng_seed)\n",
    "    merged_code_df = pd.concat([code_df, masked_code_df], axis=\"columns\")\n",
    "    return merged_code_df\n",
    "\n",
    "def window_df(code_df):\n",
    "    merged_code_df = masking_df(code_df)\n",
    "    return split_into_windows(merged_code_df, window_size=window_size, mask_token=\"<mask>\")\n",
    "\n",
    "train_df_size = int(len(train_df) * size_proportion_train)\n",
    "valid_df_size = int(len(valid_df) * size_proportion_valid)\n",
    "\n",
    "if shuffle:\n",
    "    train_df = train_df.sample(frac=1, random_state=rng_seed).reset_index(drop=True)\n",
    "\n",
    "window_train_df = window_df(train_df[:train_df_size])\n",
    "\n",
    "merged_valid_df = masking_df(valid_df[:valid_df_size])\n",
    "merged_test_df = masking_df(test_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "three ways to fine tune the model:\n",
    "1. Fine-tune the last head of the model <- Current approach\n",
    "2. Fine-tune the whole model\n",
    "3. Add extra layers to the model and fine tune the additional layer\n",
    "\n",
    "We use the first baseline model, as it showed the better performance in the baseline testing.\n",
    "See the baseline_testing notebook for more details.\n",
    "\n",
    "There are three main hyperparameters: total_epoch_number, learning_rate, and weight_decay_coefficient\n",
    "Try different combinations to fine the best performing model. I commented thoroughly so there should be no problem in understanding the codes.\n",
    "If you want to change the optimizer, then you need to directly edit the optimizer definition in the training loop cell.\n",
    "There are other changeable constants, and varying them may also give a better performance. For example, if you increase size_limit_proportion_train,\n",
    "the more training set will be used for the fine-tuning. But the fine-tuning time will be increased too.\n",
    "\n",
    "After a model is fine-tuned, its performance will then be evaluated based on the average cosine similarity with the validation set.\n",
    "You can check if the fine-tuned model performs better or worse compared to the baseline model. If your model performs well, then\n",
    "consider saving the model so that we can use it later. For performance, it is set to only use the half of the validation set.\n",
    "\n",
    "If any error occurs, or if you have a suggestion, let me know.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model investigation\n",
    "\n",
    "You need to uncomment some cells to see the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# All Huggingface models are standard torch.nn.Module, so they can easily be used in any training loop.\n",
    "\n",
    "# Model architecture information:\n",
    "# https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/roberta#transformers.RobertaForMaskedLM\n",
    "model_b1 = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base-mlm')\n",
    "tokenizer_b1 = RobertaTokenizer.from_pretrained('microsoft/codebert-base-mlm')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Model structure (Uncomment to see)\n",
    "# print(model_b1)\n",
    "\n",
    "# Embedding size = (50265, 768)\n",
    "# Dropout probability = 0.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Model parameters (Uncomment to see)\n",
    "\n",
    "# for name, param in model_b1.named_parameters():\n",
    "#     print(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight False\n",
      "roberta.embeddings.position_embeddings.weight False\n",
      "roberta.embeddings.token_type_embeddings.weight False\n",
      "roberta.embeddings.LayerNorm.weight False\n",
      "roberta.embeddings.LayerNorm.bias False\n",
      "roberta.encoder.layer.0.attention.self.query.weight False\n",
      "roberta.encoder.layer.0.attention.self.query.bias False\n",
      "roberta.encoder.layer.0.attention.self.key.weight False\n",
      "roberta.encoder.layer.0.attention.self.key.bias False\n",
      "roberta.encoder.layer.0.attention.self.value.weight False\n",
      "roberta.encoder.layer.0.attention.self.value.bias False\n",
      "roberta.encoder.layer.0.attention.output.dense.weight False\n",
      "roberta.encoder.layer.0.attention.output.dense.bias False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.0.intermediate.dense.weight False\n",
      "roberta.encoder.layer.0.intermediate.dense.bias False\n",
      "roberta.encoder.layer.0.output.dense.weight False\n",
      "roberta.encoder.layer.0.output.dense.bias False\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.1.attention.self.query.weight False\n",
      "roberta.encoder.layer.1.attention.self.query.bias False\n",
      "roberta.encoder.layer.1.attention.self.key.weight False\n",
      "roberta.encoder.layer.1.attention.self.key.bias False\n",
      "roberta.encoder.layer.1.attention.self.value.weight False\n",
      "roberta.encoder.layer.1.attention.self.value.bias False\n",
      "roberta.encoder.layer.1.attention.output.dense.weight False\n",
      "roberta.encoder.layer.1.attention.output.dense.bias False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.1.intermediate.dense.weight False\n",
      "roberta.encoder.layer.1.intermediate.dense.bias False\n",
      "roberta.encoder.layer.1.output.dense.weight False\n",
      "roberta.encoder.layer.1.output.dense.bias False\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.2.attention.self.query.weight False\n",
      "roberta.encoder.layer.2.attention.self.query.bias False\n",
      "roberta.encoder.layer.2.attention.self.key.weight False\n",
      "roberta.encoder.layer.2.attention.self.key.bias False\n",
      "roberta.encoder.layer.2.attention.self.value.weight False\n",
      "roberta.encoder.layer.2.attention.self.value.bias False\n",
      "roberta.encoder.layer.2.attention.output.dense.weight False\n",
      "roberta.encoder.layer.2.attention.output.dense.bias False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.2.intermediate.dense.weight False\n",
      "roberta.encoder.layer.2.intermediate.dense.bias False\n",
      "roberta.encoder.layer.2.output.dense.weight False\n",
      "roberta.encoder.layer.2.output.dense.bias False\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.3.attention.self.query.weight False\n",
      "roberta.encoder.layer.3.attention.self.query.bias False\n",
      "roberta.encoder.layer.3.attention.self.key.weight False\n",
      "roberta.encoder.layer.3.attention.self.key.bias False\n",
      "roberta.encoder.layer.3.attention.self.value.weight False\n",
      "roberta.encoder.layer.3.attention.self.value.bias False\n",
      "roberta.encoder.layer.3.attention.output.dense.weight False\n",
      "roberta.encoder.layer.3.attention.output.dense.bias False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.3.intermediate.dense.weight False\n",
      "roberta.encoder.layer.3.intermediate.dense.bias False\n",
      "roberta.encoder.layer.3.output.dense.weight False\n",
      "roberta.encoder.layer.3.output.dense.bias False\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.4.attention.self.query.weight False\n",
      "roberta.encoder.layer.4.attention.self.query.bias False\n",
      "roberta.encoder.layer.4.attention.self.key.weight False\n",
      "roberta.encoder.layer.4.attention.self.key.bias False\n",
      "roberta.encoder.layer.4.attention.self.value.weight False\n",
      "roberta.encoder.layer.4.attention.self.value.bias False\n",
      "roberta.encoder.layer.4.attention.output.dense.weight False\n",
      "roberta.encoder.layer.4.attention.output.dense.bias False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.4.intermediate.dense.weight False\n",
      "roberta.encoder.layer.4.intermediate.dense.bias False\n",
      "roberta.encoder.layer.4.output.dense.weight False\n",
      "roberta.encoder.layer.4.output.dense.bias False\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.5.attention.self.query.weight False\n",
      "roberta.encoder.layer.5.attention.self.query.bias False\n",
      "roberta.encoder.layer.5.attention.self.key.weight False\n",
      "roberta.encoder.layer.5.attention.self.key.bias False\n",
      "roberta.encoder.layer.5.attention.self.value.weight False\n",
      "roberta.encoder.layer.5.attention.self.value.bias False\n",
      "roberta.encoder.layer.5.attention.output.dense.weight False\n",
      "roberta.encoder.layer.5.attention.output.dense.bias False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.5.intermediate.dense.weight False\n",
      "roberta.encoder.layer.5.intermediate.dense.bias False\n",
      "roberta.encoder.layer.5.output.dense.weight False\n",
      "roberta.encoder.layer.5.output.dense.bias False\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.6.attention.self.query.weight False\n",
      "roberta.encoder.layer.6.attention.self.query.bias False\n",
      "roberta.encoder.layer.6.attention.self.key.weight False\n",
      "roberta.encoder.layer.6.attention.self.key.bias False\n",
      "roberta.encoder.layer.6.attention.self.value.weight False\n",
      "roberta.encoder.layer.6.attention.self.value.bias False\n",
      "roberta.encoder.layer.6.attention.output.dense.weight False\n",
      "roberta.encoder.layer.6.attention.output.dense.bias False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.6.intermediate.dense.weight False\n",
      "roberta.encoder.layer.6.intermediate.dense.bias False\n",
      "roberta.encoder.layer.6.output.dense.weight False\n",
      "roberta.encoder.layer.6.output.dense.bias False\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.7.attention.self.query.weight False\n",
      "roberta.encoder.layer.7.attention.self.query.bias False\n",
      "roberta.encoder.layer.7.attention.self.key.weight False\n",
      "roberta.encoder.layer.7.attention.self.key.bias False\n",
      "roberta.encoder.layer.7.attention.self.value.weight False\n",
      "roberta.encoder.layer.7.attention.self.value.bias False\n",
      "roberta.encoder.layer.7.attention.output.dense.weight False\n",
      "roberta.encoder.layer.7.attention.output.dense.bias False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.7.intermediate.dense.weight False\n",
      "roberta.encoder.layer.7.intermediate.dense.bias False\n",
      "roberta.encoder.layer.7.output.dense.weight False\n",
      "roberta.encoder.layer.7.output.dense.bias False\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.8.attention.self.query.weight False\n",
      "roberta.encoder.layer.8.attention.self.query.bias False\n",
      "roberta.encoder.layer.8.attention.self.key.weight False\n",
      "roberta.encoder.layer.8.attention.self.key.bias False\n",
      "roberta.encoder.layer.8.attention.self.value.weight False\n",
      "roberta.encoder.layer.8.attention.self.value.bias False\n",
      "roberta.encoder.layer.8.attention.output.dense.weight False\n",
      "roberta.encoder.layer.8.attention.output.dense.bias False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.8.intermediate.dense.weight False\n",
      "roberta.encoder.layer.8.intermediate.dense.bias False\n",
      "roberta.encoder.layer.8.output.dense.weight False\n",
      "roberta.encoder.layer.8.output.dense.bias False\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.9.attention.self.query.weight False\n",
      "roberta.encoder.layer.9.attention.self.query.bias False\n",
      "roberta.encoder.layer.9.attention.self.key.weight False\n",
      "roberta.encoder.layer.9.attention.self.key.bias False\n",
      "roberta.encoder.layer.9.attention.self.value.weight False\n",
      "roberta.encoder.layer.9.attention.self.value.bias False\n",
      "roberta.encoder.layer.9.attention.output.dense.weight False\n",
      "roberta.encoder.layer.9.attention.output.dense.bias False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.9.intermediate.dense.weight False\n",
      "roberta.encoder.layer.9.intermediate.dense.bias False\n",
      "roberta.encoder.layer.9.output.dense.weight False\n",
      "roberta.encoder.layer.9.output.dense.bias False\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.10.attention.self.query.weight False\n",
      "roberta.encoder.layer.10.attention.self.query.bias False\n",
      "roberta.encoder.layer.10.attention.self.key.weight False\n",
      "roberta.encoder.layer.10.attention.self.key.bias False\n",
      "roberta.encoder.layer.10.attention.self.value.weight False\n",
      "roberta.encoder.layer.10.attention.self.value.bias False\n",
      "roberta.encoder.layer.10.attention.output.dense.weight False\n",
      "roberta.encoder.layer.10.attention.output.dense.bias False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.10.intermediate.dense.weight False\n",
      "roberta.encoder.layer.10.intermediate.dense.bias False\n",
      "roberta.encoder.layer.10.output.dense.weight False\n",
      "roberta.encoder.layer.10.output.dense.bias False\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.11.attention.self.query.weight False\n",
      "roberta.encoder.layer.11.attention.self.query.bias False\n",
      "roberta.encoder.layer.11.attention.self.key.weight False\n",
      "roberta.encoder.layer.11.attention.self.key.bias False\n",
      "roberta.encoder.layer.11.attention.self.value.weight False\n",
      "roberta.encoder.layer.11.attention.self.value.bias False\n",
      "roberta.encoder.layer.11.attention.output.dense.weight False\n",
      "roberta.encoder.layer.11.attention.output.dense.bias False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "roberta.encoder.layer.11.intermediate.dense.weight False\n",
      "roberta.encoder.layer.11.intermediate.dense.bias False\n",
      "roberta.encoder.layer.11.output.dense.weight False\n",
      "roberta.encoder.layer.11.output.dense.bias False\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight False\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias False\n",
      "lm_head.bias True\n",
      "lm_head.dense.weight True\n",
      "lm_head.dense.bias True\n",
      "lm_head.layer_norm.weight True\n",
      "lm_head.layer_norm.bias True\n"
     ]
    }
   ],
   "source": [
    "# Freeze parameters except the last head\n",
    "\n",
    "for name, param in model_b1.named_parameters():\n",
    "    if \"lm_head\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    print(name, param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class CodeNetDataset(Dataset):\n",
    "    def __init__(self, window_df, tokenizer):\n",
    "        self.window_df = window_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "        # Not sure how to apply tqdm (progress bar) for this; I plan to update soon\n",
    "        self.tokenized = tokenizer(list(window_df[\"window\"]), padding=True)\n",
    "        self.input_ids = self.tokenized[\"input_ids\"]\n",
    "        self.attention_mask = self.tokenized[\"attention_mask\"]\n",
    "        self.label = self.window_df[\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_token_id = self.tokenizer.convert_tokens_to_ids(self.label[index])\n",
    "        # size is equal to the vocabulary size\n",
    "        one_hot_label = [0] * self.tokenizer.vocab_size\n",
    "        one_hot_label[label_token_id] = 1\n",
    "        mask_token_index = self.input_ids[index].index(self.mask_token_id)\n",
    "\n",
    "        return torch.tensor(self.input_ids[index]), torch.tensor(self.attention_mask[index]), torch.tensor(one_hot_label), torch.tensor(mask_token_index)\n",
    "\n",
    "train_dataset = CodeNetDataset(window_train_df, tokenizer_b1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-tuning (Cross Entropy)\n",
    "\n",
    "As we are using another sentence embedding model for calculating the cosine similarity, the loss value needs to be backpropagated through the embedding model as well. But it seems infeasible for this moment, so I will going to implement the loss function only on the cross-entropy (perplexity). Also, we only use the top_1 prediction as it showed the best performance in the baseline testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Loss function (Softmax + CrossEntropy)\n",
    "# The result values from the model is logits, which cannot be compared with one-hot true labels.\n",
    "# For example, logits varies from the negative infinity to the positive infinity, and the positive infinity is equal to the 100% probability.\n",
    "# But the one-hot true labels has only 0 and 1, and 1 means 100% probability.\n",
    "# So the Softmax will be applied on the logits, before they are compared by the CrossEntropy.\n",
    "def soft_entropy(prediction_logits, one_hot_labels, mask_token_indices):\n",
    "    # Getting the embeddings only for the mask token locations\n",
    "    mask_embedding_list = list()\n",
    "    for row, mask_token_index in zip(range(prediction_logits.shape[0]), mask_token_indices):\n",
    "        # The sliced tensors are single-dimensional vectors. So we need to add a dummy dimension at dim=0\n",
    "        # so that they can be concatenated in dim=0\n",
    "        # For example, if the sliced tensors have the shape [5], torch.unsqueeze() makes it to [1, 5]\n",
    "        mask_embedding = torch.unsqueeze(prediction_logits[row, mask_token_index], 0)\n",
    "        mask_embedding_list.append(mask_embedding)\n",
    "\n",
    "    # Shape = [batch_size, vocabulary_size]\n",
    "    mask_embeddings = torch.cat(mask_embedding_list, dim=0)\n",
    "    probabilities = F.softmax(mask_embeddings, dim=1)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    # By default, it returns a single scalar (averaged over batch)\n",
    "    return loss(probabilities, one_hot_labels.float())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/257 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e06e1edeab1344109b9d7d12a72fa7ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m prediction_logits \u001B[38;5;241m=\u001B[39m model_b1(input_ids\u001B[38;5;241m=\u001B[39mbatch_input_ids, attention_mask\u001B[38;5;241m=\u001B[39mbatch_attention_mask)\u001B[38;5;241m.\u001B[39mlogits\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Computing prediction error\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43msoft_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprediction_logits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_one_hot_label\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_mask_token_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Removing gradients from the past iteration\u001B[39;00m\n\u001B[0;32m     28\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "Cell \u001B[1;32mIn [11], line 13\u001B[0m, in \u001B[0;36msoft_entropy\u001B[1;34m(prediction_logits, one_hot_labels, mask_token_indices)\u001B[0m\n\u001B[0;32m      8\u001B[0m mask_embedding_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m()\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row, mask_token_index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mrange\u001B[39m(prediction_logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]), mask_token_indices):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# The sliced tensors are single-dimensional vectors. So we need to add a dummy dimension at dim=0\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# so that they can be concatenated in dim=0\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# For example, if the sliced tensors have the shape [5], torch.unsqueeze() makes it to [1, 5]\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m     mask_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprediction_logits\u001B[49m\u001B[43m[\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask_token_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     mask_embedding_list\u001B[38;5;241m.\u001B[39mappend(mask_embedding)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Shape = [batch_size, vocabulary_size]\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Running this cell multiple times in a single notebook can fully saturate the GPU memory, which leads the OutOfMemoryError.\n",
    "# If it happens, re-start the notebook kernel to remove all model instances from the GPU memory.\n",
    "# It seems like the memory error sometimes happens when the dataset size is too big, regardless of the batch size.\n",
    "# I am not sure why it is the case; Let me know if you encounter this issue.\n",
    "\n",
    "# Setting the model to the train mode\n",
    "model_b1.train()\n",
    "model_b1.to(device)\n",
    "\n",
    "print(f\"Total train set size: {len(train_dataset)}, batch_size: {batch_size}, batch_number: {math.ceil(len(train_dataset) / batch_size)}\")\n",
    "\n",
    "# Using the AdamW optimizer: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "# Feel free to try others\n",
    "optimizer = torch.optim.AdamW(model_b1.parameters(), lr=learning_rate, weight_decay=weight_decay_coefficient)\n",
    "\n",
    "for epoch_num in range(1, total_epoch_number+1):\n",
    "    loss_list = list()\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch_num}\", total=math.ceil(len(train_dataset) / batch_size)):\n",
    "        # Sending to GPU\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_one_hot_label = batch[2].to(device)\n",
    "        batch_mask_token_index = batch[3].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        prediction_logits = model_b1(input_ids=batch_input_ids, attention_mask=batch_attention_mask).logits\n",
    "        # Computing prediction error\n",
    "        loss = soft_entropy(prediction_logits, batch_one_hot_label, batch_mask_token_index)\n",
    "        # Removing gradients from the past iteration\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Removing to free up memory\n",
    "        del batch_input_ids\n",
    "        del batch_attention_mask\n",
    "        del batch_one_hot_label\n",
    "        del batch_mask_token_index\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch_num}, Cross Entropy Loss: {np.mean(loss_list)}\")\n",
    "\n",
    "# Setting the model back to the evaluation mode\n",
    "model_b1.eval()\n",
    "# Not to print the model structure\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation (Cosine similarity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Window split:   0%|          | 0/23107 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34919356a5f54ce49ba37597e5d10d9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Prediction:   0%|          | 0/67804 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d570a2913c64abb99211546476942cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Similarity:   0%|          | 0/67804 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1345900de62c488cbf4fa97917c39858"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b1_finetuned = pipeline('fill-mask', model=model_b1, tokenizer=tokenizer_b1, device=device_num)\n",
    "b1_finetuned_result = model_test(merged_code_df=merged_valid_df, unmasker=b1_finetuned, top_k=1, window_size=window_size, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity: 0.49379345774650574\n",
      "The fine-tuned model performance is better than the baseline, by 0.01018860936164856\n"
     ]
    }
   ],
   "source": [
    "average_similarity = np.mean(b1_finetuned_result['similarity'])\n",
    "# Baseline performance (I plan to send you the result by this evening)\n",
    "baseline_similarity = 0.4816761314868927\n",
    "print(f\"Average cosine similarity: {average_similarity}\")\n",
    "if average_similarity < baseline_similarity:\n",
    "    print(f\"The fine-tuned model performance is worse than the baseline, by {baseline_similarity-average_similarity}\")\n",
    "else:\n",
    "    print(f\"The fine-tuned model performance is better than the baseline, by {average_similarity-baseline_similarity}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the model\n",
    "\n",
    "Check the following for loading the model: https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Saving the hyperparameters and the fine-tuned model\n",
    "# The model size is around 500 MB. You may want to save the dictionary only\n",
    "training_constants = [batch_size, shuffle, size_proportion_train, total_epoch_number, learning_rate, weight_decay_coefficient, mask_prob, window_size, rng_seed, size_proportion_valid]\n",
    "result_dict = {\"training_constants\": training_constants, \"validation_similarity\": average_similarity, \"loss_metric\": \"cross_entropy\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Generating non-existing filenames\n",
    "file_index = 0\n",
    "while os.path.exists(f\"./saved_models/model_{file_index}\"):\n",
    "    file_index += 1\n",
    "\n",
    "model_filepath = f\"./saved_models/model_{file_index}\"\n",
    "hparameter_filepath = f\"./saved_models/hparameter_{file_index}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model_b1, model_filepath)\n",
    "# Saving the hyperparameters\n",
    "with open(hparameter_filepath, \"wb\") as fw:\n",
    "    pickle.dump(result_dict, fw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
