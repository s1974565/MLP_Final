{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7fa15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "# Huggingface dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Sentence similarity model\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Baseline model 1\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f883968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU acceleration is available\n",
    "if torch.cuda.is_available():\n",
    "    device_num = torch.cuda.current_device()\n",
    "else:\n",
    "    # CPU\n",
    "    device_num = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700cacb",
   "metadata": {},
   "source": [
    "### Data load and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a61540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f, orient='records', compression='gzip', lines=True)[columns] for f in file_list], \n",
    "                     sort=False)\n",
    "def get_dfs(path):\n",
    "    \"\"\"Grabs the different data splits and converts them into dataframes\"\"\"\n",
    "    dfs = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted(glob.glob(path+\"/\"+split+\"**/*.gz\"))\n",
    "        df = jsonl_list_to_dataframe(files, [\"func_name\", \"code\", \"code_tokens\", \"repo\"])\n",
    "        dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ea4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving the original files into pickle files.\n",
    "# Do not need to run this cell again, unless you removed the pickle files.\n",
    "# df_train, df_valid, df_test = get_dfs(\"data/codenet/python/final/jsonl\")\n",
    "\n",
    "# df_train.to_pickle(\"train.pickle\")\n",
    "# df_valid.to_pickle(\"valid.pickle\")\n",
    "# df_test.to_pickle(\"test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0598a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(\"train.pickle\").reset_index(drop=True)\n",
    "df_valid = pd.read_pickle(\"valid.pickle\").reset_index(drop=True)\n",
    "df_test = pd.read_pickle(\"test.pickle\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b010b5",
   "metadata": {},
   "source": [
    "### Helper methods for baseline models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4644023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_print(input_sequence, unmasker, true_labels=None, top_k=2, mask_token=\"<mask>\"):\n",
    "    mask_num = input_sequence.count(mask_token)\n",
    "    output = unmasker(input_sequence, top_k=top_k)\n",
    "    if mask_num == 1:\n",
    "        print(\"-\" * 50)\n",
    "        if true_labels:\n",
    "            print(f\"True label: {true_labels[0]}\")\n",
    "            print(\"\")\n",
    "        for candidate in output:\n",
    "            print(f\"Predicted_word: {candidate['token_str']}\")\n",
    "            print(f\"Probability: {round(candidate['score'], 3)}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\")\n",
    "        \n",
    "    else:\n",
    "        for index, word_prediction in enumerate(output):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Mask number: {index}\")\n",
    "            if true_labels:\n",
    "                print(f\"True label: {true_labels[index]}\")\n",
    "                print(\"\")\n",
    "            for candidate in word_prediction:\n",
    "                print(f\"Predicted_word: {candidate['token_str']}\")\n",
    "                print(f\"Probability: {round(candidate['score'], 3)}\")\n",
    "            print(\"-\" * 50)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605b5645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def learn(env,\\n          network,\\n          seed=None,\\n          callback=None,\\n          load_path=None,\\n          **network_kwargs\\n            )',\n",
       " '# Create all the functions necessary to train the model\\n',\n",
       " 'sess = get_session()\\n',\n",
       " 'q_func = build_q_func(network, **network_kwargs)\\n',\n",
       " '# capture the shape outside the closure so that the env object is not serialized\\n',\n",
       " '# by cloudpickle when serializing make_obs_ph\\n',\n",
       " 'observation_space = env.observation_space\\n',\n",
       " 'def make_obs_ph(name)',\n",
       " 'return ObservationInput(observation_space, name=name)\\n',\n",
       " 'act, train, update_target, debug = deepq.build_train(\\n',\n",
       " 'make_obs_ph=make_obs_ph,\\n',\n",
       " 'q_func=q_func,\\n',\n",
       " 'num_actions=env.action_space.n,\\n',\n",
       " 'optimizer=tf.train.AdamOptimizer(learning_rate=lr),\\n',\n",
       " 'gamma=gamma,\\n',\n",
       " 'grad_norm_clipping=10,\\n',\n",
       " 'param_noise=param_noise\\n',\n",
       " 'act_params = {\\n',\n",
       " 'act = ActWrapper(act, act_params)\\n',\n",
       " '# Create the replay buffer\\n',\n",
       " 'replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\\n',\n",
       " 'prioritized_replay_beta_iters = total_timesteps\\n',\n",
       " 'beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\\n',\n",
       " 'initial_p=prioritized_replay_beta0,\\n',\n",
       " 'final_p=1.0)\\n',\n",
       " 'replay_buffer = ReplayBuffer(buffer_size)\\n',\n",
       " 'beta_schedule = None\\n',\n",
       " '# Create the schedule for exploration starting from 1.\\n',\n",
       " 'exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\\n',\n",
       " 'initial_p=1.0,\\n',\n",
       " 'final_p=exploration_final_eps)\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable masker testing\n",
    "test = \"\"\"\n",
    "def learn(env,\n",
    "          network,\n",
    "          seed=None,\n",
    "          callback=None,\n",
    "          load_path=None,\n",
    "          **network_kwargs\n",
    "            ):\n",
    "    act: ActWrapper\n",
    "        Wrapper over act function. Adds ability to save it and load it.\n",
    "        See header of baselines/deepq/categorical.py for details on the act function.\n",
    "    # Create all the functions necessary to train the model\n",
    "\n",
    "    sess = get_session()\n",
    "    set_global_seeds(seed)\n",
    "\n",
    "    q_func = build_q_func(network, **network_kwargs)\n",
    "\n",
    "    # capture the shape outside the closure so that the env object is not serialized\n",
    "    # by cloudpickle when serializing make_obs_ph\n",
    "\n",
    "    observation_space = env.observation_space\n",
    "    def make_obs_ph(name):\n",
    "        return ObservationInput(observation_space, name=name)\n",
    "\n",
    "    act, train, update_target, debug = deepq.build_train(\n",
    "        make_obs_ph=make_obs_ph,\n",
    "        q_func=q_func,\n",
    "        num_actions=env.action_space.n,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
    "        gamma=gamma,\n",
    "        grad_norm_clipping=10,\n",
    "        param_noise=param_noise\n",
    "    )\n",
    "\n",
    "    act_params = {\n",
    "        'make_obs_ph': make_obs_ph,\n",
    "        'q_func': q_func,\n",
    "        'num_actions': env.action_space.n,\n",
    "    }\n",
    "\n",
    "    act = ActWrapper(act, act_params)\n",
    "\n",
    "    # Create the replay buffer\n",
    "    if prioritized_replay:\n",
    "        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "        if prioritized_replay_beta_iters is None:\n",
    "            prioritized_replay_beta_iters = total_timesteps\n",
    "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
    "                                       initial_p=prioritized_replay_beta0,\n",
    "                                       final_p=1.0)\n",
    "    else:\n",
    "        replay_buffer = ReplayBuffer(buffer_size)\n",
    "        beta_schedule = None\n",
    "    # Create the schedule for exploration starting from 1.\n",
    "    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
    "                                 initial_p=1.0,\n",
    "                                 final_p=exploration_final_eps)\n",
    "\"\"\"\n",
    "\n",
    "pattern = r\"(\\bdef\\s\\w*\\(.*?\\)):|(#\\s*.*?\\n)|(return\\s*.*?\\n)|(\\b[\\w,\\s]*=\\s*.*?\\n)\"\n",
    "matches = [str().join(x) for x in re.findall(pattern, test, flags=re.DOTALL)]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0784c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic assumption: The same line of code never occurs twice.\n",
    "def mask_variable_names(code, mask_prob):\n",
    "    \"\"\"\n",
    "    Mask the values of variables in a code with a certain probability.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to match variable assignments\n",
    "    # Function signature (to be filtered out later) | common variable definitions\n",
    "    pattern = r\"(\\bdef\\s\\w*\\(.*?\\)):|(#\\s*.*?\\n)|(return\\s*.*?\\n)|(\\b[\\w,\\s]*=\\s*.*?\\n)\"\n",
    "    matches = [str().join(x) for x in re.findall(pattern, code, flags=re.DOTALL)]\n",
    "    var_indices = list()\n",
    "    var_labels = list()\n",
    "    # characters that should not exist in the first sub part of a found match.\n",
    "    invalid_list = [\"(\", \")\", \"def\", \"#\", \"return\"]\n",
    "    \n",
    "    # If there is a variable found\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            # Split the match into sub-parts by the equal sign, and check if the first sub-part contain any parenthesis\n",
    "            # or \"def\" (implies function signature).\n",
    "            # If not, then the first sub-part is variable(s).\n",
    "            first_sub_part = match.split(\"=\")[0]\n",
    "            if not any([invalid_character in first_sub_part for invalid_character in invalid_list]):\n",
    "                variables = set(re.split(\",|=\", first_sub_part))\n",
    "                \n",
    "                # Masking variables based on the mask_prob\n",
    "                masked_match = str(match)\n",
    "                match_begin_index = code.find(masked_match)\n",
    "                for var in variables:\n",
    "                    # If beginning of the function call, then process no further.\n",
    "                    if \"(\" in var:\n",
    "                        break\n",
    "                    if np.random.uniform() < mask_prob:\n",
    "                        var_begin_index = masked_match.find(var.strip())\n",
    "                        var_index = (var_begin_index + match_begin_index, var_begin_index + match_begin_index + len(var.strip()))\n",
    "                        var_indices.append(var_index)\n",
    "                        var_labels.append(var.strip())\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return var_indices, var_labels\n",
    "    \n",
    "    # If no variable is found\n",
    "    else:\n",
    "        return code, list()\n",
    "        \n",
    "def mask_variable_df(df, code_column_name=\"code\", mask_prob=0.5, return_df=True):\n",
    "    variable_indices_list = list()\n",
    "    variable_labels_list = list()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        variable_indices, variable_labels = mask_variable_names(row[\"code\"], mask_prob)\n",
    "        variable_indices_list.append(variable_indices)\n",
    "        variable_labels_list.append(variable_labels)\n",
    "        \n",
    "    if return_df:\n",
    "        return pd.DataFrame({\"variable_indices\" : variable_indices_list, \"variable_labels\" : variable_labels_list})\n",
    "    else:\n",
    "        return variable_indices_list, variable_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250b2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_se = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "def cosine_similarity(sentences, model=model_se):\n",
    "    embeddings = model.encode(sentences)\n",
    "    return np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fec8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_docstring(code):\n",
    "    pattern = r'(\"\"\".*?\"\"\")|(\\'\\'\\'.*?\\'\\'\\')'\n",
    "    return re.sub(pattern, '', code, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e4a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_substring_indices(text, substring):\n",
    "    pattern = re.compile(f'{substring}')\n",
    "    indices = [(match.start(), match.end()-1) for match in pattern.finditer(text)]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9d2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_windows(row, window_size, mask_token, testing=False):\n",
    "    windows = list()\n",
    "    \n",
    "    for variable_indices, variable_labels in zip(row[\"variable_indices\"], row[\"variable_labels\"]):\n",
    "        # Window indices\n",
    "        begin_index = variable_indices[0] - window_size if variable_indices[0] - window_size > 0 else 0\n",
    "        end_index = variable_indices[1] + window_size if variable_indices[1] + window_size < len(row[\"code\"]) else len(row[\"code\"])\n",
    "        \n",
    "        current_window = row[\"code\"][begin_index : variable_indices[0]] + mask_token + row[\"code\"][variable_indices[1] : end_index]\n",
    "        windows.append(current_window)\n",
    "\n",
    "        if testing:\n",
    "            print(current_window)\n",
    "            print(\"--------\")\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4c9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the prediction to the given masked code. If top_k is bigger than 1, then the top_k predictions\n",
    "# will be concatenated by the given top_k_connection. Each prediction(s) will be stripped to remove unnecessary whitespaces.\n",
    "def mask_prediction(row, top_k, unmasker, top_k_connection, mask_token, window_size):\n",
    "    mask_num = len(row[\"variable_indices\"])\n",
    "    predictions = list()\n",
    "    \n",
    "    if mask_num == 0:\n",
    "        return predictions\n",
    "    \n",
    "    else:\n",
    "        windows = split_into_windows(row, window_size, mask_token)\n",
    "        for window in windows:\n",
    "            output = unmasker(window, top_k=top_k)\n",
    "            candidate_concat = top_k_connection.join([candidate['token_str'].strip() for candidate in output])\n",
    "            predictions.append(candidate_concat)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd68e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the given code dataframe, it automatically masks the codes and fill the masks by the supplied unmasker.\n",
    "# The predicted results are then compared with the true labels, with cosine similarity.\n",
    "# If top_k is set bigger than 1, then top_k number of predictions will be concatenated to form a single predictions\n",
    "# by the top_k_connection (default to the underscore).\n",
    "# For example, if the predictions are: \"A\", \"B\", and \"C\", then top_k = 2, the final prediction will be \"A_B\".\n",
    "\n",
    "# Currently runtime errors will be ignored. Runtime errors happen when the given code is longer than the maximum\n",
    "# size of the unmasker model (512 tokens)\n",
    "\n",
    "# Pre-trained transformers typically can take up to 512 tokens. Thus, if the given code is larger than this,\n",
    "# then a RuntimeError will be raised. To avoid this, the window_size variable is added. It regulates the amount of\n",
    "# context which will be give to the unmasker. If it is set to 100, total 200 characters will be given to the unmasker:\n",
    "# 100 characters before the mask token, and 100 characters after the mask token.\n",
    "# For example, 100 characters <mask> 100 characters\n",
    "def model_test(code_df, unmasker, mask_token=\"<mask>\", mask_prob=0.5, top_k=1, top_k_connection=\"_\", \n",
    "                  code_column_name=\"code\", window_size=100):\n",
    "    \n",
    "    masked_code_df = mask_variable_df(code_df, mask_prob=mask_prob, code_column_name=code_column_name)\n",
    "    merged_code_df = pd.concat([code_df, masked_code_df], axis=\"columns\")\n",
    "    \n",
    "    similarity_scores_list = list()\n",
    "    predictions_list = list()\n",
    "    true_labels_list = list()\n",
    "    \n",
    "    total_size = len(code_df)\n",
    "    for index, row in merged_code_df.iterrows():\n",
    "        if index % 1000 == 0:\n",
    "            print(f\"Progress: {round(index / total_size, 3) * 100}%\")\n",
    "        \n",
    "        true_labels = row[\"variable_labels\"]\n",
    "        true_labels_list.append(true_labels)\n",
    "        \n",
    "        # If the current code snippet is longer than the maximum input size of the given unmasker \n",
    "        # then the runtime error will be raised. Try to reduce window_size.\n",
    "        try:\n",
    "            predictions = mask_prediction(row, top_k, unmasker, top_k_connection, mask_token, window_size)\n",
    "            predictions_list.append(predictions)\n",
    "        except RuntimeError:\n",
    "            raise RuntimeError(\"The given input size is bigger than the maximum model input. Reduce the window size.\")\n",
    "        \n",
    "        similarity_scores = list()\n",
    "        for prediction, true_label in zip(predictions, true_labels):\n",
    "            similarity_scores.append(cosine_similarity([prediction, true_label]))\n",
    "        similarity_scores_list.append(similarity_scores)\n",
    "        \n",
    "    return predictions_list, true_labels_list, similarity_scores_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc2c9f",
   "metadata": {},
   "source": [
    "### Baseline score 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480788bc",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/microsoft/codebert-base-mlm <br>\n",
    "As stated in https://github.com/microsoft/CodeBERT, the basic CobeBERT is not suitable for filling-mask task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70fd1e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b1 = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base-mlm')\n",
    "tokenizer_b1 = RobertaTokenizer.from_pretrained('microsoft/codebert-base-mlm')\n",
    "fill_mask_b1 = pipeline('fill-mask', model=model_b1, tokenizer=tokenizer_b1, device=device_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9aaf530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted_word:  and\n",
      "Probability: 0.724\n",
      "Predicted_word:  &\n",
      "Probability: 0.106\n",
      "Predicted_word: and\n",
      "Probability: 0.022\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_example = \"if (x is not None) <mask> (x>1)\"\n",
    "output_print(code_example, fill_mask_b1, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a228f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40455922"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "model_ss = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "cosine_similarity(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae1abd12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinyoung\\anaconda3\\envs\\mlp\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 4.3%\n",
      "Progress: 8.7%\n",
      "Progress: 13.0%\n",
      "Progress: 17.299999999999997%\n",
      "Progress: 21.6%\n",
      "Progress: 26.0%\n",
      "Progress: 30.3%\n",
      "Progress: 34.599999999999994%\n",
      "Progress: 38.9%\n",
      "Progress: 43.3%\n",
      "Progress: 47.599999999999994%\n",
      "Progress: 51.9%\n",
      "Progress: 56.3%\n",
      "Progress: 60.6%\n",
      "Progress: 64.9%\n",
      "Progress: 69.19999999999999%\n",
      "Progress: 73.6%\n",
      "Progress: 77.9%\n",
      "Progress: 82.19999999999999%\n",
      "Progress: 86.6%\n",
      "Progress: 90.9%\n",
      "Progress: 95.19999999999999%\n",
      "Progress: 99.5%\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# It takes a lot of time, even though GPU acceleration is applied. It is because of many variables in each code snippet\n",
    "# and using the dataframe, not the huggingface api dataset. It should be refactored soon.\n",
    "# I do recommend to load the pickle files I saved below, instead of running this again.\n",
    "\n",
    "# You can just implement another metric and use it instead (implementation of perplexity should be straightforward).\n",
    "\n",
    "# top_k = 1\n",
    "b1_result_k1 = model_test(df_valid, fill_mask_b1, top_k=1)\n",
    "print(\"Finished\")\n",
    "\n",
    "# top_k = 2\n",
    "b1_result_k2 = model_test(df_valid, fill_mask_b1, top_k=2)\n",
    "print(\"Finished\")\n",
    "\n",
    "# top_k = 3\n",
    "b1_result_k3 = model_test(df_valid, fill_mask_b1, top_k=3)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8120ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out to prevent accidently overwriting these files.\n",
    "\n",
    "# with open(\"b1_result_k1.pickle\", \"wb\") as fw:\n",
    "#     pickle.dump(b1_result_k1, fw)\n",
    "\n",
    "# with open(\"b1_result_k2.pickle\", \"wb\") as fw:\n",
    "#     pickle.dump(b1_result_k2, fw)\n",
    "\n",
    "# with open(\"b1_result_k3.pickle\", \"wb\") as fw:\n",
    "#     pickle.dump(b1_result_k3, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e92bb",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d0eb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_result_k1 = pd.read_pickle(\"b1_result_k1.pickle\")\n",
    "b1_result_k2 = pd.read_pickle(\"b1_result_k2.pickle\")\n",
    "b1_result_k3 = pd.read_pickle(\"b1_result_k3.pickle\")\n",
    "b2_result_k1 = pd.read_pickle(\"b2_result_k1.pickle\")\n",
    "b2_result_k2 = pd.read_pickle(\"b2_result_k2.pickle\")\n",
    "b2_result_k3 = pd.read_pickle(\"b2_result_k3.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e26b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_result_print(result_dict, n=0):\n",
    "    predictions_list, true_labels_list, similarity_scores_list = result_dict\n",
    "    \n",
    "    n = 0\n",
    "    for prediction, true, score in zip(predictions_list[n], true_labels_list[n], similarity_scores_list[n]):\n",
    "        print(f\"True: {true}\")\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "        print(f\"Similarity: {score}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"Average similarity:\", np.mean(similarity_scores_list[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cc7a3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: act\n",
      "Prediction: obs\n",
      "Similarity: 0.21532297134399414\n",
      "--------------------------------------------------\n",
      "True: train\n",
      "Prediction: target\n",
      "Similarity: 0.3601682782173157\n",
      "--------------------------------------------------\n",
      "True: make_obs_ph\n",
      "Prediction: fn\n",
      "Similarity: 0.07105673104524612\n",
      "--------------------------------------------------\n",
      "True: num_actions\n",
      "Prediction: n\n",
      "Similarity: 0.2538803219795227\n",
      "--------------------------------------------------\n",
      "True: optimizer\n",
      "Prediction: model\n",
      "Similarity: 0.32537388801574707\n",
      "--------------------------------------------------\n",
      "True: act_params\n",
      "Prediction: env\n",
      "Similarity: 0.20330965518951416\n",
      "--------------------------------------------------\n",
      "True: replay_buffer\n",
      "Prediction: buff\n",
      "Similarity: 0.30218684673309326\n",
      "--------------------------------------------------\n",
      "True: prioritized_replay_beta_iters\n",
      "Prediction: beta\n",
      "Similarity: 0.46546584367752075\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: beta\n",
      "Similarity: 0.16273127496242523\n",
      "--------------------------------------------------\n",
      "True: beta_schedule\n",
      "Prediction: p\n",
      "Similarity: 0.09043783694505692\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: p\n",
      "Similarity: 0.34524935483932495\n",
      "--------------------------------------------------\n",
      "True: final_p\n",
      "Prediction: steps\n",
      "Similarity: 0.2611233592033386\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: obs\n",
      "Similarity: 0.10260843485593796\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: train\n",
      "Similarity: 0.13209030032157898\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: result\n",
      "Similarity: 0.11304620653390884\n",
      "--------------------------------------------------\n",
      "True: kwargs\n",
      "Prediction: t\n",
      "Similarity: 0.24170541763305664\n",
      "--------------------------------------------------\n",
      "True: update_eps\n",
      "Prediction: threshold\n",
      "Similarity: -0.022503988817334175\n",
      "--------------------------------------------------\n",
      "True: done\n",
      "Prediction: _\n",
      "Similarity: 0.25104501843452454\n",
      "--------------------------------------------------\n",
      "True: rew\n",
      "Prediction: _\n",
      "Similarity: 0.2951238453388214\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: /\n",
      "Similarity: 0.23818574845790863\n",
      "--------------------------------------------------\n",
      "True: reset\n",
      "Prediction: obs\n",
      "Similarity: 0.18008483946323395\n",
      "--------------------------------------------------\n",
      "True: train_freq\n",
      "Prediction: 2\n",
      "Similarity: 0.1965228021144867\n",
      "--------------------------------------------------\n",
      "True: obses_tp1\n",
      "Prediction: losses\n",
      "Similarity: 0.1120685413479805\n",
      "--------------------------------------------------\n",
      "True: actions\n",
      "Prediction: weights\n",
      "Similarity: 0.2512039542198181\n",
      "--------------------------------------------------\n",
      "True: rewards\n",
      "Prediction: rewards\n",
      "Similarity: 1.0\n",
      "--------------------------------------------------\n",
      "True: batch_idxes\n",
      "Prediction: _\n",
      "Similarity: 0.03399688005447388\n",
      "--------------------------------------------------\n",
      "True: num_episodes\n",
      "Prediction: pr\n",
      "Similarity: 0.13339605927467346\n",
      "--------------------------------------------------\n",
      "True: print_freq\n",
      "Prediction: 2\n",
      "Similarity: 0.05696188285946846\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: *\n",
      "Similarity: 0.20381414890289307\n",
      "--------------------------------------------------\n",
      "Average similarity: 0.22674678\n"
     ]
    }
   ],
   "source": [
    "baseline_result_print(b1_result_k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cebac60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: sess\n",
      "Prediction: #_~\n",
      "Similarity: 0.2401028424501419\n",
      "--------------------------------------------------\n",
      "True: q_func\n",
      "Prediction: q_network\n",
      "Similarity: 0.565737247467041\n",
      "--------------------------------------------------\n",
      "True: observation_space\n",
      "Prediction: *_/\n",
      "Similarity: 0.1880900263786316\n",
      "--------------------------------------------------\n",
      "True: update_target\n",
      "Prediction: log_test\n",
      "Similarity: 0.19106894731521606\n",
      "--------------------------------------------------\n",
      "True: act\n",
      "Prediction: obs_train\n",
      "Similarity: 0.2138705849647522\n",
      "--------------------------------------------------\n",
      "True: train\n",
      "Prediction: target_update\n",
      "Similarity: 0.21113455295562744\n",
      "--------------------------------------------------\n",
      "True: debug\n",
      "Prediction: __target\n",
      "Similarity: 0.2655183672904968\n",
      "--------------------------------------------------\n",
      "True: make_obs_ph\n",
      "Prediction: fn_model\n",
      "Similarity: 0.18106552958488464\n",
      "--------------------------------------------------\n",
      "True: q_func\n",
      "Prediction: q_func\n",
      "Similarity: 1.0\n",
      "--------------------------------------------------\n",
      "True: num_actions\n",
      "Prediction: n_N\n",
      "Similarity: 0.16884814202785492\n",
      "--------------------------------------------------\n",
      "True: optimizer\n",
      "Prediction: model_trainer\n",
      "Similarity: 0.17832933366298676\n",
      "--------------------------------------------------\n",
      "True: gamma\n",
      "Prediction: gamma_alpha\n",
      "Similarity: 0.7922375798225403\n",
      "--------------------------------------------------\n",
      "True: grad_norm_clipping\n",
      "Prediction: beta_batch\n",
      "Similarity: 0.13248951733112335\n",
      "--------------------------------------------------\n",
      "True: param_noise\n",
      "Prediction: noise_noisy\n",
      "Similarity: 0.7045753598213196\n",
      "--------------------------------------------------\n",
      "True: act_params\n",
      "Prediction: env_*\n",
      "Similarity: 0.3816404938697815\n",
      "--------------------------------------------------\n",
      "True: act\n",
      "Prediction: act_action\n",
      "Similarity: 0.7178312540054321\n",
      "--------------------------------------------------\n",
      "True: replay_buffer\n",
      "Prediction: buff_r\n",
      "Similarity: 0.3822135925292969\n",
      "--------------------------------------------------\n",
      "True: prioritized_replay_beta_iters\n",
      "Prediction: beta_n\n",
      "Similarity: 0.3659617602825165\n",
      "--------------------------------------------------\n",
      "True: beta_schedule\n",
      "Prediction: schedule_sched\n",
      "Similarity: 0.6216965913772583\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: beta_alpha\n",
      "Similarity: 0.328564316034317\n",
      "--------------------------------------------------\n",
      "True: final_p\n",
      "Prediction: beta_alpha\n",
      "Similarity: 0.3944200277328491\n",
      "--------------------------------------------------\n",
      "True: replay_buffer\n",
      "Prediction: b_buf\n",
      "Similarity: 0.21648672223091125\n",
      "--------------------------------------------------\n",
      "True: beta_schedule\n",
      "Prediction: p_f\n",
      "Similarity: 0.16522163152694702\n",
      "--------------------------------------------------\n",
      "True: exploration\n",
      "Prediction: schedule_s\n",
      "Similarity: 0.11000853031873703\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: p_alpha\n",
      "Similarity: 0.4153524935245514\n",
      "--------------------------------------------------\n",
      "True: final_p\n",
      "Prediction: steps_p\n",
      "Similarity: 0.51381516456604\n",
      "--------------------------------------------------\n",
      "True: episode_rewards\n",
      "Prediction: /_*\n",
      "Similarity: 0.22935470938682556\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: obs_~\n",
      "Similarity: 0.21580705046653748\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: #_output\n",
      "Similarity: 0.13492047786712646\n",
      "--------------------------------------------------\n",
      "True: reset\n",
      "Prediction: obs_cont\n",
      "Similarity: 0.24347208440303802\n",
      "--------------------------------------------------\n",
      "True: td\n",
      "Prediction: td_dd\n",
      "Similarity: 0.6360378861427307\n",
      "--------------------------------------------------\n",
      "True: model_file\n",
      "Prediction: +_f\n",
      "Similarity: 0.1835397630929947\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: train_loaded\n",
      "Similarity: 0.2801292836666107\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: result_success\n",
      "Similarity: 0.26805809140205383\n",
      "--------------------------------------------------\n",
      "True: kwargs\n",
      "Prediction: t_*\n",
      "Similarity: 0.17060887813568115\n",
      "--------------------------------------------------\n",
      "True: update_eps\n",
      "Prediction: t_value\n",
      "Similarity: 0.2114836871623993\n",
      "--------------------------------------------------\n",
      "True: update_param_noise_threshold\n",
      "Prediction: /_*\n",
      "Similarity: 0.0307383481413126\n",
      "--------------------------------------------------\n",
      "True: update_eps\n",
      "Prediction: threshold_t\n",
      "Similarity: 0.07701138406991959\n",
      "--------------------------------------------------\n",
      "True: update_param_noise_threshold\n",
      "Prediction: p_t\n",
      "Similarity: 0.11615577340126038\n",
      "--------------------------------------------------\n",
      "True: True\n",
      "            action\n",
      "Prediction: action_scale\n",
      "Similarity: 0.4174080193042755\n",
      "--------------------------------------------------\n",
      "True: env_action\n",
      "Prediction: reset_action\n",
      "Similarity: 0.5044904351234436\n",
      "--------------------------------------------------\n",
      "True: reset\n",
      "Prediction: done_action\n",
      "Similarity: 0.2595355808734894\n",
      "--------------------------------------------------\n",
      "True: done\n",
      "Prediction: __reset\n",
      "Similarity: 0.3205026090145111\n",
      "--------------------------------------------------\n",
      "True: rew\n",
      "Prediction: __reset\n",
      "Similarity: 0.32598018646240234\n",
      "--------------------------------------------------\n",
      "True: _\n",
      "Prediction: __J\n",
      "Similarity: 0.4968516528606415\n",
      "--------------------------------------------------\n",
      "True: new_obs\n",
      "Prediction: __res\n",
      "Similarity: 0.32639235258102417\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: /_*\n",
      "Similarity: 0.22393091022968292\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: #_output\n",
      "Similarity: 0.13492047786712646\n",
      "--------------------------------------------------\n",
      "True: reset\n",
      "Prediction: obs_cont\n",
      "Similarity: 0.24347208440303802\n",
      "--------------------------------------------------\n",
      "True: train_freq\n",
      "Prediction: 2_3\n",
      "Similarity: 0.2877919673919678\n",
      "--------------------------------------------------\n",
      "True: experience\n",
      "Prediction: t_r\n",
      "Similarity: 0.1928478628396988\n",
      "--------------------------------------------------\n",
      "True: obses_tp1\n",
      "Prediction: losses_rewards\n",
      "Similarity: 0.19121669232845306\n",
      "--------------------------------------------------\n",
      "True: dones\n",
      "Prediction: weights_weight\n",
      "Similarity: 0.13362522423267365\n",
      "--------------------------------------------------\n",
      "True: obses_t\n",
      "Prediction: experience_rewards\n",
      "Similarity: 0.2953135073184967\n",
      "--------------------------------------------------\n",
      "True: actions\n",
      "Prediction: weights_weight\n",
      "Similarity: 0.10068301111459732\n",
      "--------------------------------------------------\n",
      "True: rewards\n",
      "Prediction: rewards_outcomes\n",
      "Similarity: 0.7872101664543152\n",
      "--------------------------------------------------\n",
      "True: weights\n",
      "Prediction: #_None\n",
      "Similarity: 0.13757388293743134\n",
      "--------------------------------------------------\n",
      "True: batch_idxes\n",
      "Prediction: __weights\n",
      "Similarity: 0.015574327670037746\n",
      "--------------------------------------------------\n",
      "True: td_errors\n",
      "Prediction: acc_results\n",
      "Similarity: 0.26100561022758484\n",
      "--------------------------------------------------\n",
      "True: new_priorities\n",
      "Prediction: ba_ba\n",
      "Similarity: 0.23707427084445953\n",
      "--------------------------------------------------\n",
      "True: target_network_update_freq\n",
      "Prediction: 2_3\n",
      "Similarity: 0.09417448192834854\n",
      "--------------------------------------------------\n",
      "True: mean_100ep_reward\n",
      "Prediction: /_*\n",
      "Similarity: 0.22365178167819977\n",
      "--------------------------------------------------\n",
      "True: num_episodes\n",
      "Prediction: pr_p\n",
      "Similarity: 0.2210979461669922\n",
      "--------------------------------------------------\n",
      "True: print_freq\n",
      "Prediction: 2_100\n",
      "Similarity: 0.1965571790933609\n",
      "--------------------------------------------------\n",
      "True: checkpoint_freq\n",
      "Prediction: 2_100\n",
      "Similarity: 0.1741710603237152\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: result_success\n",
      "Similarity: 0.26805809140205383\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: *_/\n",
      "Similarity: 0.17262041568756104\n",
      "--------------------------------------------------\n",
      "Average similarity: 0.29384077\n"
     ]
    }
   ],
   "source": [
    "baseline_result_print(b1_result_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc247134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: sess\n",
      "Prediction: #_~_=\n",
      "Similarity: 0.245636448264122\n",
      "--------------------------------------------------\n",
      "True: q_func\n",
      "Prediction: q_network_net\n",
      "Similarity: 0.47787046432495117\n",
      "--------------------------------------------------\n",
      "True: update_target\n",
      "Prediction: log_test_output\n",
      "Similarity: 0.1782887727022171\n",
      "--------------------------------------------------\n",
      "True: train\n",
      "Prediction: target_update_obs\n",
      "Similarity: 0.12537170946598053\n",
      "--------------------------------------------------\n",
      "True: num_actions\n",
      "Prediction: n_N_k\n",
      "Similarity: 0.18665048480033875\n",
      "--------------------------------------------------\n",
      "True: gamma\n",
      "Prediction: gamma_alpha_Gamma\n",
      "Similarity: 0.759575366973877\n",
      "--------------------------------------------------\n",
      "True: grad_norm_clipping\n",
      "Prediction: beta_batch_n\n",
      "Similarity: 0.13871708512306213\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: beta_alpha_beta\n",
      "Similarity: 0.3393092155456543\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: obs_~_cov\n",
      "Similarity: 0.21981918811798096\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: #_output_result\n",
      "Similarity: 0.0738007128238678\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: result_success_cont\n",
      "Similarity: 0.28299680352211\n",
      "--------------------------------------------------\n",
      "True: kwargs\n",
      "Prediction: t_*_f\n",
      "Similarity: 0.11614686250686646\n",
      "--------------------------------------------------\n",
      "True: update_eps\n",
      "Prediction: t_value_s\n",
      "Similarity: 0.24533110857009888\n",
      "--------------------------------------------------\n",
      "True: update_param_noise_threshold\n",
      "Prediction: /_*_*\n",
      "Similarity: 0.0470690056681633\n",
      "--------------------------------------------------\n",
      "True: update_param_noise_threshold\n",
      "Prediction: p_t_r\n",
      "Similarity: 0.17035634815692902\n",
      "--------------------------------------------------\n",
      "True: True\n",
      "            action\n",
      "Prediction: action_scale_obs\n",
      "Similarity: 0.3032773733139038\n",
      "--------------------------------------------------\n",
      "True: env_action\n",
      "Prediction: reset_action_res\n",
      "Similarity: 0.4331687092781067\n",
      "--------------------------------------------------\n",
      "True: rew\n",
      "Prediction: __reset_res\n",
      "Similarity: 0.30238017439842224\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: /_*_obs\n",
      "Similarity: 0.8922308683395386\n",
      "--------------------------------------------------\n",
      "True: train_freq\n",
      "Prediction: 2_3_4\n",
      "Similarity: 0.27631399035453796\n",
      "--------------------------------------------------\n",
      "True: experience\n",
      "Prediction: t_r_result\n",
      "Similarity: 0.05429886281490326\n",
      "--------------------------------------------------\n",
      "True: obses_tp1\n",
      "Prediction: losses_rewards_outcomes\n",
      "Similarity: 0.08281370252370834\n",
      "--------------------------------------------------\n",
      "True: dones\n",
      "Prediction: weights_weight__\n",
      "Similarity: 0.1888299137353897\n",
      "--------------------------------------------------\n",
      "True: rewards\n",
      "Prediction: rewards_outcomes_weights\n",
      "Similarity: 0.6718571782112122\n",
      "--------------------------------------------------\n",
      "True: new_priorities\n",
      "Prediction: ba_ba_bo\n",
      "Similarity: 0.23070913553237915\n",
      "--------------------------------------------------\n",
      "True: target_network_update_freq\n",
      "Prediction: 2_3_4\n",
      "Similarity: 0.06435677409172058\n",
      "--------------------------------------------------\n",
      "True: print_freq\n",
      "Prediction: 2_100_10\n",
      "Similarity: 0.2068009376525879\n",
      "--------------------------------------------------\n",
      "True: checkpoint_freq\n",
      "Prediction: 2_100_3\n",
      "Similarity: 0.2133147120475769\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: result_success_cont\n",
      "Similarity: 0.28299680352211\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: *_/_model\n",
      "Similarity: 0.15406954288482666\n",
      "--------------------------------------------------\n",
      "Average similarity: 0.26547858\n"
     ]
    }
   ],
   "source": [
    "baseline_result_print(b1_result_k3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac13422",
   "metadata": {},
   "source": [
    "### Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e9f9aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: train\n",
      "Prediction: listen\n",
      "Similarity: 0.3025377690792084\n",
      "--------------------------------------------------\n",
      "True: num_actions\n",
      "Prediction: n\n",
      "Similarity: 0.25388044118881226\n",
      "--------------------------------------------------\n",
      "True: gamma\n",
      "Prediction: scale\n",
      "Similarity: 0.2730632424354553\n",
      "--------------------------------------------------\n",
      "True: act\n",
      "Prediction: act\n",
      "Similarity: 1.0000001192092896\n",
      "--------------------------------------------------\n",
      "True: beta_schedule\n",
      "Prediction: pr\n",
      "Similarity: 0.17340350151062012\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: beta\n",
      "Similarity: 0.16273126006126404\n",
      "--------------------------------------------------\n",
      "True: beta_schedule\n",
      "Prediction: timeout\n",
      "Similarity: 0.18161919713020325\n",
      "--------------------------------------------------\n",
      "True: exploration\n",
      "Prediction: r\n",
      "Similarity: 0.2917519211769104\n",
      "--------------------------------------------------\n",
      "True: episode_rewards\n",
      "Prediction: value\n",
      "Similarity: 0.26213863492012024\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: obs\n",
      "Similarity: 0.10260840505361557\n",
      "--------------------------------------------------\n",
      "True: td\n",
      "Prediction: os\n",
      "Similarity: 0.14934439957141876\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: default\n",
      "Similarity: 0.18189994990825653\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: logger\n",
      "Similarity: 0.13784566521644592\n",
      "--------------------------------------------------\n",
      "True: kwargs\n",
      "Prediction: t\n",
      "Similarity: 0.24170538783073425\n",
      "--------------------------------------------------\n",
      "True: update_eps\n",
      "Prediction: t\n",
      "Similarity: 0.07814161479473114\n",
      "--------------------------------------------------\n",
      "True: update_param_noise_threshold\n",
      "Prediction: t\n",
      "Similarity: 0.05742797628045082\n",
      "--------------------------------------------------\n",
      "True: True\n",
      "            action\n",
      "Prediction: action\n",
      "Similarity: 0.7902818322181702\n",
      "--------------------------------------------------\n",
      "True: reset\n",
      "Prediction: Status\n",
      "Similarity: 0.2963002324104309\n",
      "--------------------------------------------------\n",
      "True: done\n",
      "Prediction: _\n",
      "Similarity: 0.2510449290275574\n",
      "--------------------------------------------------\n",
      "True: new_obs\n",
      "Prediction: reset\n",
      "Similarity: 0.21339377760887146\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: env\n",
      "Similarity: 0.2691129446029663\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: env\n",
      "Similarity: 0.2691129446029663\n",
      "--------------------------------------------------\n",
      "True: reset\n",
      "Prediction: obs\n",
      "Similarity: 0.18008488416671753\n",
      "--------------------------------------------------\n",
      "True: rewards\n",
      "Prediction: weights\n",
      "Similarity: 0.42324143648147583\n",
      "--------------------------------------------------\n",
      "True: obses_tp1\n",
      "Prediction: actions\n",
      "Similarity: 0.14004802703857422\n",
      "--------------------------------------------------\n",
      "True: dones\n",
      "Prediction: batch\n",
      "Similarity: 0.2451983541250229\n",
      "--------------------------------------------------\n",
      "True: batch_idxes\n",
      "Prediction: weights\n",
      "Similarity: -0.0050620995461940765\n",
      "--------------------------------------------------\n",
      "True: target_network_update_freq\n",
      "Prediction: s\n",
      "Similarity: 0.026492446660995483\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: logger\n",
      "Similarity: 0.13784566521644592\n",
      "--------------------------------------------------\n",
      "Average similarity: 0.24438603\n"
     ]
    }
   ],
   "source": [
    "baseline_result_print(b2_result_k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01774394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: q_func\n",
      "Prediction: network_socket\n",
      "Similarity: 0.16541656851768494\n",
      "--------------------------------------------------\n",
      "True: make_obs_ph\n",
      "Prediction: f_name\n",
      "Similarity: 0.16172657907009125\n",
      "--------------------------------------------------\n",
      "True: q_func\n",
      "Prediction: func_callback\n",
      "Similarity: 0.5328258872032166\n",
      "--------------------------------------------------\n",
      "True: optimizer\n",
      "Prediction: r_train\n",
      "Similarity: 0.20479491353034973\n",
      "--------------------------------------------------\n",
      "True: gamma\n",
      "Prediction: scale_slope\n",
      "Similarity: 0.09026288241147995\n",
      "--------------------------------------------------\n",
      "True: param_noise\n",
      "Prediction: noise_param\n",
      "Similarity: 0.9270164370536804\n",
      "--------------------------------------------------\n",
      "True: act\n",
      "Prediction: act_action\n",
      "Similarity: 0.7178313136100769\n",
      "--------------------------------------------------\n",
      "True: replay_buffer\n",
      "Prediction: def_t\n",
      "Similarity: 0.21838600933551788\n",
      "--------------------------------------------------\n",
      "True: prioritized_replay_beta_iters\n",
      "Prediction: time_delay\n",
      "Similarity: 0.1442066878080368\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: beta_alpha\n",
      "Similarity: 0.3285643458366394\n",
      "--------------------------------------------------\n",
      "True: replay_buffer\n",
      "Prediction: buffer_Buffer\n",
      "Similarity: 0.7364556789398193\n",
      "--------------------------------------------------\n",
      "True: exploration\n",
      "Prediction: r_exploration\n",
      "Similarity: 0.7851594686508179\n",
      "--------------------------------------------------\n",
      "True: episode_rewards\n",
      "Prediction: value_default\n",
      "Similarity: 0.1659054458141327\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: obs_reset\n",
      "Similarity: 0.2463945746421814\n",
      "--------------------------------------------------\n",
      "True: model_file\n",
      "Prediction: model_model\n",
      "Similarity: 0.803358256816864\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: logger_log\n",
      "Similarity: 0.2536921799182892\n",
      "--------------------------------------------------\n",
      "True: update_eps\n",
      "Prediction: t_t\n",
      "Similarity: 0.12585575878620148\n",
      "--------------------------------------------------\n",
      "True: update_param_noise_threshold\n",
      "Prediction: t_t\n",
      "Similarity: 0.09135245531797409\n",
      "--------------------------------------------------\n",
      "True: update_eps\n",
      "Prediction: t_threshold\n",
      "Similarity: 0.05388859659433365\n",
      "--------------------------------------------------\n",
      "True: env_action\n",
      "Prediction: env__\n",
      "Similarity: 0.7471114993095398\n",
      "--------------------------------------------------\n",
      "True: reset\n",
      "Prediction: Status_()\n",
      "Similarity: 0.18066465854644775\n",
      "--------------------------------------------------\n",
      "True: rew\n",
      "Prediction: done_reset\n",
      "Similarity: 0.20508027076721191\n",
      "--------------------------------------------------\n",
      "True: new_obs\n",
      "Prediction: reset_Reset\n",
      "Similarity: 0.23466961085796356\n",
      "--------------------------------------------------\n",
      "True: rewards\n",
      "Prediction: weights_data\n",
      "Similarity: 0.16422109305858612\n",
      "--------------------------------------------------\n",
      "True: batch_idxes\n",
      "Prediction: weights_rewards\n",
      "Similarity: 0.07435883581638336\n",
      "--------------------------------------------------\n",
      "True: td_errors\n",
      "Prediction: es_s\n",
      "Similarity: 0.21065464615821838\n",
      "--------------------------------------------------\n",
      "True: new_priorities\n",
      "Prediction: ba_ba\n",
      "Similarity: 0.23707430064678192\n",
      "--------------------------------------------------\n",
      "True: model_saved\n",
      "Prediction: logger_log\n",
      "Similarity: 0.2536921799182892\n",
      "--------------------------------------------------\n",
      "Average similarity: 0.32359362\n"
     ]
    }
   ],
   "source": [
    "baseline_result_print(b2_result_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2da68234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: sess\n",
      "Prediction: seed_seeds_Seed\n",
      "Similarity: 0.15857379138469696\n",
      "--------------------------------------------------\n",
      "True: act\n",
      "Prediction: map_q_stack\n",
      "Similarity: 0.0807545930147171\n",
      "--------------------------------------------------\n",
      "True: update_target\n",
      "Prediction: num_log_run\n",
      "Similarity: 0.14218053221702576\n",
      "--------------------------------------------------\n",
      "True: q_func\n",
      "Prediction: func_callback_function\n",
      "Similarity: 0.49083712697029114\n",
      "--------------------------------------------------\n",
      "True: grad_norm_clipping\n",
      "Prediction: r_min_alpha\n",
      "Similarity: 0.16232645511627197\n",
      "--------------------------------------------------\n",
      "True: act_params\n",
      "Prediction: }_})_)\n",
      "Similarity: 0.09707866609096527\n",
      "--------------------------------------------------\n",
      "True: replay_buffer\n",
      "Prediction: def_t_act\n",
      "Similarity: 0.18078705668449402\n",
      "--------------------------------------------------\n",
      "True: beta_schedule\n",
      "Prediction: pr_)_Pr\n",
      "Similarity: 0.22758910059928894\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: beta_alpha_test\n",
      "Similarity: 0.2447570115327835\n",
      "--------------------------------------------------\n",
      "True: final_p\n",
      "Prediction: beta_alpha_version\n",
      "Similarity: 0.20965224504470825\n",
      "--------------------------------------------------\n",
      "True: replay_buffer\n",
      "Prediction: buffer_Buffer_buffers\n",
      "Similarity: 0.6323374509811401\n",
      "--------------------------------------------------\n",
      "True: beta_schedule\n",
      "Prediction: timeout_buffer_size\n",
      "Similarity: 0.14161404967308044\n",
      "--------------------------------------------------\n",
      "True: exploration\n",
      "Prediction: r_exploration_R\n",
      "Similarity: 0.7221541404724121\n",
      "--------------------------------------------------\n",
      "True: initial_p\n",
      "Prediction: time_min_max\n",
      "Similarity: 0.2583446204662323\n",
      "--------------------------------------------------\n",
      "True: episode_rewards\n",
      "Prediction: value_default_values\n",
      "Similarity: 0.15479113161563873\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: obs_reset_defaults\n",
      "Similarity: 0.20027051866054535\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: env_value_reset\n",
      "Similarity: 0.06187860667705536\n",
      "--------------------------------------------------\n",
      "True: True\n",
      "            action\n",
      "Prediction: action_actions_act\n",
      "Similarity: 0.583137035369873\n",
      "--------------------------------------------------\n",
      "True: reset\n",
      "Prediction: Status_()_Id\n",
      "Similarity: 0.13884474337100983\n",
      "--------------------------------------------------\n",
      "True: done\n",
      "Prediction: __reset_s\n",
      "Similarity: 0.26293644309043884\n",
      "--------------------------------------------------\n",
      "True: new_obs\n",
      "Prediction: reset_Reset_start\n",
      "Similarity: 0.1867683082818985\n",
      "--------------------------------------------------\n",
      "True: rew\n",
      "Prediction: done_reset_run\n",
      "Similarity: 0.1424301713705063\n",
      "--------------------------------------------------\n",
      "True: obs\n",
      "Prediction: env_done_abs\n",
      "Similarity: 0.1850934624671936\n",
      "--------------------------------------------------\n",
      "True: obses_tp1\n",
      "Prediction: actions_rewards_tasks\n",
      "Similarity: 0.17806468904018402\n",
      "--------------------------------------------------\n",
      "True: weights\n",
      "Prediction: 1_2_None\n",
      "Similarity: 0.15824611485004425\n",
      "--------------------------------------------------\n",
      "True: td_errors\n",
      "Prediction: es_s_x\n",
      "Similarity: 0.21692785620689392\n",
      "--------------------------------------------------\n",
      "True: checkpoint_freq\n",
      "Prediction: s_t_1\n",
      "Similarity: 0.2521430253982544\n",
      "--------------------------------------------------\n",
      "True: saved_mean_reward\n",
      "Prediction: model_value_max\n",
      "Similarity: 0.27790766954421997\n",
      "--------------------------------------------------\n",
      "Average similarity: 0.24101523\n"
     ]
    }
   ],
   "source": [
    "baseline_result_print(b2_result_k3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b806487",
   "metadata": {},
   "source": [
    "### For debug (Printing out all windows)\n",
    "\n",
    "To use, uncomment the following two lines in the function \"split_into_windows\" <br>\n",
    "print(current_window) <br>\n",
    "print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5745f5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0%\n",
      "tails on the act function.\n",
      "    \"\"\"\n",
      "    # Create all the functions necessary to train the model\n",
      "\n",
      "    <mask> = get_session()\n",
      "    set_global_seeds(seed)\n",
      "\n",
      "    q_func = build_q_func(network, **network_kwargs)\n",
      "\n",
      " \n",
      "--------\n",
      "he functions necessary to train the model\n",
      "\n",
      "    sess = get_session()\n",
      "    set_global_seeds(seed)\n",
      "\n",
      "    <mask> = build_q_func(network, **network_kwargs)\n",
      "\n",
      "    # capture the shape outside the closure so that the \n",
      "--------\n",
      "ure so that the env object is not serialized\n",
      "    # by cloudpickle when serializing make_obs_ph\n",
      "\n",
      "    <mask> = env.observation_space\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_spac\n",
      "--------\n",
      "\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, <mask>, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "\n",
      "--------\n",
      "space\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    <mask>, train, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q\n",
      "--------\n",
      "f make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, train, <mask>, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_act\n",
      "--------\n",
      "ame):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, train, update_target, <mask> = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=en\n",
      "--------\n",
      "ut(observation_space, name=name)\n",
      "\n",
      "    act, train, update_target, debug = deepq.build_train(\n",
      "        <mask>=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        optimizer=tf.tr\n",
      "--------\n",
      "\n",
      "    act, train, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        <mask>=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_r\n",
      "--------\n",
      "_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        <mask>=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma\n",
      "--------\n",
      "    make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        <mask>=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "     \n",
      "--------\n",
      "num_actions=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        <mask>=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "   \n",
      "--------\n",
      "n_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        <mask>=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      " \n",
      "--------\n",
      ".train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        <mask>=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "--------\n",
      "lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    <mask> = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "        'num_actions': env.action\n",
      "--------\n",
      "s_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "        'num_actions': env.action_space.n,\n",
      "    }\n",
      "\n",
      "    <mask> = ActWrapper(act, act_params)\n",
      "\n",
      "    # Create the replay buffer\n",
      "    if prioritized_replay:\n",
      "        re\n",
      "--------\n",
      "ct = ActWrapper(act, act_params)\n",
      "\n",
      "    # Create the replay buffer\n",
      "    if prioritized_replay:\n",
      "        <mask> = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
      "        if prioritized_repla\n",
      "--------\n",
      "size, alpha=prioritized_replay_alpha)\n",
      "        if prioritized_replay_beta_iters is None:\n",
      "            <mask> = total_timesteps\n",
      "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
      "           \n",
      "--------\n",
      "ized_replay_beta_iters is None:\n",
      "            prioritized_replay_beta_iters = total_timesteps\n",
      "        <mask> = LinearSchedule(prioritized_replay_beta_iters,\n",
      "                                       initial_p=pr\n",
      "--------\n",
      "eta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
      "                                       <mask>=prioritized_replay_beta0,\n",
      "                                       final_p=1.0)\n",
      "    else:\n",
      "        rep\n",
      "--------\n",
      "                         initial_p=prioritized_replay_beta0,\n",
      "                                       <mask>=1.0)\n",
      "    else:\n",
      "        replay_buffer = ReplayBuffer(buffer_size)\n",
      "        beta_schedule = None\n",
      "    #\n",
      "--------\n",
      "l_p=prioritized_replay_beta0,\n",
      "                                       final_p=1.0)\n",
      "    else:\n",
      "        <mask> = ReplayBuffer(buffer_size)\n",
      "        beta_schedule = None\n",
      "    # Create the schedule for exploration \n",
      "--------\n",
      "                   final_p=1.0)\n",
      "    else:\n",
      "        replay_buffer = ReplayBuffer(buffer_size)\n",
      "        <mask> = None\n",
      "    # Create the schedule for exploration starting from 1.\n",
      "    exploration = LinearSchedule(\n",
      "--------\n",
      "r_size)\n",
      "        beta_schedule = None\n",
      "    # Create the schedule for exploration starting from 1.\n",
      "    <mask> = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
      "                  \n",
      "--------\n",
      "le(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
      "                                 <mask>=1.0,\n",
      "                                 final_p=exploration_final_eps)\n",
      "\n",
      "    # Initialize the paramete\n",
      "--------\n",
      " total_timesteps),\n",
      "                                 initial_p=1.0,\n",
      "                                 <mask>=exploration_final_eps)\n",
      "\n",
      "    # Initialize the parameters and copy them to the target network.\n",
      "    U.\n",
      "--------\n",
      "ize the parameters and copy them to the target network.\n",
      "    U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    <mask> = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.Temp\n",
      "--------\n",
      "them to the target network.\n",
      "    U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    <mask> = None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "      \n",
      "--------\n",
      "   U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    <mask> = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoi\n",
      "--------\n",
      "update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    <mask> = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        m\n",
      "--------\n",
      " None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        <mask> = checkpoint_path or td\n",
      "\n",
      "        model_file = os.path.join(td, \"model\")\n",
      "        model_saved = False\n",
      "--------\n",
      "t = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        <mask> = os.path.join(td, \"model\")\n",
      "        model_saved = False\n",
      "\n",
      "        if tf.train.latest_checkpoint(td) \n",
      "--------\n",
      ") as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        model_file = os.path.join(td, \"model\")\n",
      "        <mask> = False\n",
      "\n",
      "        if tf.train.latest_checkpoint(td) is not None:\n",
      "            load_variables(model_fi\n",
      "--------\n",
      "variables(model_file)\n",
      "            logger.log('Loaded model from {}'.format(model_file))\n",
      "            <mask> = True\n",
      "        elif load_path is not None:\n",
      "            load_variables(load_path)\n",
      "            logger\n",
      "--------\n",
      "             break\n",
      "            # Take action and update exploration to the newest value\n",
      "            <mask> = {}\n",
      "            if not param_noise:\n",
      "                update_eps = exploration.value(t)\n",
      "            \n",
      "--------\n",
      "oration to the newest value\n",
      "            kwargs = {}\n",
      "            if not param_noise:\n",
      "                <mask> = exploration.value(t)\n",
      "                update_param_noise_threshold = 0.\n",
      "            else:\n",
      "        \n",
      "--------\n",
      "}\n",
      "            if not param_noise:\n",
      "                update_eps = exploration.value(t)\n",
      "                <mask> = 0.\n",
      "            else:\n",
      "                update_eps = 0.\n",
      "                # Compute the threshold such\n",
      "--------\n",
      "ration.value(t)\n",
      "                update_param_noise_threshold = 0.\n",
      "            else:\n",
      "                <mask> = 0.\n",
      "                # Compute the threshold such that the KL divergence between perturbed and non-\n",
      "--------\n",
      " for Exploration, Plappert et al., 2017\n",
      "                # for detailed explanation.\n",
      "                <mask> = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\n",
      "           \n",
      "--------\n",
      "ise_threshold'] = update_param_noise_threshold\n",
      "                kwargs['update_param_noise_scale'] = <mask> = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "    \n",
      "--------\n",
      " True\n",
      "            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            <mask> = action\n",
      "            reset = False\n",
      "            new_obs, rew, done, _ = env.step(env_action)\n",
      "       \n",
      "--------\n",
      "p.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            <mask> = False\n",
      "            new_obs, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in \n",
      "--------\n",
      "s, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new_obs, rew, <mask>, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "            replay_b\n",
      "--------\n",
      "te_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new_obs, <mask>, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "            re\n",
      "--------\n",
      "s=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new<mask>obs, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "      \n",
      "--------\n",
      "_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            <mask>, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "         \n",
      "--------\n",
      "he replay buffer.\n",
      "            replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
      "            <mask> = new_obs\n",
      "\n",
      "            episode_rewards[-1] += rew\n",
      "            if done:\n",
      "                obs = env.re\n",
      "--------\n",
      "   U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    <mask> = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoi\n",
      "--------\n",
      "update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    <mask> = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        m\n",
      "--------\n",
      "pisode_rewards.append(0.0)\n",
      "                reset = True\n",
      "\n",
      "            if t > learning_starts and t % <mask> == 0:\n",
      "                # Minimize the error in Bellman's equation on a batch sampled from replay buf\n",
      "--------\n",
      "n on a batch sampled from replay buffer.\n",
      "                if prioritized_replay:\n",
      "                    <mask> = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n",
      "                    (obses_t, actio\n",
      "--------\n",
      "nes, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, <mask>, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.\n",
      "--------\n",
      "idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, rewards, obses_tp1, <mask> = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(rewards)\n",
      "--------\n",
      "rds, obses_tp1, dones, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    <mask>, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights,\n",
      "--------\n",
      "hts, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, rewards, <mask>, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(r\n",
      "--------\n",
      "s_tp1, dones, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, <mask>, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_id\n",
      "--------\n",
      " actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, <mask> = np.ones_like(rewards), None\n",
      "                td_errors = train(obses_t, actions, rewards, obses_tp\n",
      "--------\n",
      " obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    <mask>, batch_idxes = np.ones_like(rewards), None\n",
      "                td_errors = train(obses_t, actions, rewa\n",
      "--------\n",
      "(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(rewards), None\n",
      "                <mask> = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
      "                if prioritized_replay\n",
      "--------\n",
      "ons, rewards, obses_tp1, dones, weights)\n",
      "                if prioritized_replay:\n",
      "                    <mask> = np.abs(td_errors) + prioritized_replay_eps\n",
      "                    replay_buffer.update_priorities(ba\n",
      "--------\n",
      "y_buffer.update_priorities(batch_idxes, new_priorities)\n",
      "\n",
      "            if t > learning_starts and t % <mask> == 0:\n",
      "                # Update target network periodically.\n",
      "                update_target()\n",
      "\n",
      "      \n",
      "--------\n",
      "\n",
      "                # Update target network periodically.\n",
      "                update_target()\n",
      "\n",
      "            <mask> = round(np.mean(episode_rewards[-101:-1]), 1)\n",
      "            num_episodes = len(episode_rewards)\n",
      "     \n",
      "--------\n",
      "e_target()\n",
      "\n",
      "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
      "            <mask> = len(episode_rewards)\n",
      "            if done and print_freq is not None and len(episode_rewards) % pr\n",
      "--------\n",
      "es = len(episode_rewards)\n",
      "            if done and print_freq is not None and len(episode_rewards) % <mask> == 0:\n",
      "                logger.record_tabular(\"steps\", t)\n",
      "                logger.record_tabular(\"epis\n",
      "--------\n",
      "ckpoint_freq is not None and t > learning_starts and\n",
      "                    num_episodes > 100 and t % <mask> == 0):\n",
      "                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n",
      "     \n",
      "--------\n",
      "variables(model_file)\n",
      "            logger.log('Loaded model from {}'.format(model_file))\n",
      "            <mask> = True\n",
      "        elif load_path is not None:\n",
      "            load_variables(load_path)\n",
      "            logger\n",
      "--------\n",
      "              save_variables(model_file)\n",
      "                    model_saved = True\n",
      "                    <mask> = mean_100ep_reward\n",
      "        if model_saved:\n",
      "            if print_freq is not None:\n",
      "                \n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinyoung\\anaconda3\\envs\\mlp\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['#',\n",
       "   'q',\n",
       "   '*',\n",
       "   'target',\n",
       "   'obs',\n",
       "   'log',\n",
       "   '_',\n",
       "   'fn',\n",
       "   'q',\n",
       "   'n',\n",
       "   'model',\n",
       "   'gamma',\n",
       "   'beta',\n",
       "   'noise',\n",
       "   'env',\n",
       "   'act',\n",
       "   'buff',\n",
       "   'beta',\n",
       "   'schedule',\n",
       "   'beta',\n",
       "   'beta',\n",
       "   'b',\n",
       "   'p',\n",
       "   'schedule',\n",
       "   'p',\n",
       "   'steps',\n",
       "   '/',\n",
       "   'obs',\n",
       "   '#',\n",
       "   'obs',\n",
       "   'td',\n",
       "   '+',\n",
       "   'train',\n",
       "   'result',\n",
       "   't',\n",
       "   't',\n",
       "   '/',\n",
       "   'threshold',\n",
       "   'p',\n",
       "   'action',\n",
       "   'reset',\n",
       "   'done',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   '/',\n",
       "   '#',\n",
       "   'obs',\n",
       "   '2',\n",
       "   't',\n",
       "   'rewards',\n",
       "   'weights',\n",
       "   'experience',\n",
       "   'losses',\n",
       "   'weights',\n",
       "   '_',\n",
       "   '#',\n",
       "   'acc',\n",
       "   'ba',\n",
       "   '2',\n",
       "   '/',\n",
       "   'pr',\n",
       "   '2',\n",
       "   '2',\n",
       "   'result',\n",
       "   '*']],\n",
       " [['sess',\n",
       "   'q_func',\n",
       "   'observation_space',\n",
       "   'train',\n",
       "   'act',\n",
       "   'update_target',\n",
       "   'debug',\n",
       "   'make_obs_ph',\n",
       "   'q_func',\n",
       "   'num_actions',\n",
       "   'optimizer',\n",
       "   'gamma',\n",
       "   'grad_norm_clipping',\n",
       "   'param_noise',\n",
       "   'act_params',\n",
       "   'act',\n",
       "   'replay_buffer',\n",
       "   'prioritized_replay_beta_iters',\n",
       "   'beta_schedule',\n",
       "   'initial_p',\n",
       "   'final_p',\n",
       "   'replay_buffer',\n",
       "   'beta_schedule',\n",
       "   'exploration',\n",
       "   'initial_p',\n",
       "   'final_p',\n",
       "   'episode_rewards',\n",
       "   'saved_mean_reward',\n",
       "   'obs',\n",
       "   'reset',\n",
       "   'td',\n",
       "   'model_file',\n",
       "   'model_saved',\n",
       "   'model_saved',\n",
       "   'kwargs',\n",
       "   'update_eps',\n",
       "   'update_param_noise_threshold',\n",
       "   'update_eps',\n",
       "   'update_param_noise_threshold',\n",
       "   'True\\n            action',\n",
       "   'env_action',\n",
       "   'reset',\n",
       "   'done',\n",
       "   'rew',\n",
       "   '_',\n",
       "   'new_obs',\n",
       "   'obs',\n",
       "   'obs',\n",
       "   'reset',\n",
       "   'train_freq',\n",
       "   'experience',\n",
       "   'rewards',\n",
       "   'dones',\n",
       "   'obses_t',\n",
       "   'obses_tp1',\n",
       "   'actions',\n",
       "   'batch_idxes',\n",
       "   'weights',\n",
       "   'td_errors',\n",
       "   'new_priorities',\n",
       "   'target_network_update_freq',\n",
       "   'mean_100ep_reward',\n",
       "   'num_episodes',\n",
       "   'print_freq',\n",
       "   'checkpoint_freq',\n",
       "   'model_saved',\n",
       "   'saved_mean_reward']],\n",
       " [[0.2947819,\n",
       "   0.41529685,\n",
       "   0.1636069,\n",
       "   0.36016828,\n",
       "   0.21532297,\n",
       "   0.07122346,\n",
       "   0.13182333,\n",
       "   0.07105673,\n",
       "   0.41529685,\n",
       "   0.25388032,\n",
       "   0.3253739,\n",
       "   1.0000001,\n",
       "   0.06346449,\n",
       "   0.60662854,\n",
       "   0.20330966,\n",
       "   1.0000001,\n",
       "   0.30218685,\n",
       "   0.46546584,\n",
       "   0.5948588,\n",
       "   0.16273127,\n",
       "   0.27964023,\n",
       "   0.101940885,\n",
       "   0.09043784,\n",
       "   0.19328178,\n",
       "   0.34524935,\n",
       "   0.26112336,\n",
       "   0.15396966,\n",
       "   0.102608435,\n",
       "   0.19715221,\n",
       "   0.18008484,\n",
       "   1.0000001,\n",
       "   0.04936334,\n",
       "   0.1320903,\n",
       "   0.11304621,\n",
       "   0.24170542,\n",
       "   0.078141645,\n",
       "   0.015610911,\n",
       "   -0.022503989,\n",
       "   0.08729151,\n",
       "   0.7902819,\n",
       "   0.11731528,\n",
       "   0.3038026,\n",
       "   0.25104502,\n",
       "   0.29512385,\n",
       "   1.0,\n",
       "   0.25797737,\n",
       "   0.23818575,\n",
       "   0.19715221,\n",
       "   0.18008484,\n",
       "   0.1965228,\n",
       "   0.2609732,\n",
       "   1.0,\n",
       "   0.13446906,\n",
       "   0.28792956,\n",
       "   0.11206854,\n",
       "   0.25120395,\n",
       "   0.03399688,\n",
       "   0.17106567,\n",
       "   0.054185577,\n",
       "   0.14459443,\n",
       "   0.06412354,\n",
       "   0.1709067,\n",
       "   0.13339606,\n",
       "   0.056961883,\n",
       "   0.12620357,\n",
       "   0.11304621,\n",
       "   0.20381415]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test(df_valid.head(1), fill_mask_b1, top_k=1, mask_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde7f756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def learn(env,\n",
      "          network,\n",
      "          seed=None,\n",
      "          lr=5e-4,\n",
      "          total_timesteps=100000,\n",
      "          buffer_size=50000,\n",
      "          exploration_fraction=0.1,\n",
      "          exploration_final_eps=0.02,\n",
      "          train_freq=1,\n",
      "          batch_size=32,\n",
      "          print_freq=100,\n",
      "          checkpoint_freq=10000,\n",
      "          checkpoint_path=None,\n",
      "          learning_starts=1000,\n",
      "          gamma=1.0,\n",
      "          target_network_update_freq=500,\n",
      "          prioritized_replay=False,\n",
      "          prioritized_replay_alpha=0.6,\n",
      "          prioritized_replay_beta0=0.4,\n",
      "          prioritized_replay_beta_iters=None,\n",
      "          prioritized_replay_eps=1e-6,\n",
      "          param_noise=False,\n",
      "          callback=None,\n",
      "          load_path=None,\n",
      "          **network_kwargs\n",
      "            ):\n",
      "    \"\"\"Train a deepq model.\n",
      "\n",
      "    Parameters\n",
      "    -------\n",
      "    env: gym.Env\n",
      "        environment to train on\n",
      "    network: string or a function\n",
      "        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n",
      "        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n",
      "        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n",
      "    seed: int or None\n",
      "        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n",
      "    lr: float\n",
      "        learning rate for adam optimizer\n",
      "    total_timesteps: int\n",
      "        number of env steps to optimizer for\n",
      "    buffer_size: int\n",
      "        size of the replay buffer\n",
      "    exploration_fraction: float\n",
      "        fraction of entire training period over which the exploration rate is annealed\n",
      "    exploration_final_eps: float\n",
      "        final value of random action probability\n",
      "    train_freq: int\n",
      "        update the model every `train_freq` steps.\n",
      "        set to None to disable printing\n",
      "    batch_size: int\n",
      "        size of a batched sampled from replay buffer for training\n",
      "    print_freq: int\n",
      "        how often to print out training progress\n",
      "        set to None to disable printing\n",
      "    checkpoint_freq: int\n",
      "        how often to save the model. This is so that the best version is restored\n",
      "        at the end of the training. If you do not wish to restore the best version at\n",
      "        the end of the training set this variable to None.\n",
      "    learning_starts: int\n",
      "        how many steps of the model to collect transitions for before learning starts\n",
      "    gamma: float\n",
      "        discount factor\n",
      "    target_network_update_freq: int\n",
      "        update the target network every `target_network_update_freq` steps.\n",
      "    prioritized_replay: True\n",
      "        if True prioritized replay buffer will be used.\n",
      "    prioritized_replay_alpha: float\n",
      "        alpha parameter for prioritized replay buffer\n",
      "    prioritized_replay_beta0: float\n",
      "        initial value of beta for prioritized replay buffer\n",
      "    prioritized_replay_beta_iters: int\n",
      "        number of iterations over which beta will be annealed from initial value\n",
      "        to 1.0. If set to None equals to total_timesteps.\n",
      "    prioritized_replay_eps: float\n",
      "        epsilon to add to the TD errors when updating priorities.\n",
      "    param_noise: bool\n",
      "        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n",
      "    callback: (locals, globals) -> None\n",
      "        function called at every steps with state of the algorithm.\n",
      "        If callback returns true training stops.\n",
      "    load_path: str\n",
      "        path to load the model from. (default: None)\n",
      "    **network_kwargs\n",
      "        additional keyword arguments to pass to the network builder.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    act: ActWrapper\n",
      "        Wrapper over act function. Adds ability to save it and load it.\n",
      "        See header of baselines/deepq/categorical.py for details on the act function.\n",
      "    \"\"\"\n",
      "    # Create all the functions necessary to train the model\n",
      "\n",
      "    sess = get_session()\n",
      "    set_global_seeds(seed)\n",
      "\n",
      "    q_func = build_q_func(network, **network_kwargs)\n",
      "\n",
      "    # capture the shape outside the closure so that the env object is not serialized\n",
      "    # by cloudpickle when serializing make_obs_ph\n",
      "\n",
      "    observation_space = env.observation_space\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, train, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "        'num_actions': env.action_space.n,\n",
      "    }\n",
      "\n",
      "    act = ActWrapper(act, act_params)\n",
      "\n",
      "    # Create the replay buffer\n",
      "    if prioritized_replay:\n",
      "        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
      "        if prioritized_replay_beta_iters is None:\n",
      "            prioritized_replay_beta_iters = total_timesteps\n",
      "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
      "                                       initial_p=prioritized_replay_beta0,\n",
      "                                       final_p=1.0)\n",
      "    else:\n",
      "        replay_buffer = ReplayBuffer(buffer_size)\n",
      "        beta_schedule = None\n",
      "    # Create the schedule for exploration starting from 1.\n",
      "    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
      "                                 initial_p=1.0,\n",
      "                                 final_p=exploration_final_eps)\n",
      "\n",
      "    # Initialize the parameters and copy them to the target network.\n",
      "    U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        model_file = os.path.join(td, \"model\")\n",
      "        model_saved = False\n",
      "\n",
      "        if tf.train.latest_checkpoint(td) is not None:\n",
      "            load_variables(model_file)\n",
      "            logger.log('Loaded model from {}'.format(model_file))\n",
      "            model_saved = True\n",
      "        elif load_path is not None:\n",
      "            load_variables(load_path)\n",
      "            logger.log('Loaded model from {}'.format(load_path))\n",
      "\n",
      "\n",
      "        for t in range(total_timesteps):\n",
      "            if callback is not None:\n",
      "                if callback(locals(), globals()):\n",
      "                    break\n",
      "            # Take action and update exploration to the newest value\n",
      "            kwargs = {}\n",
      "            if not param_noise:\n",
      "                update_eps = exploration.value(t)\n",
      "                update_param_noise_threshold = 0.\n",
      "            else:\n",
      "                update_eps = 0.\n",
      "                # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
      "                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
      "                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
      "                # for detailed explanation.\n",
      "                update_param_noise_threshold = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\n",
      "                kwargs['reset'] = reset\n",
      "                kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
      "                kwargs['update_param_noise_scale'] = True\n",
      "            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new_obs, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "            replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
      "            obs = new_obs\n",
      "\n",
      "            episode_rewards[-1] += rew\n",
      "            if done:\n",
      "                obs = env.reset()\n",
      "                episode_rewards.append(0.0)\n",
      "                reset = True\n",
      "\n",
      "            if t > learning_starts and t % train_freq == 0:\n",
      "                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
      "                if prioritized_replay:\n",
      "                    experience = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n",
      "                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(rewards), None\n",
      "                td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
      "                if prioritized_replay:\n",
      "                    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n",
      "                    replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
      "\n",
      "            if t > learning_starts and t % target_network_update_freq == 0:\n",
      "                # Update target network periodically.\n",
      "                update_target()\n",
      "\n",
      "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
      "            num_episodes = len(episode_rewards)\n",
      "            if done and print_freq is not None and len(episode_rewards) % print_freq == 0:\n",
      "                logger.record_tabular(\"steps\", t)\n",
      "                logger.record_tabular(\"episodes\", num_episodes)\n",
      "                logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
      "                logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n",
      "                logger.dump_tabular()\n",
      "\n",
      "            if (checkpoint_freq is not None and t > learning_starts and\n",
      "                    num_episodes > 100 and t % checkpoint_freq == 0):\n",
      "                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n",
      "                    if print_freq is not None:\n",
      "                        logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
      "                                   saved_mean_reward, mean_100ep_reward))\n",
      "                    save_variables(model_file)\n",
      "                    model_saved = True\n",
      "                    saved_mean_reward = mean_100ep_reward\n",
      "        if model_saved:\n",
      "            if print_freq is not None:\n",
      "                logger.log(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n",
      "            load_variables(model_file)\n",
      "\n",
      "    return act\n"
     ]
    }
   ],
   "source": [
    "print(df_valid.loc[0, 'code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a804e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62429af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final testing (in progress)\n",
    "\n",
    "# print(\"Total mean value of all cosine similarities\")\n",
    "# for index, result in enumerate([b1_result_k1, b1_result_k2, b1_result_k3]):\n",
    "#     similarity_scores_list = result[2]\n",
    "#     total_values = 0\n",
    "#     total_mask_num = 0\n",
    "#     for similarity_scores in similarity_scores_list:\n",
    "#         if similarity_scores:\n",
    "#             total_values += np.sum(similarity_scores)\n",
    "#             total_mask_num += len(similarity_scores)\n",
    "    \n",
    "#     total_average = total_values / total_mask_num\n",
    "#     print(f\"Top_k={index+1}\\n Total mask number: {total_mask_num}, Total average cosine similarity: {round(total_average, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461527e3",
   "metadata": {},
   "source": [
    "### Baseline score 2: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}